{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demonstration for how to use ThirdAI's document search engine. It assumes you are running in the ThirdAI demo docker container. Once in the container, you can get this notebook by running\n",
    "```\n",
    "wget https://raw.githubusercontent.com/ThirdAILabs/Demos/main/DocSearch.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to import our document search package and the embedding model we will use to embed the documents, and a couple of other things that will help us along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import thirdai\n",
    "from thirdai.search import DocRetrieval\n",
    "\n",
    "from embeddings import DocSearchModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you're going to need a dataset, which just consists of a collection of (document_id, document_text) pairs. Our engine works best with document sizes between 20 and 200 words long. Feel free to split big documents up into many small passages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for you: Populate dataset (choose whatever you want!)\n",
    "dataset = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an initially empty index! As a quick description of the input to the constructor, \n",
    "* dense_input_dimension is the dimension of the output of our embedding model (128 with our preloaded model)\n",
    "\n",
    "* num_tables and hashes_per_table are hyperparameters of the index\n",
    "  * increasing num_tables increases the accuracy of the model at the cost of speed and memory (a good value is the prepopulated 16)\n",
    "  \n",
    "  * hashes_per_table has a sweet spot in accuracy at around log_2(average document size)\n",
    "  \n",
    "* centroids are a numpy array that represents precomputed centroids for the embedding space; we've already calculated and stored these for you! We've precomputed 2^18 of them, but if you're only adding a few points a simple heuristic is to select a random subsample of them, which we do below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_centroids = embedding_model.centroids()\n",
    "np.random.shuffle(all_centroids)\n",
    "reduced_centroids = all_centroids[:len(dataset) / 10]\n",
    "\n",
    "our_index = DocRetrieval(\n",
    "  dense_input_dimension=128, \n",
    "  num_tables=16, \n",
    "  hashes_per_table=6, \n",
    "  centroids=reduced_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's populate the index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, doc_text in dataset:\n",
    "  embedding = embedding_model.encodeDocs([doc_text])[0]\n",
    "  scratch_index.add_doc(doc_id=doc_id, doc_text=doc_text, doc_embeddings=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can query it! The following cell starts up an interactive demo, where you can type any query you want and the index will return the result most semantically likely to answer your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "  print(\"> \", end='')\n",
    "  query_text = input()\n",
    "  if (query_text == \"q\"):\n",
    "    break\n",
    "\n",
    "  start = time.time()\n",
    "  embedding = embedding_model.encodeQuery(query_text)\n",
    "  result = our_index.query(embedding, top_k=8192)\n",
    "  total_time = time.time() - start\n",
    "\n",
    "  print(result[0])\n",
    "  print(f\"Took {total_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
