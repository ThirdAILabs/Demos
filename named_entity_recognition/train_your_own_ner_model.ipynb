{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts to train a NER Model\n",
    "\n",
    "This script use ThirdAI's NER library to train a model on an NER dataset from scratch. For this demonstration, we are using the `https://huggingface.co/datasets/conll2003` dataset. We also show how to load a saved ThirdAI NER model and further fine-tune it, provided the labels remain the same.\n",
    "\n",
    "If you want to just use ThirdAI's pretrained multi-lingual PII detection model, please refer to [the other notebook](https://github.com/ThirdAILabs/Demos/blob/main/named_entity_recognition/pretrained_pii_model.ipynb) in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "from thirdai import bolt, dataset, licensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate ThirdAI's license key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tag to label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_MAP = {\n",
    "    \"O\": 0,\n",
    "    \"B-PER\": 1,\n",
    "    \"I-PER\": 2,\n",
    "    \"B-ORG\": 3,\n",
    "    \"I-ORG\": 4,\n",
    "    \"B-LOC\": 5,\n",
    "    \"I-LOC\": 6,\n",
    "    \"B-MISC\": 7,\n",
    "    \"I-MISC\": 8,\n",
    "}\n",
    "\n",
    "entries = list(TAG_MAP.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to download and process the dataset\n",
    "\n",
    "Functions to load dataset from huggingface and save training data as JSONL file. \n",
    "It could be txt file too, given each line contains a json of format: \n",
    "\n",
    "`{\"source\": [List of Text Tokens], \"target\": [List of Corresponding Text Tags]}`. \n",
    "\n",
    "Note: Make sure tags should not be outside of TAG_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_as_jsonl(filename, loaded_data):\n",
    "    with open(filename, \"w\") as file:\n",
    "        for example in loaded_data:\n",
    "            data = {\n",
    "                \"source\": example[\"tokens\"],\n",
    "                \"target\": [entries[tag] for tag in example[\"ner_tags\"]],\n",
    "            }\n",
    "            file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "\n",
    "def download_dataset_as_file(subset):\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"conll2003\")\n",
    "    loaded_data = dataset[f\"{subset}\"]\n",
    "    filename = f\"{subset}_ner_data.jsonl\"\n",
    "    save_dataset_as_jsonl(filename, loaded_data)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Bolt NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = bolt.NER(TAG_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use thirdai's dataset module to load train file into a NerDataSource and pass it to train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_source = dataset.NerDataSource(type=ner_model.type(), file_path=download_dataset_as_file(\"train\"), token_column=\"source\", tag_column=\"target\")\n",
    "\n",
    "ner_model.train(\n",
    "    train_data=train_data_source,\n",
    "    epochs=1,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=1024,\n",
    "    train_metrics=[\"loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have validation data, you can pass that to train function too as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_source = dataset.NerDataSource(type=ner_model.type(), file_path=download_dataset_as_file(\"train\"), token_column=\"source\", tag_column=\"target\")\n",
    "val_data_source = dataset.NerDataSource(type=ner_model.type(), file_path=download_dataset_as_file(\"validation\"), token_column=\"source\", tag_column=\"target\")\n",
    "\n",
    "ner_model.train(\n",
    "    train_data=train_data_source,\n",
    "    epochs=2,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=1024,\n",
    "    train_metrics=[\"loss\"],\n",
    "    val_data=val_data_source,\n",
    "    val_metrics=[\"loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model.save(\"thirdai_ner_model\")\n",
    "ner_model = bolt.NER.load(\"thirdai_ner_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_dataset(\"conll2003\")[\"test\"]\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "for example in test_data:\n",
    "    tokens = example[\"tokens\"]\n",
    "    actual_tags = [entries[tag] for tag in example[\"ner_tags\"]]\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predicted_tags = ner_model.get_ner_tags([tokens])[0]\n",
    "\n",
    "    predictions.extend(predicted_tags)\n",
    "    actuals.extend(actual_tags)\n",
    "\n",
    "correct_predictions = sum(p[0][0] == a for p, a in zip(predictions, actuals))\n",
    "total_predictions = len(predictions)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model further\n",
    "\n",
    "In case, you want to further finetune on a already trained model, using a subset of tags. Here, we are creating a small retraining data, further we save it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sample sentences with corresponding NER tags\n",
    "sentences = [\n",
    "    (\"John Doe went to Paris\", [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    (\n",
    "        \"Alice and Bob are from New York City\",\n",
    "        [\"B-PER\", \"O\", \"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "    ),\n",
    "    (\"The Eiffel Tower is in France\", [\"O\", \"B-LOC\", \"I-LOC\", \"O\", \"O\", \"B-LOC\"]),\n",
    "    (\"Microsoft Corporation was founded by Bill Gates\", [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-PER\", \"I-PER\"]),\n",
    "    (\"She visited the Louvre Museum in Paris last summer\", [\"O\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\", \"B-LOC\", \"O\", \"O\"]),\n",
    "    (\"Google and IBM are big tech companies\", [\"B-ORG\", \"O\", \"B-ORG\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    (\"Mount Everest is the highest mountain in the world\", [\"B-LOC\", \"I-LOC\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]),\n",
    "    (\"Leonardo DiCaprio won an Oscar\", [\"B-PER\", \"I-PER\", \"O\", \"O\", \"O\"]),\n",
    "]\n",
    "\n",
    "# File to write the data\n",
    "retrain_filename = \"retraining_ner_data.json\"\n",
    "with open(retrain_filename, \"w\") as file:\n",
    "    for sentence, tags in sentences:\n",
    "        tokens = sentence.split()\n",
    "        data = {\"source\": tokens, \"target\": tags}\n",
    "        json_line = json.dumps(data)\n",
    "        file.write(json_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can just call the train function again for retraining the NER model on subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_data_source = dataset.NerDataSource(type=ner_model.type(), file_path=retrain_filename, token_column=\"source\", tag_column=\"target\")\n",
    "\n",
    "ner_model.train(\n",
    "    train_data=retrain_data_source,\n",
    "    epochs=3,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=1024,\n",
    "    train_metrics=[\"loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use this pretrained model for other use-cases too with `from_pretrained` function, when you call this function we replaces the tags with new set of tags, while conserving the learning from last training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'tl_ner_data.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_times': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'epoch_times': [1.1979999542236328,\n",
       "  0.017000000923871994,\n",
       "  0.017000000923871994,\n",
       "  0.014999999664723873,\n",
       "  0.014999999664723873,\n",
       "  0.01600000075995922,\n",
       "  0.01600000075995922,\n",
       "  0.017000000923871994,\n",
       "  0.017000000923871994,\n",
       "  0.017000000923871994],\n",
       " 'train_loss': [1.5787938833236694,\n",
       "  0.8555572032928467,\n",
       "  0.5551876425743103,\n",
       "  0.32909536361694336,\n",
       "  0.1694101244211197,\n",
       "  0.07679398357868195,\n",
       "  0.03332304581999779,\n",
       "  0.015141624957323074,\n",
       "  0.007578184362500906,\n",
       "  0.004184901714324951]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'tl_ner_data.json' | vectors 60 | batches 30 | time 0.009s | complete\n",
      "\n",
      "train | epoch 0 | train_steps 30 | train_loss=1.57879  | train_batches 30 | time 1.198s\n",
      "\n",
      "validate | epoch 0 | train_steps 30 |  | val_batches 0 | time 0.000s       \n",
      "\n",
      "train | epoch 1 | train_steps 60 | train_loss=0.855557  | train_batches 30 | time 0.017s\n",
      "\n",
      "validate | epoch 1 | train_steps 60 |  | val_batches 0 | time 0.000s       \n",
      "\n",
      "train | epoch 2 | train_steps 90 | train_loss=0.555188  | train_batches 30 | time 0.017s\n",
      "\n",
      "validate | epoch 2 | train_steps 90 |  | val_batches 0 | time 0.000s       \n",
      "\n",
      "train | epoch 3 | train_steps 120 | train_loss=0.329095  | train_batches 30 | time 0.015s\n",
      "\n",
      "validate | epoch 3 | train_steps 120 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 4 | train_steps 150 | train_loss=0.16941  | train_batches 30 | time 0.015s\n",
      "\n",
      "validate | epoch 4 | train_steps 150 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 5 | train_steps 180 | train_loss=0.076794  | train_batches 30 | time 0.016s\n",
      "\n",
      "validate | epoch 5 | train_steps 180 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 6 | train_steps 210 | train_loss=0.033323  | train_batches 30 | time 0.016s\n",
      "\n",
      "validate | epoch 6 | train_steps 210 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 7 | train_steps 240 | train_loss=0.0151416  | train_batches 30 | time 0.017s\n",
      "\n",
      "validate | epoch 7 | train_steps 240 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 8 | train_steps 270 | train_loss=0.00757818  | train_batches 30 | time 0.017s\n",
      "\n",
      "validate | epoch 8 | train_steps 270 |  | val_batches 0 | time 0.000s      \n",
      "\n",
      "train | epoch 9 | train_steps 300 | train_loss=0.0041849  | train_batches 30 | time 0.017s\n",
      "\n",
      "validate | epoch 9 | train_steps 300 |  | val_batches 0 | time 0.000s      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"thirdai_ner_model\"\n",
    "ner_model.save(model_save_path)\n",
    "\n",
    "NEW_TAG_MAP = {\n",
    "    \"O\": 0,          \n",
    "    \"B-FN\": 1,       \n",
    "    \"I-FN\": 2,       \n",
    "    \"B-LN\": 3,       \n",
    "    \"I-LN\": 4,      \n",
    "    \"B-CITY\": 5,\n",
    "    \"I-CITY\": 6,     \n",
    "    \"B-STATE\": 7,    \n",
    "    \"I-STATE\": 8,    \n",
    "    \"B-COUNTRY\": 9,  \n",
    "    \"I-COUNTRY\": 10  \n",
    "}\n",
    "\n",
    "sentences = [\n",
    "    (\"Samantha Bloom traveled to Paris France last year\", [\"B-FN\", \"B-LN\", \"O\", \"O\", \"B-CITY\", \"B-COUNTRY\", \"O\", \"O\"]),\n",
    "    (\"John Smith and Alice Johnson are from Austin Texas\", [\"B-FN\", \"B-LN\", \"O\", \"B-FN\", \"B-LN\", \"O\", \"O\", \"B-CITY\", \"B-STATE\"]),\n",
    "    (\"Timothy Dalton was born in Cardiff Wales UK\", [\"B-FN\", \"B-LN\", \"O\", \"O\", \"O\", \"B-CITY\", \"B-STATE\", \"B-COUNTRY\"]),\n",
    "    (\"Hilary Swank visited Toronto Ontario Canada last month\", [\"B-FN\", \"B-LN\", \"O\", \"B-CITY\", \"B-STATE\", \"B-COUNTRY\", \"O\", \"O\"]),\n",
    "    (\"Nikola Tesla was born in Smiljan Croatia\", [\"B-FN\", \"B-LN\", \"O\", \"O\", \"O\", \"B-CITY\", \"B-COUNTRY\"]),\n",
    "    (\"Michael Jordan, a native of Brooklyn New York, is famous worldwide\", [\"B-FN\", \"B-LN\", \"O\", \"O\", \"O\", \"B-CITY\", \"B-STATE\", \"I-STATE\", \"O\", \"O\", \"O\"]),\n",
    "    (\"Julia Roberts was seen in Malibu California this week\", [\"B-FN\", \"B-LN\", \"O\", \"O\", \"O\", \"B-CITY\", \"B-STATE\", \"O\", \"O\"]),\n",
    "]\n",
    "\n",
    "tl_filename = \"tl_ner_data.json\"\n",
    "with open(tl_filename, \"w\") as file:\n",
    "    for sentence, tags in sentences:\n",
    "        tokens = sentence.split()\n",
    "        data = {\"source\": tokens, \"target\": tags}\n",
    "        json_line = json.dumps(data)\n",
    "        file.write(json_line + \"\\n\")\n",
    "\n",
    "tl_ner_model = bolt.NER.from_pretrained(model_save_path, NEW_TAG_MAP)\n",
    "\n",
    "tl_data_source = dataset.NerDataSource(type=tl_ner_model.type(), file_path=tl_filename, token_column=\"source\", tag_column=\"target\")\n",
    "\n",
    "tl_ner_model.train(\n",
    "    train_data=tl_data_source,\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=2,\n",
    "    train_metrics=[\"loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'retraining_ner_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthirdai_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretrain_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(tl_data_source)\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_ner_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'retraining_ner_data.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.remove(\"thirdai_ner_model\")\n",
    "os.remove(retrain_filename)\n",
    "os.remove(tl_data_source)\n",
    "os.remove(\"train_ner_data.jsonl\")\n",
    "os.remove(\"validation_ner_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
