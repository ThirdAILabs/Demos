{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection\n",
    "\n",
    "This notebook shows how to build a fraud detection model with ThirdAI's Universal Deep Transformer (UDT) model, our all-purpose classifier for tabular datasets. In this demo, we will train and evaluate the model on a fraud detection dataset from kaggle, but you can easily replace this with your own dataset.\n",
    "\n",
    "To run this notebook, you will need to obtain a ThirdAI license at the following link if you have not already: https://www.thirdai.com/try-bolt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt\n",
    "!pip3 install kaggle\n",
    "!pip3 install thirdai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download\n",
    "Here we use the kaggle api to download the fraud detection dataset found here: https://www.kaggle.com/datasets/ealaxi/paysim1\n",
    "\n",
    "Downloading this dataset requires authentication from a kaggle account. To use the kaggle API like we do below requires a valid kaggle.json file with credentials stored. Visit https://github.com/Kaggle/kaggle-api#api-credentials for more documentation on the kaggle API. \n",
    "\n",
    "You may also choose to download the dataset directly from the source, in which case you should provide the path to the dataset in the prep_fraud_dataset() call later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "\n",
    "kaggle.api.authenticate()\n",
    "\n",
    "kaggle.api.dataset_download_files('ealaxi/paysim1', path='./fraud_detection', unzip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the utils module in this repo to prepare the data for training (if you have just copied this notebook and not cloned the entire repo, you will need to copy the utils.py file as well). You can replace this step and the next step with a download method and a UDT initialization step that is specific to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prep_fraud_dataset\n",
    "\n",
    "dataset_filename = \"./fraud_detection/PS_20174392719_1491204439457_log.csv\"\n",
    "\n",
    "train_filename, test_filename, inference_batch = prep_fraud_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDT Initialization\n",
    "We can now create a UDT model by passing in the types of each column in the dataset and the target column we want to be able to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "model = bolt.UniversalDeepTransformer(\n",
    "    data_types={\n",
    "        \"step\": bolt.types.categorical(),\n",
    "        \"type\": bolt.types.categorical(),\n",
    "        \"amount\": bolt.types.numerical(range=(0, 10000001)),\n",
    "        \"nameOrig\": bolt.types.categorical(),\n",
    "        \"oldbalanceOrg\": bolt.types.numerical(range=(0, 59585041)),\n",
    "        \"newbalanceOrig\": bolt.types.numerical(range=(0, 49585041)),\n",
    "        \"nameDest\": bolt.types.categorical(),\n",
    "        \"oldbalanceDest\": bolt.types.numerical(range=(0, 356015890)),\n",
    "        \"newbalanceDest\": bolt.types.numerical(range=(0, 356179279)),\n",
    "        \"isFraud\": bolt.types.categorical(),\n",
    "        \"isFlaggedFraud\": bolt.types.categorical(),\n",
    "    },\n",
    "    target=\"isFraud\",\n",
    "    n_target_classes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We can now train our UDT model with just two lines! Feel free to customize the number of epochs and the learning rate; we have chosen values that give good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_filename, epochs=2, learning_rate=0.001, metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluating the performance of the UDT model is also just two lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_filename, metrics=[\"categorical_accuracy\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading\n",
    "Saving and loading a trained UDT model to disk is also extremely straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = \"fraud_detection.model\"\n",
    "\n",
    "# Saving\n",
    "model.save(save_location)\n",
    "\n",
    "# Loading\n",
    "model = bolt.UniversalDeepTransformer.load(save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Predictions\n",
    "The evaluation method is great for testing, but it requires labels, which don't exist in a production setting. We also have a predict method that can take in an in-memory batch of rows or a single row (without the target column), allowing easy integration into production pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Inference batch:\", inference_batch, \"\\n\")\n",
    "\n",
    "prediction = model.predict(inference_batch[0])\n",
    "class_name = model.class_name(np.argmax(prediction))\n",
    "print(\"Input:\", inference_batch[0], \"Prediction:\", class_name, \"\\n\")\n",
    "\n",
    "prediction_batch = model.predict_batch(inference_batch)\n",
    "class_names = [\n",
    "    model.class_name(class_id) for class_id in np.argmax(prediction_batch, axis=1)\n",
    "]\n",
    "print(\"Batch Prediction Results\")\n",
    "for input_sample, class_name in zip(inference_batch, class_names):\n",
    "    print(\"Input:\", input_sample, \"Prediction:\", class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
