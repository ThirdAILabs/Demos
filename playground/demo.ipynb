{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's Playground\n",
    "\n",
    "In this notebook, we will show \n",
    "\n",
    "1. How to easily build a semantic QnA engine for all your documents with ThirdAI's BOLT engine.\n",
    "\n",
    "2. (Optional) How to use your OpenAI key to get retrieval augmented answers from OpenAI.\n",
    "\n",
    "3. How to teach your retrieval model with RLHF.\n",
    "\n",
    "4. (Optional) How to save your models and export to ThirdAI's Playground web-app to do interative QnA and teach your model with RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thirdai's license activation\n",
    "\n",
    "import thirdai\n",
    "try:\n",
    "    thirdai.licensing.activate(\"\")\n",
    "except:\n",
    "    print(\"You need a license key to use ThirdAI's library. Please request a trial license at https://www.thirdai.com/try-bolt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "import os\n",
    "import nltk\n",
    "nltk.data.path.append(\"./data/\")\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from doc_utils import documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your files\n",
    "\n",
    "You can load a mix of csv, pdf and docx files. If you want to train on a CSV file, set the target_column_name variable below to the ID column of the CSV file. The ID column must contain consecutive integers from 0 to num_ids - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_column_name = \"query\"\n",
    "target_column_name = \"id\"\n",
    "\n",
    "combined_pdfs = None\n",
    "combined_docxs = None\n",
    "csv_doc = None\n",
    "\n",
    "# This object does the bookkeeping for managing multiple documents\n",
    "doclist = documents.DocList()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     13\u001b[0m df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\n\u001b[1;32m     15\u001b[0m csv_doc \u001b[39m=\u001b[39m documents\u001b[39m.\u001b[39mCSV(\n\u001b[1;32m     16\u001b[0m     path\u001b[39m=\u001b[39mcsv_file,\n\u001b[1;32m     17\u001b[0m     id_col\u001b[39m=\u001b[39mtarget_column_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     display_cols\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdescription\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m doclist\u001b[39m.\u001b[39;49madd_document(csv_doc)\n",
      "File \u001b[0;32m~/Demos/playground/doc_utils/documents.py:362\u001b[0m, in \u001b[0;36mDocList.add_document\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to add document with mismatched id column to doc list.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    361\u001b[0m new_doc_df \u001b[39m=\u001b[39m document\u001b[39m.\u001b[39mget_new_samples_dataframe() \n\u001b[0;32m--> 362\u001b[0m \u001b[39mif\u001b[39;00m (new_doc_df[new_doc_id_col] \u001b[39m<\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_n_new_ids())\u001b[39m.\u001b[39many():\n\u001b[1;32m    363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to add document with invalid id offset to doc list.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    365\u001b[0m expected_begin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_n_new_ids()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/arraylike.py:48\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__lt__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__lt__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49mlt)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/series.py:6092\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6089\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6091\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6092\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6094\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:293\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    292\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:82\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39;49mscalar_compare(x\u001b[39m.\u001b[39;49mravel(), y, op)\n\u001b[1;32m     83\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pandas/_libs/ops.pyx:107\u001b[0m, in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "csv_file = \"sample_catalog.csv\"\n",
    "\n",
    "# Visualize the dataframe and get the column names in the csv_file.\n",
    "# Your target column (id_col) name has to match the target column in the model defined above (we are using target_column_name across the notebook)\n",
    "# You will have to pick your choice of strong_columns and weak_columns for the train step shown next.\n",
    "# Strong columns are usually the most important ones like titles of documents, keywords, categories etc\n",
    "# Weak columns are usually the long descriptions\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 700\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "df.iloc[0:1]\n",
    "\n",
    "csv_doc = documents.CSV(\n",
    "    path=csv_file,\n",
    "    id_col=target_column_name,\n",
    "    strong_cols=['title', 'brand'],\n",
    "    weak_cols=['description'],\n",
    "    display_cols=['description'],\n",
    ")\n",
    "\n",
    "doclist.add_document(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: PDF or DOCX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['mutual_nda_teamplate_for_testing.pdf']\n",
    "\n",
    "pdfs = [name for name in filenames if name.endswith(\".pdf\")]\n",
    "docxs = [name for name in filenames if name.endswith(\".docx\")]\n",
    "\n",
    "if len(pdfs)>0:\n",
    "    combined_pdfs = documents.PDF(\n",
    "        files=pdfs, \n",
    "        expected_id_col=target_column_name,\n",
    "        hash_to_id_offset=doclist.get_source_hash_to_id_offset_map(),\n",
    "        next_id_offset=doclist.get_n_new_ids(),\n",
    "    )\n",
    "    doclist.add_document(combined_pdfs)\n",
    "\n",
    "if len(docxs)>0:\n",
    "    combined_docxs = documents.DOCX(\n",
    "        files=docxs, \n",
    "        expected_id_col=target_column_name,\n",
    "        hash_to_id_offset=doclist.get_source_hash_to_id_offset_map(),\n",
    "        next_id_offset=doclist.get_n_new_ids(),\n",
    "    )\n",
    "    doclist.add_document(combined_docxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "#### Option 1: Define a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bolt.UniversalDeepTransformer(\n",
    "    data_types = {\n",
    "        \"query\": bolt.types.text(tokenizer=\"char-4\"),\n",
    "        target_column_name: bolt.types.categorical(delimiter=\":\"),\n",
    "    },\n",
    "    target=target_column_name,\n",
    "    n_target_classes=doclist.get_n_new_ids(),\n",
    "    integer_target=True,\n",
    "    options={\n",
    "        \"fhr\": 50000,\n",
    "        \"embedding_dimension\": 2048,\n",
    "        \"extreme_classification\": True,\n",
    "        \"extreme_output_dim\": 50000,\n",
    "        \"rlhf\":True,\n",
    "    }\n",
    ")\n",
    "\n",
    "lr = 0.005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Load from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_checkpoint = \"/Users/tharunkr/qt-app/base_models/checkpoints_without_bias/msmarco_0_reindexes_frozen.bolt\"\n",
    "\n",
    "model = bolt.UniversalDeepTransformer.load(path_to_checkpoint)\n",
    "\n",
    "model.clear_index()\n",
    "\n",
    "for doc in [csv_doc, combined_pdfs, combined_docxs]:\n",
    "    if doc:\n",
    "        doc_config = doc.get_config()\n",
    "        model.introduce_documents(\n",
    "            doc_config.introduction_dataset,\n",
    "            strong_column_names=doc_config.strong_cols, \n",
    "            weak_column_names=[],\n",
    "            num_buckets_to_sample=16,\n",
    "        )\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in [csv_doc, combined_pdfs, combined_docxs]:\n",
    "    if doc:\n",
    "        doc_config = combined_pdfs.get_config()\n",
    "        metrics = model.cold_start(\n",
    "            filename=doc_config.unsupervised_dataset,\n",
    "            strong_column_names=doc_config.strong_cols,\n",
    "            weak_column_names=doc_config.weak_cols,\n",
    "            epochs=10,\n",
    "            learning_rate=lr,\n",
    "            metrics=[\"precision@5\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many search results do you want to retrieve from your files for every query\n",
    "N_REFERENCES = 5\n",
    "\n",
    "model.set_decode_params(min(doclist.get_n_new_ids(), N_REFERENCES), min(doclist.get_n_new_ids(), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above BOLT model. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other model of your choice. You can choose to use an on-prem open source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.qaprompts import qa_prompt, make_chain\n",
    "\n",
    "your_openai_key = \"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1, \n",
    "    openai_api_key=your_openai_key,\n",
    ")\n",
    "\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parties involved in this Confidentiality Agreement are ACME, dba ToTheMoon Inc., and StarWars, dba ToTheMars. (Context)\n"
     ]
    }
   ],
   "source": [
    "query = \"parties involved\"\n",
    "\n",
    "reference_ids = model.predict({\"query\":query})\n",
    "reference_ids = [itm[0] for itm in reference_ids]\n",
    "references = [doclist.get_new_display_items().iloc[p] for p in reference_ids]\n",
    "\n",
    "answer = qa_chain.run(question=query, context_str='\\n\\n'.join(references[:3]), length=\"abt 50 words\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to teach your model (RLHF)\n",
    "\n",
    "This is one of the marquee features that we provide. Thanks to our efficient training capabilties, we can offer you to teach the retrieval model to correct itself in the event of it not being able to get the correct paragraphs from the document. \n",
    "\n",
    "Also, the RLHF teachings done a model will generalize beyond the current documents if we run *model.clear_index()* and introduce new documents. This is because our engine has an elastic output space that adapts to the contents of new documents.\n",
    "\n",
    "To do RLHF, we provide two functions:\n",
    "\n",
    "1. Associate: Using this function, you can associate two phrases to give similar results. For examples, assume you're in the contract review domain. And you're interested in asking a question like \"who are the parties involved in this contract?\". However, most contracts have the phrase \"made by and between\" to suggest the parties involved in the contracts (like \"this agreement is made by and between company A and company B\"). In this scenario, you can simply call *model.associate([\"parties involved\",\"made by and between\"])* and the model would learn the relation. In the subsequent documents, you're more likely to retrieve the passage containing the correct information.\n",
    "\n",
    "2. Upvote: Let's say you searched for a query \"is there a limited liability clause?\" and you got 5 search results (along with their passage IDs). If you know that the corect result is actually the 2nd one instead of the first one. Then you can simpley call *model.upvote(\"is there a limited liability clause\",passage_id_of_the_best_search_result)*.\n",
    "\n",
    "We provide two interfaces to do the teaching.\n",
    "\n",
    "1. You can simply teach the model as shown below in python with model.associate() and model.upvote() calls. Refer to the \"RLHF using Python functions\".\n",
    "\n",
    "2. You can save a checkpoint of your trained model and export it to our Playground web-app to do QnA and teaching on an intuitive UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. RLHF using function calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_samples = [({\"query\":\"parties_involved\"},{\"query\":\"made by and between\"})]\n",
    "\n",
    "model.associate(rlhf_samples, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Export your model to Playground App\n",
    "\n",
    "ThirdAI's playground is a dockerized Gradio app that you can run on your laptop and use any model checkpoint to do QnA and teach using the above mentioned functions. \n",
    "\n",
    "Before you save your checkpoint, please go through the following short video tutorial to install Docker Desktop and download our image and run the webapp through a container.\n",
    "\n",
    "https://drive.google.com/file/d/16tI1OAm2Lu0OuUOCiJzGrTjiBZejJWs3/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coming Soon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you save the checkpoint, please copy the .zip file to the folder from where you're running the docker container. And then go through this short video tutorial to do QnA and teach.\n",
    "\n",
    "https://drive.google.com/file/d/1WIt2-EpYkQJpFgFiUXbc_iYU9uhOJdMn/view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
