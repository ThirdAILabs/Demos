{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's Playground\n",
    "\n",
    "In this notebook, we will show \n",
    "\n",
    "1. How to easily build a semantic QnA engine for all your documents with ThirdAI's BOLT engine.\n",
    "\n",
    "2. (Optional) How to use your OpenAI key to get retrieval augmented answers from OpenAI.\n",
    "\n",
    "3. How to teach your retrieval model with RLHF.\n",
    "\n",
    "4. (Optional) How to save your models and export to ThirdAI's Playground web-app to do interative QnA and teach your model with RLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thirdai's license activation\n",
    "\n",
    "import thirdai\n",
    "try:\n",
    "    thirdai.licensing.activate(\"\")\n",
    "except:\n",
    "    print(\"You need a license key to use ThirdAI's library. Please request a trial license at https://www.thirdai.com/try-bolt/\")\n",
    "\n",
    "thirdai.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "import os\n",
    "import nltk\n",
    "nltk.data.path.append(\"./data/\")\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from doc_utils import documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your files\n",
    "\n",
    "You can load a mix of csv, pdf and docx files. If you want to train on a CSV file, set the target_column_name variable below to the ID column of the CSV file. The ID column must contain consecutive integers from 0 to num_ids - 1.\n",
    "\n",
    "Also, if you're loading from a pre-trained checkpoint, the query_col_name and target_col_name should match the ones used for that model. For the checkpoints that ThirdAI provides, we standardize the query_column_name to \"QUERY\" and the target_column_name to \"DOC_ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_column_name = \"QUERY\"\n",
    "target_column_name = \"DOC_ID\"\n",
    "\n",
    "combined_pdfs = None\n",
    "combined_docxs = None\n",
    "csv_doc = None\n",
    "\n",
    "# This object does the bookkeeping for managing multiple documents\n",
    "doclist = documents.DocList()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC_ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0\n",
      "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN\n",
      "description      Inductive study compares related Bible texts in order to let the Bible interpret itself.  rather than approaching Scripture with predetermined notions of what it will say.  Dr. Trainas Methodical Bible Study was not intended to be the last word in inductive Bible study; but since its first publication in 1952.  it has become a foundational text in this field.  Christian colleges and seminaries have made it required reading for beginning Bible students.  while many churches have used it for their lay Bible study groups.  Dr. Traina summarizes its success in this comment:  \"If the truths of the Bible already resided in man.  there would be no need for the Bible and this manual would be sup...\n",
      "bullet_points                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            NaN\n",
      "brand                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    NaN\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"sample_catalog.csv\"\n",
    "\n",
    "# Visualize the dataframe and get the column names in the csv_file.\n",
    "# Your target column (id_col) name has to match the target column in the model defined above (we are using target_column_name across the notebook)\n",
    "# You will have to pick your choice of strong_columns and weak_columns for the train step shown next.\n",
    "# Strong columns are usually the most important ones like titles of documents, keywords, categories etc\n",
    "# Weak columns are usually the long descriptions\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 700\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_doc = documents.CSV(\n",
    "    path=csv_file,\n",
    "    id_col=target_column_name,\n",
    "    strong_cols=['title', 'brand'],\n",
    "    weak_cols=['description'],\n",
    "    display_cols=['description'],\n",
    ")\n",
    "\n",
    "doclist.add_document(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: PDF or DOCX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['mutual_nda_teamplate_for_testing.pdf']\n",
    "\n",
    "pdfs = [name for name in filenames if name.endswith(\".pdf\")]\n",
    "docxs = [name for name in filenames if name.endswith(\".docx\")]\n",
    "\n",
    "if len(pdfs)>0:\n",
    "    combined_pdfs = documents.PDF(\n",
    "        files=pdfs, \n",
    "        expected_id_col=target_column_name,\n",
    "        hash_to_id_offset=doclist.get_source_hash_to_id_offset_map(),\n",
    "        next_id_offset=doclist.get_n_new_ids(),\n",
    "    )\n",
    "    doclist.add_document(combined_pdfs)\n",
    "\n",
    "if len(docxs)>0:\n",
    "    combined_docxs = documents.DOCX(\n",
    "        files=docxs, \n",
    "        expected_id_col=target_column_name,\n",
    "        hash_to_id_offset=doclist.get_source_hash_to_id_offset_map(),\n",
    "        next_id_offset=doclist.get_n_new_ids(),\n",
    "    )\n",
    "    doclist.add_document(combined_docxs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "#### Option 1: Define a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bolt.UniversalDeepTransformer(\n",
    "    data_types = {\n",
    "        query_column_name: bolt.types.text(tokenizer=\"char-4\"),\n",
    "        target_column_name: bolt.types.categorical(delimiter=\":\"),\n",
    "    },\n",
    "    target=target_column_name,\n",
    "    n_target_classes=doclist.get_n_new_ids(),\n",
    "    integer_target=True,\n",
    "    options={\n",
    "        \"fhr\": 50000,\n",
    "        \"embedding_dimension\": 2048,\n",
    "        \"extreme_classification\": True,\n",
    "        \"extreme_output_dim\": 50000,\n",
    "        \"rlhf\":True,\n",
    "    }\n",
    ")\n",
    "\n",
    "lr = 0.005"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Load from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc_utils.documents.CSV object at 0x177c31330>\n",
      "loading data | source 'sample_catalog.csv'\n",
      "loading data | source 'sample_catalog.csv' | vectors 0 | batches 0 | time 0s | complete\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Did not find any data to load from the data source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[39mprint\u001b[39m(doc)\n\u001b[1;32m     11\u001b[0m         doc_config \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39mget_config()\n\u001b[0;32m---> 12\u001b[0m         model\u001b[39m.\u001b[39;49mintroduce_documents(\n\u001b[1;32m     13\u001b[0m             doc_config\u001b[39m.\u001b[39;49mintroduction_dataset,\n\u001b[1;32m     14\u001b[0m             strong_column_names\u001b[39m=\u001b[39;49mdoc_config\u001b[39m.\u001b[39;49mstrong_cols, \n\u001b[1;32m     15\u001b[0m             weak_column_names\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m     16\u001b[0m             num_buckets_to_sample\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         )\n\u001b[1;32m     19\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/thirdai/bolt/udt_modifications.py:148\u001b[0m, in \u001b[0;36mmodify_mach_udt.<locals>.wrapped_introduce_documents\u001b[0;34m(self, filename, strong_column_names, weak_column_names, num_buckets_to_sample)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_introduce_documents\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     num_buckets_to_sample: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    145\u001b[0m ):\n\u001b[1;32m    146\u001b[0m     data_source \u001b[39m=\u001b[39m _create_data_source(filename)\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mreturn\u001b[39;00m original_introduce_documents(\n\u001b[1;32m    149\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    150\u001b[0m         data_source,\n\u001b[1;32m    151\u001b[0m         strong_column_names,\n\u001b[1;32m    152\u001b[0m         weak_column_names,\n\u001b[1;32m    153\u001b[0m         num_buckets_to_sample,\n\u001b[1;32m    154\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Did not find any data to load from the data source."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.system(\"wget -O msmarco.bolt https://www.dropbox.com/s/sd1vxsg8v6d2u2r/msmarco_0_reindexes.bolt?dl=0\")\n",
    "\n",
    "# model = bolt.UniversalDeepTransformer.load(\"msmarco.bolt\")\n",
    "\n",
    "model.clear_index()\n",
    "\n",
    "for doc in [csv_doc, combined_pdfs, combined_docxs]:\n",
    "    if doc:\n",
    "        print(doc)\n",
    "        doc_config = doc.get_config()\n",
    "        model.introduce_documents(\n",
    "            doc_config.introduction_dataset,\n",
    "            strong_column_names=doc_config.strong_cols, \n",
    "            weak_column_names=[],\n",
    "            num_buckets_to_sample=16,\n",
    "        )\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Inductive study compares related Bible texts in order to let the Bible interpret itself.  rather than approaching Scripture with predetermined notions of what it will say.  Dr. Trainas Methodical Bible Study was not intended to be the last word in inductive Bible study; but since its first publication in 1952.  it has become a foundational text in this field.  Christian colleges and seminaries have made it required reading for beginning Bible students.  while many churches have used it for their lay Bible study groups.  Dr. Traina summarizes its success in this comment:  \"If the truths of the Bible already resided in man.  there would be no need for the Bible and this manual would be sup...\n",
       "1                                                                                                                                                                                                           GeoPuzzles are jigsaw puzzles that make learning world geography fun. The pieces of a GeoPuzzle are shaped like individual countries.  so children learn as they put the puzzle together. Award-winning Geopuzzles help to build fine motor.  cognitive.  language.  and problem-solving skills.  and are a great introduction to world geography for children 4 and up. Designed by an art professor.  jumbo sized and brightly colored GeoPuzzles are available for 5 major continents.. Geopuzzle U.S. and Canada\n",
       "2    Just a few decades ago.  owing more money than you had in your  bank account was the exception.  not the rule.  Yet.  in the last 10 years.   consumer debt has doubled and.  for the first time.  Americans are spending  more than they're saving -- or making.  This April.  award-winning former  ABC News and CNN producer Danny Schechter investigates America's mounting  debt crisis in his latest hard-hitting expose.  IN DEBT WE TRUST.   While many Americans are \"maxing out\" on credit cards.  there is a  much deeper story: power is shifting into fewer hands...with frightening  consequences.  IN DEBT WE TRUST reveals a hitherto unknown cabal of credit  card companies.  lobbyists.  media conglo...\n",
       "3                                                                                                                           This black oil pressure gauge is engineered to provide extremely accurate readings under harsh conditions and measures the pressure of oil circulating through the engine. This oil pressure gauge has a 90 degree sweep.  0-100 PSI range.  and has a backlit dial face with Boudon tube meter movement. Air core movements and simple three wire installation are also included. This gauge is easy to hook-up and mount in your car and gives an advanced warning of incorrect oil pressure.  low oil level.  defective oil pump.  restricted oil lines.  clogged oil filter.  and low oil level.\n",
       "4    This polyoxymethylene spur gear with 20-degree pressure angle PA and a diametral pitch DP of 20 transmits torque between parallel shafts to provide rotational power to another part of a motorized system. It has a 20-degree pressure angle for lower contact ratio between gears.  resulting in higher load capacity than smaller pressure angles.  and a 20-diametral pitch for applications requiring a balance between transferring torque and controlling changes in speed and direction of rotation with precision. This spur gear.  which can mesh only with spur gears that have the same pressure angle and diametral pitch.  is made of polyoxymethylene for quiet operation.  low friction and moisture abs...\n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Michael Fellman is Professor of History at Simon Fraser University and author of Citizen Sherman: A Life of William Tecumseh Sherman.\n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    2 CD import\n",
       "7    Mr. Mojo Risin': The Story of L.A. Woman' is the story of the making of the Doors' last album with Jim Morrison \"L.A. Woman\". 2011 is the 40th anniversary of the album's release and this program goes into detail of how the album came about.  its recording and what was happening to the band at the time. The story is told through new interviews with the three surviving Doors: Ray Manzarek.  Robbie Krieger and John Densmore plus contributions from Jac Holzman.  founder of their label Elektra Records.  Bill Siddons.  who was their manager.  Bruce Botnick.  engineer and co-producer of the album and others associated with the Doors at this time. The show includes archive footage of the Doors p...\n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Ori. Release '95.  Fourth studio album from the Gothic Rock duo .\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_catalog.csv\")\n",
    "df['description']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid entity in index: 23.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m doc:\n\u001b[1;32m      3\u001b[0m     doc_config \u001b[39m=\u001b[39m combined_pdfs\u001b[39m.\u001b[39mget_config()\n\u001b[0;32m----> 4\u001b[0m     metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mcold_start(\n\u001b[1;32m      5\u001b[0m         filename\u001b[39m=\u001b[39;49mdoc_config\u001b[39m.\u001b[39;49munsupervised_dataset,\n\u001b[1;32m      6\u001b[0m         strong_column_names\u001b[39m=\u001b[39;49mdoc_config\u001b[39m.\u001b[39;49mstrong_cols,\n\u001b[1;32m      7\u001b[0m         weak_column_names\u001b[39m=\u001b[39;49mdoc_config\u001b[39m.\u001b[39;49mweak_cols,\n\u001b[1;32m      8\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m     10\u001b[0m         metrics\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mprecision@5\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/thirdai/bolt/udt_modifications.py:113\u001b[0m, in \u001b[0;36mmodify_udt.<locals>.wrapped_cold_start\u001b[0;34m(self, filename, strong_column_names, weak_column_names, learning_rate, epochs, metrics, validation, callbacks, max_in_memory_batches, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m data_source \u001b[39m=\u001b[39m _create_data_source(filename)\n\u001b[1;32m    111\u001b[0m validation \u001b[39m=\u001b[39m _convert_validation(validation)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m original_cold_start(\n\u001b[1;32m    114\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    115\u001b[0m     data\u001b[39m=\u001b[39;49mdata_source,\n\u001b[1;32m    116\u001b[0m     strong_column_names\u001b[39m=\u001b[39;49mstrong_column_names,\n\u001b[1;32m    117\u001b[0m     weak_column_names\u001b[39m=\u001b[39;49mweak_column_names,\n\u001b[1;32m    118\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    119\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    120\u001b[0m     metrics\u001b[39m=\u001b[39;49mmetrics,\n\u001b[1;32m    121\u001b[0m     validation\u001b[39m=\u001b[39;49mvalidation,\n\u001b[1;32m    122\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    123\u001b[0m     max_in_memory_batches\u001b[39m=\u001b[39;49mmax_in_memory_batches,\n\u001b[1;32m    124\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    125\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid entity in index: 23."
     ]
    }
   ],
   "source": [
    "for doc in [csv_doc, combined_pdfs, combined_docxs]:\n",
    "    if doc:\n",
    "        doc_config = combined_pdfs.get_config()\n",
    "        metrics = model.cold_start(\n",
    "            filename=doc_config.unsupervised_dataset,\n",
    "            strong_column_names=doc_config.strong_cols,\n",
    "            weak_column_names=doc_config.weak_cols,\n",
    "            epochs=10,\n",
    "            learning_rate=lr,\n",
    "            metrics=[\"precision@5\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many search results do you want to retrieve from your files for every query\n",
    "N_REFERENCES = 5\n",
    "\n",
    "model.set_decode_params(min(doclist.get_n_new_ids(), N_REFERENCES), min(doclist.get_n_new_ids(), 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above BOLT model. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other model of your choice. You can choose to use an on-prem open source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.qaprompts import qa_prompt, make_chain\n",
    "\n",
    "your_openai_key = \"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\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1, \n",
    "    openai_api_key=your_openai_key,\n",
    ")\n",
    "\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query):\n",
    "    reference_ids = model.predict({\"QUERY\":query})\n",
    "    reference_ids = [itm[0] for itm in reference_ids]\n",
    "    references = [doclist.get_new_display_items().iloc[p] for p in reference_ids]\n",
    "    return references\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return qa_chain.run(question=query, context_str='\\n\\n'.join(references[:3]), length=\"abt 50 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[DRAFT FOR TESTING PURPOSES ONLY] CONFIDENTIAL CONFIDENTIALITY AGREEMENT This Confidentiality Agreement (the “Agreement”) is made by and between ACME. dba ToTheMoon Inc. with offices at 2025 Guadalupe St. Suite 260 Austin TX 78705 and StarWars dba ToTheMars with offices at the forest moon of Endor and entered as of May 3 2023 (“Effective Date”).', 'In consideration of the business discussions disclosure of Confidential Information and any future business relationship between the parties it is hereby agreed as follows: 1. CONFIDENTIAL INFORMATION. For purposes of this Agreement the term “Confidential Information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process Request for Proposal (RFP) or Request for Information (RFI) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the Effective Date.', '2. NEED TO KNOW. The receiving party shall limit its disclosure of the other party’s Confidential Information to those of its officers and employees and subcontractors (i) to which such disclosure is necessary for purposes of the discussions contemplated by this Agreement and (ii) who have agreed in writing to be bound by provisions no less restrictive than those set forth in this Agreement.', 'Each party shall take all reasonable measures to preserve the confidentiality and avoid the disclosure of the other party’s Confidential Information including but not limited to those steps taken with respect to the party’s own Confidential Information of like importance. Neither party shall disassemble decompile or otherwise reverse engineer any software product of the other party and to the extent any such activity may be permitted the results thereof shall be deemed Confidential Information subject to the requirements of this Agreement.', '12. ENTIRE AGREEMENT. This Agreement constitutes the entire agreement with respect to the subject matter hereof and supersedes all prior agreements and understandings between the parties (whether written or oral) relating to the subject matter and may not be amended or modified except in a writing signed by an authorized representative of both parties. The terms of this Agreement relating to the confidentiality and non-use of Confidential Information shall continue after the termination of this Agreement for a period of the longer of (i) five (5) years or (ii) when the Confidential Information no longer qualifies as a trade secret under applicable law.']\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the effective date of this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The effective date of this Confidentiality Agreement is May 3, 2023 (ACME, dba ToTheMoon Inc. and StarWars dba ToTheMars, 2023).\n"
     ]
    }
   ],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask a query that the model gets it wrong. Subsequently, let's teach the model to correct itself using our RLHF methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date of signing is not provided in the given context. (I cannot answer)\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the date of signing ?\"\n",
    "references = get_references(query)\n",
    "answer = get_answer(query, references)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to teach your model (RLHF)\n",
    "\n",
    "This is one of the marquee features that we provide. Thanks to our efficient training capabilties, we can offer you to teach the retrieval model to correct itself in the event of it not being able to get the correct paragraphs from the document. \n",
    "\n",
    "Also, the RLHF teachings done a model will generalize beyond the current documents if we run *model.clear_index()* and introduce new documents. This is because our engine has an elastic output space that adapts to the contents of new documents.\n",
    "\n",
    "To do RLHF, we provide two functions:\n",
    "\n",
    "1. Associate: Using this function, you can associate two phrases to give similar results. For examples, assume you're in the contract review domain. And you're interested in asking a question like \"who are the parties involved in this contract?\". However, most contracts have the phrase \"made by and between\" to suggest the parties involved in the contracts (like \"this agreement is made by and between company A and company B\"). In this scenario, you can simply call *model.associate([\"parties involved\",\"made by and between\"])* and the model would learn the relation. In the subsequent documents, you're more likely to retrieve the passage containing the correct information.\n",
    "\n",
    "2. Upvote: Let's say you searched for a query \"is there a limited liability clause?\" and you got 5 search results (along with their passage IDs). If you know that the corect result is actually the 2nd one instead of the first one. Then you can simpley call *model.upvote(\"is there a limited liability clause\",passage_id_of_the_best_search_result)*.\n",
    "\n",
    "We provide two interfaces to do the teaching.\n",
    "\n",
    "1. You can simply teach the model as shown below in python with model.associate() and model.upvote() calls. Refer to the \"RLHF using Python functions\".\n",
    "\n",
    "2. You can save a checkpoint of your trained model and export it to our Playground web-app to do QnA and teaching on an intuitive UI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1. RLHF using function calls \n",
    "\n",
    "In the above example, the model could not understand that the phrase \"parties involved\". But if you an expert in contracts, you know that \"parties involved\" usually goes with phrases like \"made by and between\" (for example, \"This Confidentiality Agreement (the “Agreement”) is made by and between ...\"). So, let's teach the model that these two phrases should retrieve similar passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_samples = [({\"QUERY\":\"date of signing\"},{\"QUERY\":\"duly executed\"})]\n",
    "\n",
    "model.associate(rlhf_samples, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's query the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date of signing of this agreement is May 7, 2023. (InkPaper ID: BynBSPWS6DaRfuSP_w1OE9X37-J6yQfQwbSFrpp2nqY)\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the date of signing of this agreement?\"\n",
    "references = get_references(query)\n",
    "answer = get_answer(query, references)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Export your model to Playground App\n",
    "\n",
    "ThirdAI's playground is a dockerized Gradio app that you can run on your laptop and use any model checkpoint to do QnA and teach using the above mentioned functions. \n",
    "\n",
    "Before you save your checkpoint, please go through the following short video tutorial to install Docker Desktop and download our image and run the webapp through a container.\n",
    "\n",
    "https://drive.google.com/file/d/16tI1OAm2Lu0OuUOCiJzGrTjiBZejJWs3/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coming Soon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you save the checkpoint, please copy the .zip file to the folder from where you're running the docker container. And then go through this short video tutorial to do QnA and teach.\n",
    "\n",
    "https://drive.google.com/file/d/1WIt2-EpYkQJpFgFiUXbc_iYU9uhOJdMn/view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
