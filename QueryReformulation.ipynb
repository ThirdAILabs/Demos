{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Real-Time Query Reformulation a.k.a Query Correction using the UniversalDeepTransformer API\n",
    "\n",
    "This notebook shows how to build a query reformulation model with ThirdAI's Universal Deep Transformer (UDT) model, our all-purpose solution for classification tasks on tabular datasets and query reformulation. In this demo, we will train and evaluate the model on a spelling correction dataset and show less than 5ms P99.9 inference latency.\n",
    "\n",
    "You can immediately run a version of this notebook in your browser on Google Colab at the following link:\n",
    "\n",
    "https://githubtocolab.com/ThirdAILabs/Demos/blob/main/QueryReformulation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.6.1 in /share/shubh/myenv/lib/python3.8/site-packages (2.6.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (2022.11.0)\n",
      "Requirement already satisfied: multiprocess in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (0.70.14)\n",
      "Requirement already satisfied: responses<0.19 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (0.18.0)\n",
      "Requirement already satisfied: packaging in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (22.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (1.5.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (6.0)\n",
      "Requirement already satisfied: aiohttp in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (3.8.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /share/shubh/myenv/lib/python3.8/site-packages (from datasets==2.6.1) (1.23.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /share/shubh/myenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.6.1) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /share/shubh/myenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.6.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /share/shubh/myenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.6.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/shubh/myenv/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.6.1) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /share/shubh/myenv/lib/python3.8/site-packages (from pandas->datasets==2.6.1) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /share/shubh/myenv/lib/python3.8/site-packages (from pandas->datasets==2.6.1) (2.8.2)\n",
      "Requirement already satisfied: filelock in /share/shubh/myenv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.6.1) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /share/shubh/myenv/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.6.1) (4.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /share/shubh/myenv/lib/python3.8/site-packages (from aiohttp->datasets==2.6.1) (4.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /share/shubh/myenv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.6.1) (1.16.0)\n",
      "Requirement already up-to-date: thirdai in /share/shubh/myenv/lib/python3.8/site-packages (0.5.13)\n",
      "Requirement already satisfied, skipping upgrade: numpy<=1.23.5 in /share/shubh/myenv/lib/python3.8/site-packages (from thirdai) (1.23.5)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /share/shubh/myenv/lib/python3.8/site-packages (from thirdai) (4.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas>=1.2.0 in /share/shubh/myenv/lib/python3.8/site-packages (from thirdai) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.1 in /share/shubh/myenv/lib/python3.8/site-packages (from pandas>=1.2.0->thirdai) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /share/shubh/myenv/lib/python3.8/site-packages (from pandas>=1.2.0->thirdai) (2022.7)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /share/shubh/myenv/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->thirdai) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets==2.6.1 \n",
    "!pip3 install thirdai --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses an activation key that will only work with this demo. If you want to try us out on your own dataset, you can obtain a free trial license at the following link: https://www.thirdai.com/try-bolt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thirdai\n",
    "thirdai.licensing.activate(\"AWK9-WPMK-3NRE-AAAV-C39P-N9JV-43VC-CFUH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "\n",
    "We will use the demos module in the ThirdAI repo to download and pre-process a dataset from HuggingFace. The dataset we will use from HuggingFace is typically used for semantic sentence similarity. We will pre-process it by adding noise so that it is suitable for query reformulation. You can replace this step and the next with a UDT initialization that is specific for your dataset - as long as your train dataset consists of **CSV files with two string columns**: The first one should be incorrect queries and the second column will be their target reformulations.  The incorrect queries column can be empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/shubh/myenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration embedding-data--sentence-compression-d643585deb6e0073\n",
      "Found cached dataset json (/home/shubh/.cache/huggingface/datasets/embedding-data___json/embedding-data--sentence-compression-d643585deb6e0073/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 260.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from thirdai.demos import prepare_query_reformulation_data\n",
    "\n",
    "supervised_train_filename, unsupervised_train_filename, test_filename, inference_batch = prepare_query_reformulation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDT Initialization\n",
    "\n",
    "We can create a UDT model specific for query reformulation by specifying the name of the source column (column containing queries to be reformulated) and the name of the target column (correct reformulations) and a dataset size parameter. The size of the input dataset can be configured to be either \"small\" (size < 1M), \"medium\"(size < 10M) or \"large\" (size >= 10M). We configure different model parameters depending on the size of the input dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "model = bolt.UniversalDeepTransformer(\n",
    "    source_column=\"source_queries\", target_column=\"target_queries\", dataset_size=\"medium\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can now train our model in just one line of code. You just have to specify the path to the training file. \n",
    "You can train the model in both **supervised** and **unsupervised** settings. When training the model in an unsupervised setting, only the target column is needed in the training file. \n",
    "\n",
    "You can pretrain your model by training in an unsupervised setting and directly evaluate it.\n",
    "*unsupervised_train_filename* has only `target_queries` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'unsupervised_train_file.csv' | vectors 900000 | batches 90 | time 0s | complete\n",
      "\n",
      "train | batches 90 | complete                                           ======================                        ] 51%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unsupervised training\n",
    "model.train(filename=unsupervised_train_filename) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "Evaluating the UDT model is also just one line of code. Since this UDT model is specific for query reformulation, you need to provide the number of suggested candidate queries that the UDT model generates. For instance, if you want to see the top 5 suggested query reformulations of the input query, set the `top_k` parameter to 5. Evaluating this model will also print out recall @k. If you also want the scores for the reformulated queries, you can specify `return_scores = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'test_file.csv' | vectors 270000 | batches 27 | time 0s | complete\n",
      "\n",
      "evaluate | batches 27 | complete                                           \n",
      "\n",
      "Recall@5 = 0.840726\n"
     ]
    }
   ],
   "source": [
    "query_reformulations, scores = model.evaluate(filename=test_filename, top_k=5, return_scores = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have supervised data, i.e., `source_queries`,`target_queries` pairs, then you can train the same model in a supervised setting. The APIs for training and evaluation remains the same. \n",
    "_supervised_train_filename_ has two columns namely `source_queries`, `target_queries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'supervised_train_file.csv' | vectors 900000 | batches 90 | time 0s | complete\n",
      "\n",
      "train | batches 90 | complete                                           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# supervised training\n",
    "model.train(filename=supervised_train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'test_file.csv' | vectors 270000 | batches 27 | time 0s | complete\n",
      "\n",
      "evaluate | batches 27 | complete                =============] 100%                           \n",
      "\n",
      "Recall@5 = 0.884885\n"
     ]
    }
   ],
   "source": [
    "query_reformulations, = model.evaluate(filename=test_filename, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading\n",
    "\n",
    "Saving and loading a trained UDT model to disk is also extremely straight forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = \"query_reformulation.model\"\n",
    "\n",
    "# Saving\n",
    "model.save(filename=model_location)\n",
    "\n",
    "# Loading\n",
    "model = bolt.UniversalDeepTransformer.load(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Predictions \n",
    "\n",
    "The evaluation method is great for testing, but it requires labels, which don't exist in a production environment. We also provide a predict method that can take a list of queries or a single query, which allows for easy integration into production pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, = model.predict_batch(queries=inference_batch, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Health officials issue red air quality alert',\n",
       "   'Air quality improves',\n",
       "   'Ahs lifts air quality advisory',\n",
       "   'Quality Health breaks ground',\n",
       "   'Air quality officials issue health notice']],)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(query=\"Health iiaclfsfo susei der air quality alert\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d1d80f13736053df7ace2f5c9aab3c44d17c860055f86d870fc646b71059e26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
