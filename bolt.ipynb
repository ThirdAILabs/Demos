{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BOLT\n",
    "## Basics.\n",
    "Let's learn to use the BOLT Python API with an exercise. We'll do a simple image classification task on the MNIST dataset. Given 28 by 28 pixel images of handwritten numbers from 0 through 9, predict which number is being drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 60000 vectors from datasets/mnist/mnist in 0 seconds\n",
      "Read 10000 vectors from datasets/mnist/mnist.t in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO(Geordie): Add download scripts and change to relative path\n",
    "from thirdai import dataset\n",
    "\n",
    "mnist_train = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"datasets/mnist/mnist\", \n",
    "    batch_size=256)\n",
    "\n",
    "mnist_test = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"datasets/mnist/mnist.t\", \n",
    "    batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this task, we want to build a simple neural network with these specifications:\n",
    "* 784 (28 x 28) input dimension\n",
    "* A single 1000-dim hidden layer with ReLU\n",
    "* 10-dim output layer with Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras_layers = [\n",
    "    keras.layers.Dense(\n",
    "        units=1000, \n",
    "        activation='relu', \n",
    "        input_shape=(784,)),\n",
    "        \n",
    "    keras.layers.Dense(\n",
    "        units=10, \n",
    "        activation='softmax')\n",
    "]\n",
    "\n",
    "keras_model = keras.Sequential(layers=keras_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Building Fully Connected Network ======\n",
      "Layer: dim=1000, load_factor=1, act_func=ReLU\n",
      "Layer: dim=10, load_factor=1, act_func=Softmax\n",
      "Initialized Network in 0 seconds\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "mnist_layers = [\n",
    "    bolt.LayerConfig(\n",
    "        dim=1000, \n",
    "        activation_function=bolt.ActivationFunctions.ReLU),\n",
    "    \n",
    "    bolt.LayerConfig(\n",
    "        dim=10, \n",
    "        activation_function=bolt.ActivationFunctions.Softmax)\n",
    "]\n",
    "\n",
    "mnist_network = bolt.Network(\n",
    "    layers=mnist_layers, \n",
    "    input_dim=784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the network to minimize categorical cross entropy loss and measure our success with the categorical accuracy metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "[==================================================] 100%\n",
      "Processed 235 training batches in 3 seconds\n",
      "[==================================================] 100%\n",
      "Processed 40 test batches in 497 milliseconds\n",
      "Accuracy: 0.9535 (9535/10000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'test_time': [497.0], 'categorical_accuracy': [0.9535]},\n",
       " array([[1.6119853e-09, 4.9061071e-09, 9.0842747e-09, ..., 9.9999881e-01,\n",
       "         7.4112470e-12, 7.1178043e-08],\n",
       "        [7.8348904e-08, 4.7955339e-04, 7.8698450e-01, ..., 1.3695576e-07,\n",
       "         5.3275298e-06, 2.8925069e-07],\n",
       "        [9.4192680e-09, 9.9960941e-01, 9.1716443e-05, ..., 2.2237067e-04,\n",
       "         1.5032618e-05, 3.1097348e-05],\n",
       "        ...,\n",
       "        [2.6579289e-10, 1.1534446e-09, 7.5483649e-07, ..., 8.0246173e-05,\n",
       "         9.5714662e-05, 2.2119880e-03],\n",
       "        [6.5210443e-05, 3.2664754e-08, 7.9705176e-09, ..., 8.9524441e-08,\n",
       "         5.7288981e-04, 1.3805798e-08],\n",
       "        [4.6801352e-07, 1.0962963e-11, 1.6620123e-06, ..., 2.0000725e-11,\n",
       "         1.0412909e-09, 7.2178027e-11]], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_network.train(\n",
    "    train_data=mnist_train, \n",
    "    loss_fn=bolt.CategoricalCrossEntropyLoss(), \n",
    "    learning_rate=0.001, \n",
    "    epochs=1)\n",
    "\n",
    "mnist_network.predict(\n",
    "    test_data=mnist_test, \n",
    "    metrics=[\"categorical_accuracy\"], \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about bigger models?\n",
    "One example of a more complicated task that requires a larger network is intent classification. To demonstrate that, we have chosen the CLINC150 dataset. It's a corpus of customer queries mapped to their intentions. For example, the dataset may have a query like \"do I have to pay for carry-ons on delta?\", and this query is assigned an intent id, so in this case the intent is \"carry-on\" and it has a unique id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 18100 vectors from datasets/intent_classification/train_shuf.svm in 2 seconds\n",
      "Read 5500 vectors from datasets/intent_classification/test_shuf.svm in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# TODO(Geordie): Add download scripts and change to relative path\n",
    "intent_class_train = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"datasets/intent_classification/train_shuf.svm\", \n",
    "    batch_size=256)\n",
    "\n",
    "intent_class_test = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"datasets/intent_classification/test_shuf.svm\", \n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converted the samples in this dataset into 5000 dimensional sparse input vectors and we'll use 10000 hidden layer. That's a 51 million parameter model so it's quite a big model and it usually takes 100 seconds to train such a model for just one epoch on CPU with other frameworks. This is where we introduce Bolt's unique offering. Take a look at the configuration for the first layer, and see that in addition to dimension and activation function, we now also have the load factor. It's a knob for setting the network's computational budget so you can have the power of deep learning for cheap. Here, we use 0.05 -> 500 neurons out of 10,000 for each input.\n",
    "\n",
    "And it's not just any 500 neurons like you would get with something like dropouts. It's the 500 most important neurons for each input sample, so Bolt curates a small network for individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_layers = [\n",
    "    bolt.LayerConfig(\n",
    "        dim=10000, \n",
    "        load_factor=0.05, \n",
    "        activation_function=bolt.ActivationFunctions.ReLU),\n",
    "    \n",
    "    bolt.LayerConfig(\n",
    "        dim=151, \n",
    "        activation_function=bolt.ActivationFunctions.Softmax)\n",
    "]\n",
    "\n",
    "bigger_network = bolt.Network(\n",
    "    layers=bigger_layers, \n",
    "    input_dim=5512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse inference\n",
    "You can also use sparsity to accelerate inference. And you can do it with just one method call. Simply call `network.enable_sparse_inference()`, and the next time the model does inference, it will only use the computational budget that you set in the load factor. Unlike other sparse techniques that involve pruning or quantization, Bolt's sparse inference is strongly tied to its sparse training because it's aware of inference sparsity during training, and optimizes for sparse inference directly, leading to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_network.train(\n",
    "    train_data=intent_class_train, \n",
    "    loss_fn=bolt.CategoricalCrossEntropyLoss(), \n",
    "    learning_rate=0.001, \n",
    "    epochs=2)\n",
    "\n",
    "bigger_network.enable_sparse_inference()\n",
    "\n",
    "bigger_network.train(\n",
    "    train_data=intent_class_train, \n",
    "    loss_fn=bolt.CategoricalCrossEntropyLoss(), \n",
    "    learning_rate=0.001, \n",
    "    epochs=1)\n",
    "\n",
    "bigger_network.predict(\n",
    "    test_data=intent_class_test, \n",
    "    metrics=[\"categorical_accuracy\"], \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does this enable?\n",
    "Now that we have shown you how well training time scales with increasingly larger models, we want to show you what you can do with an even larger model. This time, we'll do sentiment classification on the Yelp Reviews dataset. So the task is, you take a sentence, a restaurant review in this case, and predict whether it has a positive sentiment, or a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file '../sa_demo/text_data/yelp_review_full_2class_train.svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/benitogeordie/Desktop/Demos/bolt.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=0'>1</a>\u001b[0m train_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mload_bolt_svm_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=1'>2</a>\u001b[0m     filename\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../sa_demo/text_data/yelp_review_full_2class_train.svm\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=2'>3</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=4'>5</a>\u001b[0m test_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mload_bolt_svm_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=5'>6</a>\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../sa_demo/text_data/yelp_review_full_2class_test.svm\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/benitogeordie/Desktop/Demos/bolt.ipynb#ch0000025?line=6'>7</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to open file '../sa_demo/text_data/yelp_review_full_2class_train.svm'"
     ]
    }
   ],
   "source": [
    "train_data = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"../sa_demo/text_data/yelp_review_full_2class_train.svm\", \n",
    "    batch_size=1024)\n",
    "\n",
    "test_data = dataset.load_bolt_svm_dataset(\n",
    "    filename=\"../sa_demo/text_data/yelp_review_full_2class_test.svm\", \n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a benchmark, we compared it to RoBERTa, the state-of-the-art NLP model that trained for a whole month on 100 gb of data on a fleet of 8 gpus. We then fine-tuned it on this dataset, the state-of-the-art NLP model, on this dataset and got an accuracy of 83%. \n",
    "\n",
    "To do the same task with BOLT, we first convert the sentences into 100,000 dimensional sparse vectors. Which is huge! But that's one of the benefits of natively supporting sparsity: you can engineer your features as you like, and capture such a rich feature set from your dataset that you only need to train on one small dataset to build an accurate model. We define the model as follows: hidden layer of 2000 dimensions with a load factor 0.2, followed by a 2 dimensional output layer so we can choose between positive and negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_sentiment_analysis_layers = [\n",
    "    \n",
    "    bolt.LayerConfig(dim=2000, \n",
    "        load_factor=0.2, \n",
    "        activation_function=bolt.ActivationFunctions.ReLU),\n",
    "    \n",
    "    bolt.LayerConfig(dim=2,\n",
    "        load_factor=1.0, \n",
    "        activation_function=bolt.ActivationFunctions.Softmax)     \n",
    "]\n",
    "\n",
    "yelp_sentiment_analysis_network = bolt.Network(\n",
    "    layers=yelp_sentiment_analysis_layers, \n",
    "    input_dim=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & Save\n",
    "We trained this on a 10-year old CPU and that took a few minutes, which is fast for a network of this size, but a little long for this demo. So take the chance to introduce our load and save feature. We know that in practice, you want to train a model once, save it, and use it repeatedly, and we hear you. All you have to do to save a trained bolt model is to call the save method with the save file path of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Geordie): Add download scripts and change to relative path\n",
    "\n",
    "\n",
    "yelp_sentiment_analysis_network.train(\n",
    "    train_data=train_data,\n",
    "    loss_fn=bolt.CategoricalCrossEntropyLoss(), \n",
    "    learning_rate=0.0001, \n",
    "    epochs=20, \n",
    "    rehash=6400, \n",
    "    rebuild=128000)\n",
    "\n",
    "yelp_sentiment_analysis_network.save(filename=\"yelp_sentiment_analysis_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a trained model, call the `bolt.Network.load()` static method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_sentiment_analysis_network = bolt.Network.load(filename=\"yelp_sentiment_analysis_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment of truth\n",
    "RoBERTa: 83% accuracy.\n",
    "Let's see how BOLT does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/benito/Demos/bolt.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# TODO(Geordie): Add download scripts and change to relative path\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=1'>2</a>\u001b[0m test_data \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mload_bolt_svm_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=2'>3</a>\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../sa_demo/text_data/yelp_review_full_2class_test.svm\u001b[39m\u001b[39m\"\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=5'>6</a>\u001b[0m yelp_sentiment_analysis_network \u001b[39m=\u001b[39m bolt\u001b[39m.\u001b[39mNetwork\u001b[39m.\u001b[39mload(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39myelp_sentiment_analysis_cp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=7'>8</a>\u001b[0m res \u001b[39m=\u001b[39m yelp_sentiment_analysis_network\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=8'>9</a>\u001b[0m     test_data\u001b[39m=\u001b[39mtest_data, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=9'>10</a>\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcategorical_accuracy\u001b[39m\u001b[39m\"\u001b[39m], \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocal3/home/benito/Demos/bolt.ipynb#ch0000016vscode-remote?line=10'>11</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO(Geordie): Add download scripts and change to relative path\n",
    "res = yelp_sentiment_analysis_network.predict(\n",
    "    test_data=test_data, \n",
    "    metrics=[\"categorical_accuracy\"], \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also trained an even larger 2 billion parameter model on a larger text corpus to build an interactive sentiment analysis demo. We first load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(Geordie): Add download scripts and change to relative path\n",
    "sentiment_analysis_network = bolt.Network.load(filename=\"interactive_demo_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the demo to get a feel of what this network can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100%\n",
      "Processed 1 test batches in 18 milliseconds\n",
      "Accuracy: 0 (0/1)\n",
      "positive!\n",
      "[==================================================] 100%\n",
      "Processed 1 test batches in 6 milliseconds\n",
      "Accuracy: 0 (0/1)\n",
      "negative!\n",
      "[==================================================] 100%\n",
      "Processed 1 test batches in 9 milliseconds\n",
      "Accuracy: 0 (0/1)\n",
      "negative!\n",
      "[==================================================] 100%\n",
      "Processed 1 test batches in 9 milliseconds\n",
      "Accuracy: 0 (0/1)\n",
      "positive!\n",
      "Exiting demo...\n"
     ]
    }
   ],
   "source": [
    "import interactive_sentiment_analysis\n",
    "interactive_sentiment_analysis.demo(sentiment_analysis_network, verbose=False)\n",
    "# TODO(Geordie): Make the accuracy disappear when doing interactive demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's talk speed.\n",
    "Now that we've seen how fast inference is on BOLT, let's compare it with RoBERTa by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"siebert/sentiment-roberta-large-english\")\n",
    "t1 = time.time()\n",
    "out = sentiment_analysis(\"I love chocolate.\")\n",
    "t2 = time.time()\n",
    "print(out, flush=True)\n",
    "print('time elapsed: ',str(t2-t1),'s', flush=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
