{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IMPORTANT**\n",
    "To run this notebook, you need to download the ThirdAI docker container by signing up [here](https://www.thirdai.com/try-bolt/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment analysis with BOLT**\n",
    "\n",
    "We will walk through the process of building a sentiment analysis model with BOLT from data preprocessing all the way to inference. This notebook is structured as follows:\n",
    "1. Selecting and preprocessing the dataset\n",
    "2. Defining the BOLT network\n",
    "3. Training the network\n",
    "4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Choosing and preprocessing the dataset**\n",
    "At our webinar on April 6th, we showed how BOLT reached state-of-the-art accuracy on the [Yelp Reviews](https://github.com/huggingface/datasets/blob/master/datasets/yelp_polarity/yelp_polarity.py) dataset and demonstrated that a model trained on the [Amazon Polarity](https://huggingface.co/datasets/amazon_polarity) dataset can be used for interactive, real-time sentiment analysis. Now, we want to give you a chance to try out BOLT with a dataset of your choice. \n",
    "\n",
    "We provided a utility function that converts text datasets into input vectors and saves them in SVM format. The text dataset must be a CSV file where each row follows this format:\n",
    "\n",
    "\\<pos or neg\\>,\\<text\\> \n",
    "\n",
    "For example, we can have a training corpus called example_train.csv that contains the following:\n",
    "```\n",
    "pos,Had a great time at the webinar.\n",
    "neg,I hate slow deep learning models.\n",
    "```\n",
    "We recommend using a training corpus with at least 500,000 training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import dataset\n",
    "\n",
    "text_vector_dim = 100_000 # We will vectorize our samples into 100,000-dimensional sparse vectors.\n",
    "\n",
    "path_to_train_svm = \"preprocessed_data_train.svm\"\n",
    "dataset.tokenize_to_svm(\n",
    "    input_file=\"/path/to/train_data.csv\", # TODO: Change the path to train data\n",
    "    output_dim=text_vector_dim,\n",
    "    output_file=path_to_train_svm)\n",
    "\n",
    "path_to_test_svm = \"preprocessed_data_test.svm\"\n",
    "dataset.tokenize_to_svm(\n",
    "    input_file=\"/path/to/test_data.csv\", # TODO: Change the path to test data\n",
    "    output_dim=text_vector_dim,\n",
    "    output_file=path_to_test_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the SVM datasets that we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = dataset.load_bolt_svm_dataset(\n",
    "    filename=path_to_train_svm, \n",
    "    batch_size=256)\n",
    "\n",
    "test_data, test_labels = dataset.load_bolt_svm_dataset(\n",
    "    filename=path_to_test_svm, \n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Defining the BOLT network**\n",
    "**Layer configuration**\n",
    "\n",
    "First, we need to define the sequence of layers. In this limited demo version, we only support fully-connected layers, which we define with using `bolt.graph.Input(), bolt.graph.FullyConnected()`. It takes the following arguments:\n",
    "* `dim`: Int - The dimension of the layer.\n",
    "* `sparsity`: Float - The fraction of neurons to use during sparse training and sparse inference. For example, `sparsity`=0.05 means the layer uses 5% of its neurons when processing an individual sample.\n",
    "* `activation`: Bolt activation function - We support three activation functions: `relu`, `softmax` and `linear`.\n",
    "\n",
    "**Constructing the network**\n",
    "\n",
    "We then call the `bolt.graph.Model()` constructor, which takes in the sequence of layer configurations for input and output layer.\n",
    "\n",
    "**Network specifications**\n",
    "\n",
    "The network defined below has the same specifications as the network that we used for sentiment analysis on the [Yelp Reviews dataset](https://github.com/huggingface/datasets/blob/master/datasets/yelp_polarity/yelp_polarity.py) during our April 6th webinar. It is a 202,000 parameter model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "input_layer = bolt.graph.Input(dim=text_vector_dim)\n",
    "\n",
    "hidden_layer = bolt.graph.FullyConnected(\n",
    "        dim=2000,\n",
    "        sparsity=0.2,\n",
    "        activation=\"relu\",\n",
    "    )(input_layer)\n",
    "\n",
    "output_layer = bolt.graph.FullyConnected(dim=2, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "model = bolt.graph.Model(inputs=[input_layer], output=output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training\n",
    "\n",
    "**Model compilation with loss function**\n",
    "\n",
    "* `loss_fn`: BOLT loss function - The loss function to minimize. In this demo version, we only support the `bolt.CategoricalCrossEntropyLoss()` loss function.\n",
    "\n",
    "**Creating training config**\n",
    "\n",
    "* `learning_rate`: Float - The learning rate for gradient descent. The default value is 0.0001.\n",
    "* `epochs`: Int - The number of training epochs (a full cycle through the dataset).\n",
    "\n",
    "**The train() method**\n",
    "\n",
    "Train the BOLT network by calling the `train()` method, which accepts the following arguments:\n",
    "* `train_data`: BOLT dataset - The training source dataset in a format returned by `dataset.load_bolt_svm_dataset()`.\n",
    "* `train_label`: BOLT dataset - The training target label in a format returned by `dataset.load_bolt_svm_dataset()`.\n",
    "* `train_config`: Training Config - The training config provides training parameters to model training`.\n",
    "\n",
    "It then returns a dictionary that contains the loss value and elapsed time for each training epoch.\n",
    "\n",
    "\n",
    "**Saving a trained model**\n",
    "\n",
    "Simply call the `save()` method, passing in the location of the save file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=bolt.CategoricalCrossEntropyLoss())\n",
    "\n",
    "train_config = (\n",
    "        bolt.graph.TrainConfig.make(learning_rate=0.0001, epochs=20)        \n",
    "    )\n",
    "\n",
    "metrics = model.train(\n",
    "        train_data=train_data, train_labels=train_labels, train_config=train_config\n",
    "    )\n",
    "    \n",
    "model.save(filename=\"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Inference**\n",
    "\n",
    "**Defining predict config**\n",
    "\n",
    "* `with_metrics`: List of strings - Metric to evaluate our prediction. In this demo version, we only support the `\"categorical_accuracy\"` metric.\n",
    "\n",
    "**The predict() method**\n",
    "\n",
    "You can do inference by calling the `predict()` method, which accepts the following arguments:\n",
    "* `test_data`: BOLT dataset - The test dataset in a format returned by `dataset.load_bolt_svm_dataset()`.\n",
    "* `test_label`: BOLT dataset - The test label in a format returned by `dataset.load_bolt_svm_dataset()`.\n",
    "* `predict_config` : predict config with metric definition.\n",
    "\n",
    "It then returns a dictionary of metric_results:\n",
    "* `metric_results`: Dictionary - A dictionary mapping each metric name in `metrics` to a list of values for that metric.\n",
    "\n",
    "**Loading a saved model**\n",
    "\n",
    "To load a saved model, call the `bolt.graph.Model.load()` method. We commented it out by default so you can just continue from the previous cell, but you can always uncomment it so you don't have to retrain the model the next time you visit this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_config = (\n",
    "        bolt.graph.PredictConfig.make().with_metrics([\"categorical_accuracy\"])\n",
    "    )\n",
    "\n",
    "metrics = model.predict(\n",
    "        test_data=test_data, test_labels=test_labels, predict_config=predict_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Congratulations! You just mastered BOLT.**\n",
    "If you face any issue running this notebook, please reach out to us by posting about it on [GitHub Issues](https://github.com/ThirdAILabs/Demos/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
