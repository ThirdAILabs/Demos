{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with ThirdAI's Universal Deep Transformer\n",
    "\n",
    "To run this notebook, you will need to obtain a ThirdAI license at the following link if you have not already: https://www.thirdai.com/try-bolt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: thirdai in /opt/homebrew/lib/python3.9/site-packages (0.5.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.9/site-packages (from thirdai) (1.5.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.9/site-packages (from thirdai) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.9/site-packages (from thirdai) (4.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->thirdai) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.9/site-packages (from pandas->thirdai) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->thirdai) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the '/opt/homebrew/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install thirdai --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Download\n",
    "\n",
    "In the next cell, we will process an Amazon review sentiment analysis dataset from the HuggingFace datasets library. This dataset consists of product review texts with associated binary labels indicated if the review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset amazon_polarity (/Users/vihan/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/a27b32b7e7b88eb274a8fa8ba0f654f1fe998a87c22547557317793b5d2772dc)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014071941375732422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 32,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7f312dea6b4f0993db53f53632d599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from utils import to_batch\n",
    "\n",
    "def load_data(data, output_filename, split, return_inference_batch=False):\n",
    "    \n",
    "    df = pd.DataFrame(data[split])\n",
    "    df = df[['title', 'label']]    \n",
    "    df.to_csv(output_filename, index=False, sep='\\t')\n",
    "    \n",
    "    if return_inference_batch:\n",
    "        inference_batch = to_batch(df[[\"title\"]].sample(frac=1).iloc[:5])\n",
    "        return inference_batch\n",
    "\n",
    "train_filename = \"amazon_polarity_train.csv\"\n",
    "test_filename = \"amazon_polarity_test.csv\"\n",
    "\n",
    "data = load_dataset('amazon_polarity')\n",
    "load_data(data, train_filename, split='train')\n",
    "inference_batch = load_data(data, test_filename, split='test', return_inference_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# UDT Initialization\n",
    "We can now create a UDT model by passing in the types of each column in the dataset and the target column we want to be able to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "model = bolt.UniversalDeepTransformer(\n",
    "    data_types={\n",
    "        \"title\": bolt.types.text(),\n",
    "        \"label\": bolt.types.categorical(),\n",
    "    },\n",
    "    target=\"label\",\n",
    "    n_target_classes=2,\n",
    "    delimiter='\\t',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We can now train our UDT model with just two lines! Feel free to customize the number of epochs and the learning rate; we have chosen values that give good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from 'amazon_polarity_train.csv'\n",
      "Loaded 3600000 vectors from 'amazon_polarity_train.csv' in 3 seconds.\n",
      "train epoch 0:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "train | epoch 0 | updates 1758 | {categorical_accuracy: 0.841919} | batches 1758 | time 136s | complete\n",
      "\n",
      "train epoch 1:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "train | epoch 1 | updates 3516 | {categorical_accuracy: 0.909367} | batches 1758 | time 126s | complete\n",
      "\n",
      "train epoch 2:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "train | epoch 2 | updates 5274 | {categorical_accuracy: 0.949447} | batches 1758 | time 123s | complete\n",
      "\n",
      "train epoch 3:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "train | epoch 3 | updates 7032 | {categorical_accuracy: 0.964999} | batches 1758 | time 263s | complete\n",
      "\n",
      "train epoch 4:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "train | epoch 4 | updates 8790 | {categorical_accuracy: 0.972298} | batches 1758 | time 122s | complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_config = (bolt.TrainConfig(epochs=5, learning_rate=0.01)\n",
    "                    .with_metrics([\"categorical_accuracy\"]))\n",
    "\n",
    "model.train(train_filename, train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluating the performance of the UDT model is also just two lines!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from 'amazon_polarity_test.csv'\n",
      "Loaded 400000 vectors from 'amazon_polarity_test.csv' in 0 seconds.\n",
      "test:\n",
      "\n",
      "[==================================================] 100%\n",
      "\n",
      "predict | epoch 5 | updates 8790 | {categorical_accuracy: 0.830342} | batches 196 | time 4188ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_config = (bolt.EvalConfig()\n",
    "                   .with_metrics([\"categorical_accuracy\"]))\n",
    "\n",
    "model.evaluate(test_filename, eval_config);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading\n",
    "Saving and loading a trained UDT model to disk is also extremely straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = \"sentiment_analysis.model\"\n",
    "\n",
    "# Saving\n",
    "model.save(save_location)\n",
    "\n",
    "# Loading\n",
    "model = bolt.UniversalDeepTransformer.load(save_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Predictions\n",
    "The evaluation method is great for testing, but it requires labels, which don't exist in a production setting. We also have a predict method that can take in an in-memory batch of rows or a single row (without the target column), allowing easy integration into production pipelines. Note that UDT can perform inference with **sub-millisecond** latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Prediction Results\n",
      "Input: {'title': 'spotty coverage of commonly used words'} Prediction: 0\n",
      "Input: {'title': 'Simple and easy'} Prediction: 0\n",
      "Input: {'title': 'Battlestar Galactica 1980'} Prediction: 0\n",
      "Input: {'title': 'Tedious'} Prediction: 0\n",
      "Input: {'title': 'History, Finction, Fantasy & Romance'} Prediction: 0\n",
      "CPU times: user 1.85 ms, sys: 3.1 ms, total: 4.95 ms\n",
      "Wall time: 4.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "prediction_batch = model.predict_batch(inference_batch)\n",
    "class_names = [\"Positive\" if model.class_name(class_id) == 1 else 0 \n",
    "               for class_id in np.argmax(prediction_batch, axis=1)]\n",
    "\n",
    "print(\"Batch Prediction Results\")\n",
    "for input_sample, class_name in zip(inference_batch, class_names):\n",
    "    print(\"Input:\", input_sample, \"Prediction:\", class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
