{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb726af",
   "metadata": {},
   "source": [
    "# **Setup** \n",
    "\n",
    "To run this notebook, you must first download a valid license [here](https://www.thirdai.com/try-bolt/). The license must be placed in your home directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b06fde",
   "metadata": {},
   "source": [
    "### Optional: Install ThirdAI, Torch, Numpy, and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai\n",
    "!pip3 install torch\n",
    "!pip3 install numpy\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829168d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import thirdai\n",
    "from thirdai.search import DocRetrieval\n",
    "from thirdai.embeddings import DocSearchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c210b860",
   "metadata": {},
   "source": [
    "# **Outline**\n",
    "This notebook walks you through building a ThirdAI document retrieval index. \n",
    "This notebook is structured as follows:\n",
    "1. Creating the dataset\n",
    "2. Defining the index\n",
    "3. Populating the index with the dataset\n",
    "4. Saving and loading the index\n",
    "5. Querying the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0c217",
   "metadata": {},
   "source": [
    "# Step 1: Creating the Dataset\n",
    "First we need a dataset, which for our purposes just consists of a collection of (document_id, document_text) pairs. Our engine works best with document sizes between 20 and 200 words long. Feel free to split big documents up into many small passages!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81019f0b",
   "metadata": {},
   "source": [
    "### Option 1: Use a small example dataset from MSMarco\n",
    "\n",
    "This is a very small example dataset prepopulated with 25 passages about Neural Networks from [MSMarco](https://microsoft.github.io/msmarco/). Feel free add more passages from MSMarco or from anywhere else; this index is scalable to millions of documents on a single machine. Alternatively, you can download the entire dataset [here](https://microsoft.github.io/msmarco/Datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Unlike with short-term memory, forgetting occurs in long-term memory when the formerly strengthened synaptic connections among the neurons in a neural network become weakened, or when the activation of a new network is superimposed over an older one, thus causing interference in the older memory.\",\n",
    "    \"A neuron (also known as a neurone or nerve cell) is an electrically excitable cell that processes and transmits information through electrical and chemical signals. These signals between neurons occur via synapses, specialized connections with other cells. Neurons can connect to each other to form neural networks. Neurons are the core components of the brain and spinal cord of the central nervous system (CNS), and of the ganglia of the peripheral nervous system (PNS).\",\n",
    "    \"The activation level of a neuron in an artificial neural network is a real number often limited to the range 0 to 1, or â1 to 1. In the case of an input neuron the value is obtained externally to the network. In the case of a hidden neuron or output neuron the value is obtained from the neuron's activation function. In neural networks, an activation function is the function that describes the output behaviour of a neuron. Most network architectures start by computing the weighted sum of the inputs (that is, the sum of the product of each input with the weight associated with that input.\",\n",
    "    \"Deep learning refers to artificial neural networks that are composed of many layers. It's a growing trend in ML due to some favorable results in applications where the target function is very complex and the datasets are large. For example in Hinton et al.\",\n",
    "    \"The midbrain also contains a portion of the reticular formation, a neural network that is involved in arousal and alertness. Cranial nerves in the midbrain that stimulate the muscles controlling eye movement, lens shape, and pupil diameter form the nuclear complex of the oculomotor nerve and the trochlear nucleus.\",\n",
    "    \"The bottleneck mechanism occurs when the first stimulus is being processed, any other stimuli cannot be processed. The processing of the second stimulus is postponed which slows down reaction time. Neurons that take in information send this information to specific neural networks in different locations.\",\n",
    "    \"This module covers more advanced supervised learning methods that include ensembles of trees (random forests, gradient boosted trees), and neural networks (with an optional summary on deep learning). You will also learn about the critical problem of data leakage in machine learning and how to detect and avoid it.\",\n",
    "    \"These neural networks power the speech recognition software, language translation, and semantic search facilities that Google, Facebook and many apps use today. Training a neural network so that it learns works similarly to establishing connections between neurons and strengthening those connections in the brain.\",\n",
    "    \"It is a matter of contention within cognitive science whether explanation is properly conceived as the process and results of belief revision or as the activation of patterns within a neural network. This article focuses on the way thinking about explanation within the philosophy of science has changed since 1950.\",\n",
    "    \"Deep learning refers to both deep neural networks and other branches of machine learning like deep reinforcement learning. In the press, it usually means deep neural nets. Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns.\",\n",
    "    \"Today, the hottest area of artificial intelligence is neural networks, which are proving successful in a number of disciplines such as voice recognition and natural-language processing. There are several programming languages that are known as AI languages because they are used almost exclusively for AI applications.\",\n",
    "    \"Meanwhile, the on-chip memory has increased from 1 GB on the Fathom NCS to 4 GB LPDDR3 on the Movidius NCS, in order to facilitate larger and denser neural networks. And to cap it all off, Movidius has been able to reduce the MSRP to $79 â citing Intelâs manufacturing and design expertiseâ â lowering the cost of entry even more.\",\n",
    "    \"As Intel states, the Movidius NCS is âthe worldâs first self-contained AI accelerator in a USB format,â and is designed to permit host gadgets to course of deep neural networks natively â or in different phrases, on the edge.\",\n",
    "    \"It has been estimated that the average human brain contains about 100 billion neurons and, on average, each neuron is connected to 1000 other neurons. This results in the generation of vast and complex neural networks that are the mainstay of the brain's processing capabilities. Neurons are the basic data processing units, the 'chips', of the brain. Each neuron receives electrical inputs from about 1000 other neurons.\",\n",
    "    \"Auto-encoder What is an auto-encoder? Auto-encoder is one type of neural networks that approximates the function: f(x) = x. Basically, given an input x, network will learn to output f(x) that is as close as to x. The error between output and x is commonly measured using root mean square error (RMSE) â mean((f(x) â x) ^ 2) â which is the loss function we try to minimise in our network.\",\n",
    "    \"An autoencoder is a neural network that is trained in an unsupervised fashion. The goal of an autoencoder is to find a more compact representation of the data by learning an encoder, which transforms the data to their corresponding compact representation, and a decoder, which reconstructs the original data.\",\n",
    "    \"In computer science and machine learning, cellular neural networks are a parallel computing paradigm similar to neural networks, with the difference that communication is allowed between neighbouring units only. Typical applications include image processing, analyzing 3D surfaces, solving partial differential equations, reducing non-visual problems to geometric maps, modelling biological vision and other sensory-motor organs.\",\n",
    "    \"what is cnn A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers, pooling layers and then followed by one or more fully connected layers as in a standard neural network.\",\n",
    "    \"Conectionism is an approach to the study of human cognition that utilizes mathematical models, known as connectionist networks or artificial neural networks. Often, these come in the form of highly interconnected, neuron-like processing units.\",\n",
    "    \"The brain is a (biological) neural network: it's a network of neurons. Artificial neural networks, usually just referred to as neural networks, are computer simulations which process information in a way similar to how we think the brain does it.\",\n",
    "    \"An artificial neural network (NN for short) is a classifier. In supervised machine learning, classification is one of the most prominent problems. The aim is to assort objects into classes that are defined a priori (terminology not to be confused with Object Oriented programming).\",\n",
    "    \"To feed the input data to the neural network, we convert the data into features. Here, we will be using intensity values (of values 0 to 255) in our 20* 20 size images as input features for the algorithm; this makes the feature vector size for neural network to be 400.\",\n",
    "    \"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\",\n",
    "    \"Deep Dream Generator is a platform where you can transform photos using a powerful AI algorithms. 1  You can create different types of Deep Dream and Deep Style images. 2  Deep Dream: Initially it was invented to help scientists and engineers to see what a deep neural network is seeing when it is looking in a given image.\",\n",
    "    \"The Personal Robot, developed by New York-based startup Robotbase, is described by its creators as a revolutionary product that changes everything. The robot uses deep neural networks to enable natural language understanding, image recognition, navigation and machine learning.\",\n",
    "]\n",
    "\n",
    "dataset = [(str(i), doc) for i, doc in enumerate(documents)]\n",
    "\n",
    "print(f\"Created dataset of length {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c253d26",
   "metadata": {},
   "source": [
    "### Option 2: Use a larger example dataset (Kaggle Amazon product catalog)\n",
    "This is a larger dataset comprising of ~110,000 products in Amazon's catalog. We process each product listing to include their title, description, and features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O amazon_product_catalog.zip \"https://storage.googleapis.com/kaggle-data-sets/1505208/2486699/compressed/dataset/test.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220512%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220512T214541Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=6cd024820d6351f4df4116a5c0576a0cbdbd6c3966411b5e3475cbbf331b6e596d25f926b0fed2b8809f60254baf96fc0b594cc1cbb0e529405f6bfe84dba1272d42125f822df398eabd5715bc275a2be682e12e4911ce5695829b8058b05748f21fc25758c189192a1d0961cd64f529f8745da60d1a272005739c24402c3c90ff576aaf07dc4162e7c3e2f24ed2e4d16099e07de9297999d6b3be744eaccf6e101ded002654575ba90942fb1832577554bf2d50975d574fe409a13f60243acaba3026026a94348e4b995ed89bf06983e2a8f0d9d40d9907464e4ef2d45030d54485cffed60f7aefade22726122fa17af5675e80e0256fc75d6bb2aa66f76d77\"\n",
    "!unzip amazon_product_catalog.zip\n",
    "\n",
    "count = 0\n",
    "prodDict = {}\n",
    "listStrings = []\n",
    "with open(\"test.csv\", newline=\"\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        count += 1\n",
    "        if count == 1:\n",
    "            continue  # Ignore the csv header\n",
    "        prodDict[count] = row[1]\n",
    "        listStrings.append(row[1])\n",
    "        listStrings.append(row[1] + \" \" + row[2])\n",
    "        listStrings.append(row[1] + \" \" + row[3])\n",
    "\n",
    "dataset = [(str(i), doc) for i, doc in enumerate(listStrings)]\n",
    "\n",
    "print(f\"Created dataset of length {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e5da5d",
   "metadata": {},
   "source": [
    "### Option 3: Supply your own dataset\n",
    "Alternatively, you can use your own dataset with our index. The dataset should be formatted as a list of tuples of the form (*i*, \\[doc\\]), where *i* is a unique integer identifier formatted as a string, and \\[doc\\] is a string document between 20 and 200 words in length (again, you can split larger documents into several entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ...\n",
    "\n",
    "print(f\"Created dataset of length {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c740701",
   "metadata": {},
   "source": [
    "# Step 2: Defining the index\n",
    "The general outline of our document search system is that we will use an embedding model to embed the documents and then pass them into the index. During the query process, we'll embed the queries into the same space and then query the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da9fec",
   "metadata": {},
   "source": [
    "We can now use ThirdAI libraries to create an initially empty index and a simple document + query embedding model.\n",
    "\n",
    "Below is a quick description of the input to the DocRetrieval constructor:\n",
    "* dense_input_dimension is the dimension of the output of our embedding model (128 with our preloaded model)\n",
    "\n",
    "* num_tables and hashes_per_table are hyperparameters of the index\n",
    "  * increasing num_tables increases the accuracy of the model at the cost of speed and memory (a good value is the preset 16)\n",
    "  \n",
    "  * hashes_per_table has a sweet spot in accuracy at around log_2(average document size)\n",
    "  \n",
    "* centroids is a numpy array that represents precomputed centroids for the embedding space; we've already calculated and stored these for you! We've precomputed 2^18 of them, but really you just need the number of centroids to be a small fraction of the total documents you intend to add. A simple heuristic to get less centroids is to select a random subsample of them, which we do below (we select the number of centroids equal to 10% of the number of documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34678e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = DocSearchModel()\n",
    "\n",
    "all_centroids = embedding_model.getCentroids().copy()\n",
    "np.random.shuffle(all_centroids)\n",
    "reduced_centroids = all_centroids[: max(len(dataset) // 10, 1)]\n",
    "\n",
    "our_index = DocRetrieval(\n",
    "    dense_input_dimension=128,\n",
    "    num_tables=16,\n",
    "    hashes_per_table=6,\n",
    "    centroids=reduced_centroids,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6936e83",
   "metadata": {},
   "source": [
    "# Step 3: Populating the index\n",
    "We'll now simply loop through each document in our generated dataset, embed the document text, and\n",
    "add the document to the index by specifying doc_id, doc_text, and doc_embeddings. Please note: if you are embedding all the entries in the Amazon product catalog dataset, this could take up to an hour depending on the speed of your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_id, doc_text in dataset:\n",
    "    embedding = embedding_model.encodeDocs([doc_text])\n",
    "    our_index.add_doc(doc_id=doc_id, doc_text=doc_text, doc_embeddings=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52f017",
   "metadata": {},
   "source": [
    "# Step 4: Saving and loading the index\n",
    "We can also save the index to disk for future use. Here, we'll just save it to disk and immediately read it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e4328",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_index.serialize_to_file(\"index.serialized\")\n",
    "our_index = DocRetrieval.deserialize_from_file(\"index.serialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d422245",
   "metadata": {},
   "source": [
    "# Step 5: Querying the index\n",
    "\n",
    "The following cell starts up an interactive demo, where you can type any query you want and the index will return the document most semantically likely to answer your query. \n",
    "\n",
    "If you are using the very small example dataset we prepopulated above, try queries like \"what is deep learning\" or \"how do auto encoders work\". If you are using the Amazon product catalog dataset, try queries like \"\" or \"\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8184f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"> \", end=\"\")\n",
    "    query_text = input()\n",
    "\n",
    "    if query_text == \"q\":\n",
    "        break\n",
    "\n",
    "    start = time.time()\n",
    "    embedding = embedding_model.encodeQuery(query_text)\n",
    "    result = our_index.query(embedding, top_k=1)\n",
    "    total_time = time.time() - start\n",
    "\n",
    "    print(\"Closest match: \" + str(result[0]))\n",
    "    print(f\"Took {total_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
