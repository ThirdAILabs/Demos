{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling Tabular Data with BOLT\n",
    "\n",
    "Tabular data is some of the most common data in real world applications. Consisting of mostly categories and numbers, tabular data is used in a variety of contexts including financial services, fraud detection, healthcare, climate science, and more. For these problems, gradient boosted decision trees like XGBoost have long been the algorithms of choice for their ability to quickly learn efficient representational decision boundaries on the data. However, deep learning has made significant strides in this area with developments such as TabNet and even our very own BOLT engine.\n",
    "\n",
    "Our deep learning engine BOLT is an efficient neural framework that has proven effective on a wide array of problems. Most recently, initial experiments have shown that BOLT can beat top methods such as XGBoost and TabNet on several benchmark tabular datasets. Additionally, BOLT has the added capability that it works exceptionally well as an out of the box classifier, requiring much less effort to train than other methods. In this notebook, we'll showcase some of our exciting results on several standard tabular datasets as well as walk you through how you can try out BOLT for yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "First we'll start off with some results. We compared our BOLT tabular classifier with XGBoost and TabNet on several benchmark tabular classification datasets to produce the following numbers. We hope these numbers serve to counter the narrative that gradient boosted trees are the best solution for tabular data and inspire discussion around our emerging BOLT technology. \n",
    "\n",
    "| Dataset      | BOLT Accuracy | XGBoost Accuracy | TabNet Accuracy | \n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| Census Income      |   85%    | 84% | 78% |\n",
    "| Poker Hand Induction   |    96%     | 62% | 54% |\n",
    "| Eye Movements      |   60%    | 37% | 30% |\n",
    "| BNP Paribas Cardif Claims Management  |    76%     | 75% | 76% |\n",
    "| Churn Modeling   |    84%     | 84% | 77% |\n",
    "| Higgs Boson   |    65%     | 50% | 74% |\n",
    "\n",
    "*Results are shown with default parameters, under no specialized tuning and feature engineering, along with early stopping based on validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Using deep learning with tabular data opens up the possibility of transfer learning, self-supervised, semi-supervised learning, and many more influential paradigms, all of which are otherwise hard with only boosted trees. We hope these extensions may open up research in this area and lead to new innovations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try out the BOLT TabularClassifier\n",
    "\n",
    "Next we'll move along with a demo. Once you have recieved a valid license file from our website and followed our package installation instructions (https://www.thirdai.com/installation-instructions/), follow the instructions detailed below to try out the BOLT TabularClassifier for yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and Set Up your dataset\n",
    "\n",
    "You're welcome to download and set up any tabular dataset of your choosing. However, for this demo we'll be using the publicly available Census Income dataset which is a common benchmark in tabular classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CENSUS_INCOME_BASE_DOWNLOAD_URL = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/\"\n",
    ")\n",
    "\n",
    "TRAIN_FILE = \"./census_income_train.csv\"\n",
    "TEST_FILE = \"./census_income_test.csv\"\n",
    "PREDICTION_FILE = \"./census_income_predictions.txt\"\n",
    "\n",
    "\n",
    "def download_census_income_dataset():\n",
    "    if not os.path.exists(TRAIN_FILE):\n",
    "        os.system(\n",
    "            f\"curl {CENSUS_INCOME_BASE_DOWNLOAD_URL}adult.data --output {TRAIN_FILE}\"\n",
    "        )\n",
    "    if not os.path.exists(TEST_FILE):\n",
    "        os.system(\n",
    "            f\"curl {CENSUS_INCOME_BASE_DOWNLOAD_URL}adult.test --output {TEST_FILE}\"\n",
    "        )\n",
    "\n",
    "def reformat_test_csv():\n",
    "    with open(TEST_FILE, \"r\") as fin:\n",
    "        data = fin.read().splitlines(True)\n",
    "    with open(TEST_FILE, \"w\") as fout:\n",
    "        # for some reason each of the labels end with a \".\" in the test set\n",
    "        # loop through data[1:] since the first line is bogus\n",
    "        fout.writelines([line.replace(\".\", \"\") for line in data[1:]])\n",
    "\n",
    "def get_dataset_metadata():\n",
    "    df = pd.read_csv(TEST_FILE)\n",
    "    n_classes = df[df.columns[-1]].nunique()\n",
    "    column_datatypes = []\n",
    "    for col_type in df.dtypes[:-1]:\n",
    "        if col_type == \"int64\":\n",
    "            column_datatypes.append(\"numeric\")\n",
    "        elif col_type == \"object\":\n",
    "            column_datatypes.append(\"categorical\")\n",
    "\n",
    "    # the last column is the label\n",
    "    column_datatypes.append(\"label\")\n",
    "\n",
    "    # TabularClassifier assumes no header so we add the first label \n",
    "    # (considered a column name in pandas) as part of the test labels\n",
    "    test_labels = [df.columns[-1]] + list(df[df.columns[-1]])\n",
    "\n",
    "    return n_classes, column_datatypes, test_labels\n",
    "\n",
    "\n",
    "download_census_income_dataset()\n",
    "reformat_test_csv()\n",
    "(n_classes, column_datatypes, test_labels) = get_dataset_metadata()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Defining the TabularClassifier model\n",
    "\n",
    "### Arguments\n",
    "- `model_size`: Controls how big the model is. Options are ‘small’, ‘medium’,  ‘large’, or a target model size in gigabytes, i.e. “4 Gb” or “4Gb”.\n",
    "- `n_classes`: How many output classes are in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = bolt.TabularClassifier(model_size=\"medium\", n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the TabularClassifier\n",
    "\n",
    "### Arguments\n",
    "- `train_file`: The dataset to train on. The expected format is a csv file with comma separated columns. This csv file should not have a header. The TabularClassifier also does not attempt to shuffle the dataset, the user should shuffle the dataset before calling train if necessary.\n",
    "- `column_datatypes`: List of column types. Should be a list of strings with the types of the columns, which are one of: \"numeric\", \"categorical\", or \"label\". Numeric columns will be interpreted as integers or floating point values. Categorical columns and label columns will be interpreted as strings. Empty values are supported for categorical and numeric columns. Finally, there must be one and only one \"label\" column. \n",
    "- `epochs`: This determines the number of epochs to train on.\n",
    "- `learning_rate`: Determines the learning_rate for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(\n",
    "    train_file=TRAIN_FILE,\n",
    "    column_datatypes=column_datatypes,\n",
    "    epochs=1,\n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Testing The Classifier\n",
    "### Arguments\n",
    "- `test_file`: The test dataset to run. The file should follow the same format as the test file, column datatypes are assumed to be identical.\n",
    "- `output_file`: This is an optional parameter. If it is specified then as the data is processed it writes the names of the predicted classes into this file. Each predicted class name is on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(test_file=TEST_FILE, output_file=PREDICTION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Verifying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_labels, pred_file):\n",
    "    with open(pred_file) as pred:\n",
    "        predictions = pred.readlines()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    assert len(predictions) == len(test_labels)\n",
    "    for (prediction, answer) in zip(predictions, test_labels):\n",
    "        if prediction[:-1] == answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "acc = compute_accuracy(test_labels, PREDICTION_FILE)\n",
    "\n",
    "print(\"Computed Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(TRAIN_FILE)\n",
    "os.remove(TEST_FILE)\n",
    "os.remove(PREDICTION_FILE)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "466a5e549cd4130abce17507970063a94f0693bc44f35ac9e31b9c274917fb93"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('thirdai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
