{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's NeuralDB\n",
    "\n",
    "NeuralDB, as the name suggests, is a combination of a neural network and a database. It provides a high-level API for users to insert different types of files into it and search through the file contents with natural language queries. The neural network part of it enables semantic search while the database part of it stores the paragraphs of the files that are inserted into it.\n",
    "\n",
    "First, let's install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade\n",
    "!pip3 install \"thirdai[neural_db]\"\n",
    "!pip3 install langchain --upgrade\n",
    "!pip3 install openai --upgrade\n",
    "!pip3 install paper-qa --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import licensing, neural_db as ndb\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import os\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"366E41-392A4E-A4D03D-0AAD37-7DC14A-V3\")  # Enter your ThirdAI key here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import the relevant module and define a neural db class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ndb.NeuralDB(user_id=\"my_user\") # you can use any username, in the future, this username will let you push models to the model hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You even load from a base DB from our Bazaar (optional but recommended)\n",
    "\n",
    "We have a model bazaar that provides users with domain specific NeuralDBs that can jumpstart searching on their private documents. The Bazaar has two main types of DBs\n",
    "\n",
    "1. Base DBs: These come with models that have either general QnA capabilities or domain specific capabilities like search on Medical Documents, Financial documents or Contracts. These come with an empty data index into which users can insert their files.\n",
    "\n",
    "2. Pre-Indexed DBs: These are ready-to-search DBs that come with pre-trained models and their corresponding datasets. These are meant to  search through large public datasets like PubMed or Amazon 3MM Products or Stackoverflow issues etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a cache directory\n",
    "import os\n",
    "if not os.path.isdir(\"bazaar_cache\"):\n",
    "    os.mkdir(\"bazaar_cache\")\n",
    "\n",
    "from pathlib import Path\n",
    "from thirdai.neural_db import Bazaar\n",
    "bazaar = Bazaar(cache_dir=Path(\"bazaar_cache\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call fetch to refresh list of available DBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bazaar.fetch() # Optional arg filter=\"model name\" to filter by model name.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of all DBs in the Bazaar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contract Review', 'Finance QnA', 'General QnA']\n"
     ]
    }
   ],
   "source": [
    "print(bazaar.list_model_names())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally load the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = bazaar.get_model(\"General QnA\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert your files\n",
    "\n",
    "Let's insert things into it!\n",
    "\n",
    "Currently, we natively support adding `CSV, PDF, DOCX, TXT, PPTX, EML, Outlook message (MSG)` files. We also have a support to automatically scrape and parse URLs. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document. \n",
    "\n",
    "In addition to the above documents, if you have a database with substantial number of rows in a table or multiple native documents, we offer support for SQLAlchemy and SharePoint. This enables you to train the data from remote locations. \n",
    "\n",
    "Furthermore, you have the option to specify the parameter `max_in_memory_batches`, which will divide the data into manageable segments and allow you to train the database model on them, mitigating the risk of memory errors.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: CSV files\n",
    "The first example below shows how to insert a CSV file. Please note that a CSV file is required to have a column named \"DOC_ID\" with rows numbered from 0 to n_rows-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "csv_files = ['data/sample_nda.csv']\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = ndb.CSV(\n",
    "        path=file,\n",
    "        id_column=\"DOC_ID\",\n",
    "        strong_columns=[\"passage\"],\n",
    "        weak_columns=[\"para\"],  \n",
    "        reference_columns=[\"passage\"])\n",
    "    #\n",
    "    insertable_docs.append(csv_doc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "pdf_files = ['data/sample_nda.pdf']\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = ndb.PDF(file)\n",
    "    insertable_docs.append(pdf_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: DOCX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "doc_files = ['data/sample_nda.docx']\n",
    "\n",
    "for file in doc_files:\n",
    "    doc = ndb.DOCX(file)\n",
    "    insertable_docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: Parse from URLs directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you can use our utility to generate a set of candidate URLs containing data of interest. You can also use your own list of URLs to extract data from.\n",
    "```python\n",
    "valid_url_data = ndb.parsing_utils.recursive_url_scrape(\n",
    "  base_url=\"https://www.thirdai.com/pocketllm/\", max_crawl_depth=0\n",
    ")\n",
    "```\n",
    "Then you can create a list of insertable documents from those URLs:\n",
    "```python\n",
    "insertable_docs = []\n",
    "for url, response in valid_url_data:\n",
    "    try:\n",
    "        insertable_docs.append(ndb.URL(url, response))\n",
    "    except:\n",
    "        continue\n",
    "```\n",
    "These can be inserted into Neural DB just like any other document.\n",
    "```python\n",
    "db.insert(insertable_docs, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5: EML, PPTX, and TXT files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "doc_files = ['data/sample_nda.eml' ,'data/sample_nda.txt', 'data/sample_nda.pptx']\n",
    "for file in doc_files:\n",
    "    doc = ndb.Unstructured(file)\n",
    "    insertable_docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 6: SQLDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the data directly from a database. Please note that the concerned table in the database should have a simple primary key as the id column ranging from 0 to n_rows - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "insertable_docs = []\n",
    "db_url = \"sqlite:///data/sample.db\"\n",
    "engine = create_engine(url = db_url)\n",
    "table_name = \"nda_table\"\n",
    "\n",
    "db_doc = ndb.SQLDatabase(engine = engine,\n",
    "                         table_name = table_name,\n",
    "                         id_col = \"id\",\n",
    "                         strong_columns = [\"passage\"],\n",
    "                         weak_columns=[\"para\"],\n",
    "                         reference_columns=[\"passage\"],\n",
    "                    )\n",
    "\n",
    "insertable_docs.append(db_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 6: SharePoint files\n",
    "\n",
    "First you need to create a `ClientContext` object for your site. There are multiple ways to create it. The simplest way is to use the `(client_id and client_secret)` OR `(username and password)`\n",
    "\n",
    "\n",
    "To make it more easy, we provide a method to do that also\n",
    "```python\n",
    "ctx = ndb.SharePoint.setup_clientContext(\n",
    "  base_url=\"https://<domain>.sharepoint.com/sites/<site-name>\", credentials = {\"username\": username, \"password\": password}\n",
    ")\n",
    "```\n",
    "OR\n",
    "```python\n",
    "ctx = ndb.SharePoint.setup_clientContext(\n",
    "  base_url=\"https://<domain>.sharepoint.com/sites/<site-name>\", credentials = {\"client_id\": client_id, \"client_secret\": client_secret}\n",
    ")\n",
    "```\n",
    "currently, we only support creation of ctx object with those above pair of credentials. Other ways are more interactive way to create it. For more details: https://github.com/vgrem/Office365-REST-Python-Client\n",
    "\n",
    "Then you can create a ndb-sharepoint document\n",
    "```python\n",
    "db_doc = ndb.SharePoint(\n",
    "                  ctx = ctx,\n",
    "                  library_path = <Library path where the documents are present>)   # Default is \"Shared Documents\"\n",
    "  \n",
    "insertable_docs = [db_doc]\n",
    "```\n",
    "Now it can be inserted into Neural DB just like any other document.\n",
    "```python\n",
    "db.insert(insertable_docs, ...)\n",
    "```\n",
    "\n",
    "NOTE: Since we are not storing any of the remote document data, we are only able to point you to the filename and page no. (for PPTX & PDF) for the sharepoint document resulted query"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into NeuralDB\n",
    "\n",
    "If you wish to insert without unsupervised training, you can set 'train=False' in the insert() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(insertable_docs, train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command is intended to be used with a base DB which already has reasonable knowledge of the domain. In general, we always recommend using 'train=True' as shown below.\n",
    "\n",
    "#### Insert and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(insertable_docs, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you call the insert() method multiple times, the documents will automatically be de-duplicated. If insert=True, then the training will be done multiple times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "Now let's start searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage: 12. entire agreement. this agreement constitutes the entire agreement with respect to the subject matter hereof and supersedes all prior agreements and understandings between the parties (whether written or oral) relating to the subject matter and may not be amended or modified except in a writing signed by an authorized representative of both parties. the terms of this agreement relating to the confidentiality and non-use of confidential information shall continue after the termination of this agreement for a period of the longer of (i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret under applicable law.\n",
      "************\n",
      "passage: 13. severability. each party acknowledges that should any provision of this agreement be determined to be void invalid or otherwise unenforceable by any court of competent jurisdiction such determination shall not affect the remaining provisions hereof which shall remain in full force and effect.\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = db.search(\n",
    "    query=\"what is the termination period\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage: confidentiality agreement this confidentiality agreement (the “agreement”) is made by and between acme. dba tothemoon inc. with offices at 2025 guadalupe st. suite 260 austin tx 78705 and starwars dba tothemars with offices at the forest moon of endor and entered as of may 3 2023 (“effective date”).\n",
      "************\n",
      "passage: in consideration of the business discussions disclosure of confidential information and any future business relationship between the parties it is hereby agreed as follows: 1. confidential information. for purposes of this agreement the term “confidential information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process request for proposal (rfp) or request for information (rfi) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the effective date.\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = db.search(\n",
    "    query=\"made by and between\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
    "\n",
    "Now let's ask a tricky question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage: 3. joint undertaking. each party agrees that it will not at any time disclose give or transmit in any manner or for any purpose the confidential information received from the other party to any person firm or corporation or use such confidential information for its own benefit or the benefit of anyone else or for any purpose other than to engage in discussions regarding a possible business relationship or the current business relationship involving both parties.\n",
      "************\n",
      "passage: each party shall take all reasonable measures to preserve the confidentiality and avoid the disclosure of the other party’s confidential information including but not limited to those steps taken with respect to the party’s own confidential information of like importance. neither party shall disassemble decompile or otherwise reverse engineer any software product of the other party and to the extent any such activity may be permitted the results thereof shall be deemed confidential information subject to the requirements of this agreement.\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = db.search(\n",
    "    query=\"who are the parties involved?\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! looks like when we search for \"parties involved\", we do not get the correct paragraph in the 1st position (we should be expecting the first paragraph as the correct results instead fo the last). \n",
    "\n",
    "No worries, we'll show shot to teach the model to correct it's retrieval.\n",
    "\n",
    "### RLHF\n",
    "\n",
    "Let's go over some of NeuralDB's advanced features. The first one is text-to-text association. This allows you to teach the model that two keywords, phrases, or concepts are related.\n",
    "\n",
    "Based on the above example, let's teach the model that \"parties involved\" and the phrase \"made by between\" are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.associate(source=\"parties involved\", target=\"made by and between\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search again with the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passage: confidentiality agreement this confidentiality agreement (the “agreement”) is made by and between acme. dba tothemoon inc. with offices at 2025 guadalupe st. suite 260 austin tx 78705 and starwars dba tothemars with offices at the forest moon of endor and entered as of may 3 2023 (“effective date”).\n",
      "************\n",
      "passage: in consideration of the business discussions disclosure of confidential information and any future business relationship between the parties it is hereby agreed as follows: 1. confidential information. for purposes of this agreement the term “confidential information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process request for proposal (rfp) or request for information (rfi) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the effective date.\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "search_results = db.search(\n",
    "    query=\"who are the parties involved?\",\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print('************')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! In just a line, you taught the model to correct itself and retrieve the correct result.\n",
    "\n",
    "Now, let's see the 2nd option which is text-to-result association. Let's say that you know that \"parties involved\" should go the paragraph with DOC_ID=0, you can simply teach the model to associate the query to the corresponding label using the following API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.text_to_result(\"made by and between\",0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the above RLHF methods in a batch instead of a single sample, you can simply use the batched versions of the APIs as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.associate_batch([(\"parties involved\",\"made by and between\"),(\"date of signing\",\"duly executed\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.text_to_result_batch([(\"parties involved\",0),(\"date of signing\",16)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Optional)\n",
    "\n",
    "If you have supervised data for a specific CSV file in your list, you can simply train the DB on that file by specifying a source_id = source_ids[*file_number_in_your_list*].\n",
    "\n",
    "Note: The supervised file should have the query_column and id_column that you specify in the following call. The id_column should match the id_column that you specified in the \"Prep CSV Data\" step or default to \"DOC_ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'Supervised training samples'\n",
      "loaded data | source 'Supervised training samples' | vectors 3 | batches 1 | time 0.007s | complete\n",
      "\n",
      "train | epoch 0 | train_steps 2479 |  | train_batches 1 | time 0.021s   \n",
      "\n",
      "train | epoch 1 | train_steps 2480 |  | train_batches 1 | time 0.019s   \n",
      "\n",
      "train | epoch 2 | train_steps 2481 |  | train_batches 1 | time 0.019s   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sup_files = ['data/sample_nda_sup.csv']\n",
    "\n",
    "db.supervised_train([ndb.Sup(path, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0]) for path in sup_files])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI using Langchain\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"\"  # Enter your OpenAI key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.prompts import qa_prompt\n",
    "from paperqa.chains import make_chain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query):\n",
    "    search_results = db.search(query,top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        references.append(result.text)\n",
    "    return references\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:3]), answer_length=\"abt 50 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['passage: confidentiality agreement this confidentiality agreement (the “agreement”) is made by and between acme. dba tothemoon inc. with offices at 2025 guadalupe st. suite 260 austin tx 78705 and starwars dba tothemars with offices at the forest moon of endor and entered as of may 3 2023 (“effective date”).', 'passage: in consideration of the business discussions disclosure of confidential information and any future business relationship between the parties it is hereby agreed as follows: 1. confidential information. for purposes of this agreement the term “confidential information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process request for proposal (rfp) or request for information (rfi) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the effective date.', 'passage: 14. governing law. this agreement shall be construed for all purposes in accordance with the laws of the state of texas without regard to the conflicts of law provisions of any state or jurisdiction. any action or suit related to this agreement shall be brought in austin texas and each party hereby submits to the exclusive jurisdiction of such courts.']\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the effective date of this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(db, queries):\n",
    "    return db._savable_state.model.model.embedding_representation([{\"query\":query} for query in queries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2048)\n",
      "[[-0.44150677  0.00377565 -0.11008609 ... -0.17087384 -0.10667692\n",
      "  -0.21856037]\n",
      " [-0.43653303 -0.10459366 -0.07299916 ... -0.13396175 -0.03077642\n",
      "  -0.18951523]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(db, ['query 1', 'query 2'])\n",
    "print(embeddings.shape)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save\n",
    "As usual, saving and loading the DB are one-liners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your db\n",
    "db.save(\"sample_nda.ndb\")\n",
    "\n",
    "# Loading is just like we showed above, with an optional progress handler\n",
    "db.from_checkpoint(\"sample_nda.ndb\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "we never store data from the remote documents and thus DB model's functionality gets limited if you save and load the models containing these remote document objects. \n",
    "\n",
    "We do provide the connection re-establishment mechanism in our remote document objects. Also, these remote end-points and remote document content should not changes in any aspect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Re-establishing connection to SQLDatabse\n",
    "First, find the corressponding document object\n",
    "\n",
    "```\n",
    "docs = db.sources()\n",
    "```\n",
    "It returns an Ordered-dict of `{doc-source: DB doc object}` \n",
    "\n",
    "For ex: `{'16cd6625838b9d5f97e1a49a9f0a6189c90b1b2d': <thirdai.neural_db.documents.SQLDatabase at 0x7f6b04ca23e0>, ...}`\n",
    "\n",
    "you can verify that this is the required one by iterating over this dictionary and printing their name\n",
    "```python\n",
    "for source, doc in docs.items():\n",
    "    print(f\"{doc.name = } and {source = }\")\n",
    "```\n",
    "\n",
    "\n",
    "And later \n",
    "\n",
    "```python\n",
    "sql_doc = docs['16cd6625838b9d5f97e1a49a9f0a6189c90b1b2d']\n",
    "engine = create_engine(url = db_url)\n",
    "db_doc.setup_connection(engine = engine)\n",
    "```\n",
    "\n",
    "It would be re-connected to the same table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#### Re-establishing connection to SharePoint\n",
    "\n",
    "find the corressponding document object as shown above\n",
    "\n",
    "Then, provide the same `clientContext` object\n",
    "\n",
    "```python\n",
    "sp_doc.setup_connection(ctx = ctx)\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
