{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's NeuralDB\n",
    "\n",
    "NeuralDB, provides a high-level API for users to insert different types of files into it and search through the file contents with natural language queries.\n",
    "\n",
    "First, let's install the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import licensing, neural_db as ndb\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import os\n",
    "\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a neural db class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ndb.NeuralDB()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert your files\n",
    "\n",
    "Let's insert things into it!\n",
    "\n",
    "Currently, we natively support adding CSV, PDF and DOCX files. We also have a support to automatically scrape and parse URLs. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: CSV files\n",
    "The first example below shows how to insert a CSV file. Please note that a CSV file is required to have a column named \"DOC_ID\" with rows numbered from 0 to n_rows-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "csv_files = [\"data/sample_nda.csv\"]\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = ndb.CSV(\n",
    "        path=file,\n",
    "        id_column=\"DOC_ID\",\n",
    "        strong_columns=[\"passage\"],\n",
    "        weak_columns=[\"para\"],\n",
    "        reference_columns=[\"passage\"],\n",
    "    )\n",
    "    #\n",
    "    insertable_docs.append(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "pdf_files = [\"data/sample_nda.pdf\"]\n",
    "\n",
    "for file in pdf_files:\n",
    "    pdf_doc = ndb.PDF(file)\n",
    "    insertable_docs.append(pdf_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: DOCX files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "doc_files = [\"data/sample_nda.docx\"]\n",
    "\n",
    "for file in doc_files:\n",
    "    doc = ndb.DOCX(file)\n",
    "    insertable_docs.append(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 4: Parse from URLs directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can use our utility to generate a set of candidate URLs containing data of interest. You can also use your own list of URLs to extract data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_url_data = ndb.parsing_utils.recursive_url_scrape(\n",
    "    base_url=\"https://www.thirdai.com/pocketllm/\", max_crawl_depth=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can create a list of insertable documents from those URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_docs = []\n",
    "for url, response in valid_url_data:\n",
    "    try:\n",
    "        url_docs.append(ndb.URL(url, response))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# The data in the URL is not relevant to the demo, this is just to illustrate how\n",
    "# url data can be incorporated into NeuralDB\n",
    "# insertable_docs += url_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These insertable docs can be inserted into Neural DB just like with any other document type that we support (as shown below)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into NeuralDB\n",
    "\n",
    "If you wish to insert without unsupervised training, you can set 'train=False' in the insert() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing a checkpoint config (optional) while inserting will checkpoint the DB state in the specified location.\n",
    "db.insert(insertable_docs, train=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command is intended to be used with a base DB which already has reasonable knowledge of the domain. In general, we always recommend using 'train=True' as shown below.\n",
    "\n",
    "#### Insert and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(insertable_docs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpointing Configuration for NeuralDB (Optional)\n",
    "\n",
    "When we insert with Train=True, to facilitate recovery in case of machine failures, a straightforward checkpoint configuration can be provided as shown below. This configuration allows users to define where and how often checkpoints are saved, and whether to resume from a checkpoint if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_config = ndb.CheckpointConfig(\n",
    "    checkpoint_dir=\"./data/sample_checkpoint\",  # Specify the location for storing checkpoint data\n",
    "    resume_from_checkpoint=False,  # Set to True if you want to resume from a checkpoint\n",
    "    checkpoint_interval=3,  # Granularity of checkpoints (lower value implies more frequent checkpoints)\n",
    ")\n",
    "\n",
    "source_ids = db.insert(insertable_docs, train=True, checkpoint_config=checkpoint_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you call the insert() method multiple times, the documents will automatically be de-duplicated. If insert=True, then the training will be done multiple times."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "Now let's start searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(query=\"what is the termination period\", top_k=2)\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(query=\"made by and between\", top_k=2)\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
    "\n",
    "Now let's ask a tricky question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(query=\"who are the parties involved?\", top_k=2)\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.context(radius=1))\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! looks like when we search for \"parties involved\", we do not get the correct paragraph in the 1st position (we should be expecting the first paragraph as the correct results instead fo the last). \n",
    "\n",
    "No worries, we'll show shot to teach the model to correct it's retrieval.\n",
    "\n",
    "### RLHF\n",
    "\n",
    "Let's go over some of NeuralDB's advanced features. The first one is text-to-text association. This allows you to teach the model that two keywords, phrases, or concepts are related.\n",
    "\n",
    "Based on the above example, let's teach the model that \"parties involved\" and the phrase \"made by between\" are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.associate(source=\"who are the parties involved\", target=\"made by and between\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search again with the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"who are the parties involved?\",\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    "    print(\"************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! In just a line, you taught the model to correct itself and retrieve the correct result.\n",
    "\n",
    "Now, let's see the 2nd option which is text-to-result association. Let's say that you know that \"parties involved\" should go the paragraph with DOC_ID=0, you can simply teach the model to associate the query to the corresponding label using the following API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.text_to_result(\"made by and between\", 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the above RLHF methods in a batch instead of a single sample, you can simply use the batched versions of the APIs as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.associate_batch(\n",
    "    [(\"parties involved\", \"made by and between\"), (\"date of signing\", \"duly executed\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.text_to_result_batch([(\"parties involved\", 0), (\"date of signing\", 16)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Optional)\n",
    "\n",
    "If you have supervised data for a specific CSV file in your list, you can simply train the DB on that file by specifying a source_id = source_ids[*file_number_in_your_list*].\n",
    "\n",
    "Note: The supervised file should have the query_column and id_column that you specify in the following call. The id_column should match the id_column that you specified in the \"Prep CSV Data\" step or default to \"DOC_ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_files = [\"data/sample_nda_sup.csv\"]\n",
    "\n",
    "db.supervised_train(\n",
    "    [\n",
    "        ndb.Sup(path, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0])\n",
    "        for path in sup_files\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI using Langchain\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = None  # Enter your OpenAI key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.utils import generate_answers\n",
    "\n",
    "\n",
    "def get_references(query):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        references.append(result.text)\n",
    "    return references\n",
    "\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return generate_answers(\n",
    "        query=query,\n",
    "        references=references,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the effective date of this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save(\"sample_nda.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the saved DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading is just like we showed above, with an optional progress handler\n",
    "new_db = ndb.NeuralDB.from_checkpoint(\n",
    "    \"sample_nda.ndb\",\n",
    "    on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
