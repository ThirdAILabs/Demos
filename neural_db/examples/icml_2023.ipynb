{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA with 1800+ papers from ICML 2023\n",
    "\n",
    "In this notebook, you will be able to\n",
    "\n",
    "1. Download ThirdAI's Neural DB trained on all the papers accepted to ICML 2023.\n",
    "\n",
    "2. Ask any question and get relevant references from any of the 1800+ papers.\n",
    "\n",
    "3. (Optional) How to use your OpenAI key to generate grounded answers without hallucination.\n",
    "\n",
    "To learn more about NeuralDB and it's capabilities, refer to our demo here: https://github.com/ThirdAILabs/Demos/blob/main/neural_db/main_example.ipynb\n",
    "\n",
    "\n",
    "You can optionally run this notebook in Google Colab using this link: https://githubtocolab.com/ThirdAILabs/Demos/blob/main/neural_db/examples/icml_2023.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade\n",
    "!pip3 install \"thirdai[neural_db]\"  # You may have to unquote this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import licensing\n",
    "import os\n",
    "\n",
    "from utils import generate_answers\n",
    "\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\n",
    "        \"002099-64C584-3E02C8-7E51A0-DE65D9-V3\"\n",
    "    )  # Enter your ThirdAI key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb\n",
    "\n",
    "db = ndb.NeuralDB(\n",
    "    user_id=\"my_user\"\n",
    ")  # you can use any username, in the future, this username will let you push models to the model bazaar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download and extract the ICML 2023 Neural DB checkpoint\n",
    "## This step takes ~2 mins\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "checkpoint = \"./icml_2023\"\n",
    "if not os.path.exists(checkpoint):\n",
    "    if not os.path.exists(checkpoint + \".zip\"):\n",
    "        os.system(\n",
    "            \"wget -nv -O \"\n",
    "            + checkpoint\n",
    "            + \".zip 'https://www.dropbox.com/scl/fi/8q1yni3ncf0a82qyrzj3k/icml_2023.zip?rlkey=t47p8bu4x1l8arvycpemz2htm&dl=0'\"\n",
    "        )\n",
    "\n",
    "    with zipfile.ZipFile(checkpoint + \".zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the checkpoint\n",
    "\n",
    "db = ndb.NeuralDB.from_checkpoint(\"icml_2023/icml_2023\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"some ideas to align memory access pattern with parameter access pattern and cache hierarchies\",\n",
    "    top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This paper proposes a model-agnostic cache-friendly and hardware- aware model compression approach: Random Operation Access Specific Tile (ROAST) hash- ing. ROAST collapses the parameters by club- bing them through a lightweight mapping. While clubbing these parameters ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern.\n",
      "************\n",
      "Separability Constant k. Our data generating distribu- tion Dk with larger k is more separable in the sense that the two Gaussian distributions overlap less and they are more likely to generate well-separated training data. This separa- bility constant k is of great importance in our analysis as we will demonstrate the curse of separability phenomenon based on the dependency of sample complexity on k.\n",
      "************\n"
     ]
    }
   ],
   "source": [
    "for result in search_results:\n",
    "    print(result.text)\n",
    "    # print(result.source)\n",
    "    print(\"************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI using Langchain\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like LLAMA 2 or MPT for answer generation with the same prompt that you use with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = None  # Enter your OpenAI key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        references.append(result.text)\n",
    "    return references\n",
    "\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return generate_answers(\n",
    "        query=query,\n",
    "        references=references,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This paper proposes a model-agnostic cache-friendly and hardware- aware model compression approach: Random Operation Access Specific Tile (ROAST) hash- ing. ROAST collapses the parameters by club- bing them through a lightweight mapping. While clubbing these parameters ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern.', 'Separability Constant k. Our data generating distribu- tion Dk with larger k is more separable in the sense that the two Gaussian distributions overlap less and they are more likely to generate well-separated training data. This separa- bility constant k is of great importance in our analysis as we will demonstrate the curse of separability phenomenon based on the dependency of sample complexity on k.', 'This indicates that as the number of training samples/classes increases there exists a progressively larger mismatch between the knowl- edge learned from ImageNet and the knowledge needed for distinguishing new classes in these two datasets. Thus training a large model on a big dataset that can solve every possible task well is not a realistic hope unless the training dataset already contains all possible tasks.How to choose a part of the training dataset to train a model on or how to select positive/useful knowledge from a learned model depending on only a small amount of data in the specified adaptation scenario is an important research direction in few-shot classification.']\n"
     ]
    }
   ],
   "source": [
    "query = \"some ideas to align memory access pattern with parameter access pattern and cache hierarchies\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To align memory access pattern with parameter access pattern and cache hierarchies, the ROAST hashing approach compresses parameters by mapping them together efficiently. By utilizing cache hierarchies, ROAST ensures that memory access aligns with parameter access, optimizing performance. This method is model-agnostic, cache-friendly, and hardware-aware, providing a solution for efficient memory management in machine learning models.\n"
     ]
    }
   ],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
