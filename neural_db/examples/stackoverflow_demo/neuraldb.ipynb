{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade\n",
    "!pip3 install \"thirdai[neural_db]\"  # You may have to unquote this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pratyush/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from thirdai import licensing, neural_db as ndb\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import os\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here\n",
    "    \n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 10:00:10 URL:https://uc2ff0f28034af7151cc590a892f.dl.dropboxusercontent.com/cd/0/inline/CO56XcCi4QgH5aqBCiA-LfqTmYqg6zXn-CS8G5D4dBmr2KWnKLMLJUcT-jfBGsADcSUtVnvLZfvUTLkiU5xT52vfSsibHIs5TkQh1l-bhluPRz4c-Kv-4Xo-A1Jof3SrlzSmzK_TLDOKDuN4qal6krg7/file [26625547/26625547] -> \"./stackoverflow_data/train.csv\" [1]\n",
      "2024-03-11 10:00:11 URL:https://uc79d18f4a29378589b05ac80d77.dl.dropboxusercontent.com/cd/0/inline/CO6TqT9id6BmVIZEVcPX5UExmUosZ4crTh88iWPcX6c5UeB6PiKN0kqXMZclZ2NlCrz8Ch3NwdT0ESXbGAeo3KpECFHcuEuTDL2w-Nvhj0Qzuj6byf7JNFo-PA2N9sPU9UwuiW62912oSYWOfhs4qyar/file [136081/136081] -> \"./stackoverflow_data/test.csv\" [1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"./stackoverflow_data/\"\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.system(\"mkdir \"+data_dir)\n",
    "\n",
    "os.system(\"wget -nv -O \"+data_dir+\"train.csv 'https://www.dropbox.com/scl/fi/e7ltns3teqmngai3fp71o/stackoverflow_train.csv?rlkey=f9pq0amo5swfrdi1hcvk52cuc&dl=0'\")\n",
    "os.system(\"wget -nv -O \"+data_dir+\"test.csv 'https://www.dropbox.com/scl/fi/9rqf0nf2cmti0ihn5x4uu/stackoverflow_test.csv?rlkey=cn3ttfenhgxrsaemidnat0kt2&dl=0'\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = data_dir+\"train.csv\"\n",
    "test_file = data_dir+\"test.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize NeuralDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ndb.NeuralDB(fhr=100_000, \n",
    "                  embedding_dimension=1024,\n",
    "                  extreme_output_dim=4000,\n",
    "                  extreme_num_hashes=4, \n",
    "                  use_inverted_index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate metrics\n",
    "\n",
    "-   Relevance denotes that model predicts a revalant answer to query\n",
    "-   Helpfulness denotes that model predicts the highest scoring answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(db):\n",
    "    def precision_at_k(predicted, original, k):\n",
    "        total_precision = 0\n",
    "        for pred_instance, orig_instance in zip(predicted, original):\n",
    "            total_precision += len(set(pred_instance[:k]) & set(orig_instance)) / k\n",
    "        return total_precision / len(predicted) \n",
    "    test_df = pd.read_csv(test_file)\n",
    "    questions = test_df['query'].to_list()\n",
    "    true_labels_all = list(map(lambda x: list(map(int, x.split(\",\"))), test_df['ids'].to_list()))\n",
    "    predicted_all, original_all, original_top  = [], [], []\n",
    "    results = db.search_batch(questions, top_k=1)\n",
    "    for result, true_labels in list(zip(results, true_labels_all)):\n",
    "        predicted_labels = [res.metadata['id'] for res in result]\n",
    "        predicted_all.append(predicted_labels)\n",
    "        original_all.append(true_labels)\n",
    "        original_top.append(true_labels[:1])\n",
    "    print(\"Relevance Precision@1 =\", precision_at_k(predicted_all, original_all, 1)) \n",
    "    print(\"Helpfulness Precision@1 =\", precision_at_k(predicted_all, original_top, 1)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training on answer and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.195s | complete\n",
      "\n",
      "train | epoch 0 | train_steps 10 | train_hash_precision@5=0.04441  | train_batches 10 | time 2.748s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.103s | complete\n",
      "\n",
      "train | epoch 1 | train_steps 20 | train_hash_precision@5=0.06808  | train_batches 10 | time 1.586s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.164s | complete\n",
      "\n",
      "train | epoch 2 | train_steps 30 | train_hash_precision@5=0.09506  | train_batches 10 | time 1.547s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.059s | complete\n",
      "\n",
      "train | epoch 3 | train_steps 40 | train_hash_precision@5=0.12612  | train_batches 10 | time 1.485s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.071s | complete\n",
      "\n",
      "train | epoch 4 | train_steps 50 | train_hash_precision@5=0.16191  | train_batches 10 | time 1.558s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.044s | complete\n",
      "\n",
      "train | epoch 5 | train_steps 60 | train_hash_precision@5=0.24899  | train_batches 10 | time 1.531s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.103s | complete\n",
      "\n",
      "train | epoch 6 | train_steps 70 | train_hash_precision@5=0.3494  | train_batches 10 | time 1.521s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.132s | complete\n",
      "\n",
      "train | epoch 7 | train_steps 80 | train_hash_precision@5=0.44446  | train_batches 10 | time 1.460s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.005s | complete\n",
      "\n",
      "train | epoch 8 | train_steps 90 | train_hash_precision@5=0.52764  | train_batches 10 | time 1.456s\n",
      "\n",
      "loading data | source 'Documents:\n",
      "train.csv'\n",
      "loading data | source 'Documents:\n",
      "train.csv' | vectors 20000 | batches 10 | time 2.211s | complete\n",
      "\n",
      "train | epoch 9 | train_steps 100 | train_hash_precision@5=0.5956  | train_batches 10 | time 1.600s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file = ndb.CSV(train_file, id_column=\"id\", strong_columns=['title', 'answer'])\n",
    "source_ids = db.insert([csv_file], train=True, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Precision@1 = 0.04522613065326633\n",
      "Helpfulness Precision@1 = 0.04020100502512563\n"
     ]
    }
   ],
   "source": [
    "test(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised training on questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'Supervised training samples'\n",
      "loading data | source 'Supervised training samples' | vectors 20000 | batches 10 | time 0.826s | complete\n",
      "\n",
      "train | epoch 10 | train_steps 110 |  | train_batches 10 | time 2.980s  \n",
      "\n",
      "train | epoch 11 | train_steps 120 |  | train_batches 10 | time 2.978s  \n",
      "\n",
      "train | epoch 12 | train_steps 130 |  | train_batches 10 | time 2.972s  \n",
      "\n",
      "train | epoch 13 | train_steps 140 |  | train_batches 10 | time 3.004s  \n",
      "\n",
      "train | epoch 14 | train_steps 150 |  | train_batches 10 | time 3.005s  \n",
      "\n",
      "train | epoch 15 | train_steps 160 |  | train_batches 10 | time 2.620s  \n",
      "\n",
      "train | epoch 16 | train_steps 170 |  | train_batches 10 | time 2.921s  \n",
      "\n",
      "train | epoch 17 | train_steps 180 |  | train_batches 10 | time 2.944s  \n",
      "\n",
      "train | epoch 18 | train_steps 190 |  | train_batches 10 | time 3.111s  \n",
      "\n",
      "train | epoch 19 | train_steps 200 |  | train_batches 10 | time 3.085s  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sup_data = ndb.Sup(\n",
    "            train_file,\n",
    "            query_column=\"query\",\n",
    "            id_delimiter=\"\",\n",
    "            id_column=\"id\",\n",
    "            source_id=source_ids[0],\n",
    "        )\n",
    "db.supervised_train([sup_data], learning_rate=0.001, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Precision@1 = 0.9547738693467337\n",
      "Helpfulness Precision@1 = 0.7738693467336684\n"
     ]
    }
   ],
   "source": [
    "test(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upvoting highest scoring answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_file)\n",
    "train_df = train_df.sort_values('score', ascending=False).groupby('query').first().reset_index()\n",
    "batches_to_upvote=[(row.query, row.id) for row in train_df.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(batches_to_upvote)\n",
    "\n",
    "db.text_to_result_batch(batches_to_upvote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Precision@1 = 0.9597989949748744\n",
      "Helpfulness Precision@1 = 0.9195979899497487\n"
     ]
    }
   ],
   "source": [
    "test(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = textwrap.TextWrapper(width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You can build a markov-chain of a huge english text. Afterwards you can feed words into the markov\n",
      "chain and check how high the probability is that the word is english. See here:\n",
      "http://en.wikipedia.org/wiki/Markov_chain At the bottom of the page you can see the markov text\n",
      "generator. What you want is exactly the reverse of it. In a nutshell: The markov-chain stores for\n",
      "each character the probabilities of which next character will follow. You can extend this idea to\n",
      "two or three characters if you have enough memory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"I possess an algorithm that produces strings from a list of input words. \n",
    "How do I isolate only the strings that sound like English words? ie. reject \n",
    "RDLO while retaining LORD. EDIT: To clarify, they don't have to be real words \n",
    "in the dictionary. They just need to resemble English. For instance, KEAL would be \n",
    "acceptable.\"\n",
    "\"\"\"\n",
    "\n",
    "results = db.search(query,top_k=1)\n",
    "for result in results:\n",
    "    answer = result.metadata['answer']\n",
    "    wrapped_text = wrapper.wrap(text = answer)\n",
    "    for element in wrapped_text:\n",
    "        print(element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You basically need 2 tests. 1) Pass in a string like \"The Quick Brown Fox Jumps!\" (length greater\n",
      "than five) makes sure that the value is affected by replaceit(...) 2) Pass in a string like \"Foo\"\n",
      "(length is less than five) and make sure that the value is affected by changeit(...) Your test (in\n",
      "pseudo code) might look like this: testLongValue() { string testValue = \"A value longer than 5\n",
      "chars\"; string expected = \"Replaced!\"; string actual = modify(testValue); assertEqual(expected,\n",
      "actual); } testShortValue() { string testValue = \"len4\"; string expected = \"Changed!\"; string actual\n",
      "= modify(testValue); assertEqual(expected, actual); } Obviously I could give you a more realistic\n",
      "example if I knew what replacit() and changeit() were supposed to do, but this should give you the\n",
      "idea. If it mutates the original value reference instead of returning it, you can just use testValue\n",
      "as the actual value after the call occurs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"What is the optimal way to unit test a method that invokes multiple methods, \n",
    "for instance: modify(string value) { if(value.Length &gt; 5) replaceit(value); else \n",
    "changeit(value); } This pseudo code has a modify method that (currently) calls either replaceit() \n",
    "or changeit() . I have already written tests for replaceit and changeit , so creating a new test for\n",
    "modify will be 99% the same set of code. I need to test it though because it might change \n",
    "in the future. So do I duplicate the existing test code? Shift the test code to a common function? \n",
    "Any other suggestions? I'm uncertain of the best practice here.\n",
    "\"\"\"\n",
    "\n",
    "results = db.search(query,top_k=1)\n",
    "for result in results:\n",
    "    answer = result.metadata['answer']\n",
    "    wrapped_text = wrapper.wrap(text = answer)\n",
    "    for element in wrapped_text:\n",
    "        print(element)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The question is pretty broad, but I'm partial to markup-based UIs. Here's a window with a text box\n",
      "in WPF: &lt;Window xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"&gt;\n",
      "&lt;Grid&gt; &lt;TextBox x:Name=\"InputBox\"/&gt; &lt;/Grid&gt; &lt;/Window&gt; Now I won't even try\n",
      "to claim that WPF has the shortest learning curve, but it is the most powerful on Windows and it's\n",
      "pretty easy to pick up with the right tooling. (i.e. Expression Blend). Blend isn't cheap but some\n",
      "folks already have it for free and don't know it (students, MSDN subscribers, some startups). Visual\n",
      "Studio 2010 is much improved in this area too, so Blend may not be needed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"I am seeking to develop a very lightweight GUI front end in Windows.\n",
    "It's meant to perform a simple task - when a hot key combination is pressed it\n",
    "opens up a text box. Any text can be pasted in and then saved with a simple text \n",
    "box. I am aiming to avoid any menu bar or toolbars completely. What would be the\n",
    "perfect GUI library to create something like this?\"\"\"\n",
    "\n",
    "results = db.search(query,top_k=1)\n",
    "for result in results:\n",
    "    answer = result.metadata['answer']\n",
    "    wrapped_text = wrapper.wrap(text = answer)\n",
    "    for element in wrapped_text:\n",
    "        print(element)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
