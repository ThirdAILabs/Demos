{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Code Search System**\n",
    "We will build a code search system based on the Langchain codebase. Our system will be able to answer queries such as:\n",
    "* get relevant documents in arxiv retriever\n",
    "* base class for retriever that does not use vector store\n",
    "* bm25 retriever test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clone the Langchain Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 171026, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 171026 (delta 0), reused 0 (delta 0), pack-reused 171025\u001b[K\n",
      "Receiving objects: 100% (171026/171026), 231.80 MiB | 37.50 MiB/s, done.\n",
      "Resolving deltas: 100% (127816/127816), done.\n",
      "Updating files: 100% (7421/7421), done.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: thirdai in /home/gautam/.local/lib/python3.8/site-packages (0.8.4)\n",
      "Requirement already satisfied: Office365-REST-Python-Client==2.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.5.1)\n",
      "Requirement already satisfied: PyMuPDF==1.23.26 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.23.26)\n",
      "Requirement already satisfied: PyTrie in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.4.0)\n",
      "Requirement already satisfied: SQLAlchemy>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.0.17)\n",
      "Requirement already satisfied: bs4 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.0.1)\n",
      "Requirement already satisfied: dask[complete] in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2023.5.0)\n",
      "Requirement already satisfied: ipython in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (8.12.2)\n",
      "Requirement already satisfied: langchain in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.1.16)\n",
      "Requirement already satisfied: langchain-community in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.0.33)\n",
      "Requirement already satisfied: lxml[html_clean] in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.9.3)\n",
      "Requirement already satisfied: nltk in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.24.4)\n",
      "Requirement already satisfied: openai in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.19.0)\n",
      "Requirement already satisfied: pandas<=2.1.4,>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.0.3)\n",
      "Requirement already satisfied: pdfplumber in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.10.3)\n",
      "Requirement already satisfied: pydantic in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.4.2)\n",
      "Requirement already satisfied: python-docx in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.8.11)\n",
      "Requirement already satisfied: requests in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.2.2)\n",
      "Requirement already satisfied: simple-salesforce==1.12.5 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.12.5)\n",
      "Requirement already satisfied: sortedcontainers in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.65.0)\n",
      "Requirement already satisfied: trafilatura in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.11.0)\n",
      "Requirement already satisfied: unidecode in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.3.6)\n",
      "Requirement already satisfied: url-normalize in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.4.3)\n",
      "Requirement already satisfied: msal in /home/gautam/.local/lib/python3.8/site-packages (from Office365-REST-Python-Client==2.5.1->thirdai) (1.24.1)\n",
      "Requirement already satisfied: pytz in /home/gautam/.local/lib/python3.8/site-packages (from Office365-REST-Python-Client==2.5.1->thirdai) (2023.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/gautam/.local/lib/python3.8/site-packages (from PyMuPDF==1.23.26->thirdai) (1.23.22)\n",
      "Requirement already satisfied: cryptography in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (42.0.1)\n",
      "Requirement already satisfied: zeep in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (4.2.1)\n",
      "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from simple-salesforce==1.12.5->thirdai) (1.7.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from simple-salesforce==1.12.5->thirdai) (4.2.0)\n",
      "Requirement already satisfied: pendulum in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gautam/.local/lib/python3.8/site-packages (from pandas<=2.1.4,>=2.0.0->thirdai) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gautam/.local/lib/python3.8/site-packages (from pandas<=2.1.4,>=2.0.0->thirdai) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/gautam/.local/lib/python3.8/site-packages (from SQLAlchemy>=2.0.0->thirdai) (2.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/gautam/.local/lib/python3.8/site-packages (from bs4->thirdai) (4.8.2)\n",
      "Requirement already satisfied: click>=8.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask[complete]->thirdai) (5.3.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (6.7.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (12.0.1)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (4.3.3)\n",
      "Requirement already satisfied: backcall in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython->thirdai) (4.6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.1.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.1.48)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from pydantic->thirdai) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/gautam/.local/lib/python3.8/site-packages (from pydantic->thirdai) (2.10.1)\n",
      "\u001b[33mWARNING: lxml 4.9.3 does not provide the extra 'html-clean'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: joblib in /home/gautam/.local/lib/python3.8/site-packages (from nltk->thirdai) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from nltk->thirdai) (2023.8.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (1.3.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (4.26.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/gautam/.local/lib/python3.8/site-packages (from scikit-learn->thirdai) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from scikit-learn->thirdai) (3.1.0)\n",
      "Requirement already satisfied: courlan>=0.9.4 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (0.9.4)\n",
      "Requirement already satisfied: htmldate>=1.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (1.5.1)\n",
      "Requirement already satisfied: justext>=3.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (3.0.0)\n",
      "Requirement already satisfied: six in /home/gautam/.local/lib/python3.8/site-packages (from url-normalize->thirdai) (1.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gautam/.local/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai->thirdai) (1.1.3)\n",
      "Requirement already satisfied: langcodes>=3.3.0 in /home/gautam/.local/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura->thirdai) (3.3.0)\n",
      "Requirement already satisfied: tld>=0.13 in /home/gautam/.local/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura->thirdai) (0.13)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/gautam/.local/lib/python3.8/site-packages (from cryptography->simple-salesforce==1.12.5->thirdai) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gautam/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (0.9.0)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /home/gautam/.local/lib/python3.8/site-packages (from htmldate>=1.5.1->trafilatura->thirdai) (1.1.8)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gautam/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai->thirdai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gautam/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->thirdai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gautam/.local/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask[complete]->thirdai) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython->thirdai) (0.8.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain->thirdai) (2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/gautam/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->thirdai) (3.10.1)\n",
      "Requirement already satisfied: locket in /home/gautam/.local/lib/python3.8/site-packages (from partd>=1.2.0->dask[complete]->thirdai) (1.0.0)\n",
      "Requirement already satisfied: wcwidth in /home/gautam/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->thirdai) (0.2.6)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/gautam/.local/lib/python3.8/site-packages (from beautifulsoup4->bs4->thirdai) (2.5)\n",
      "Requirement already satisfied: distributed==2023.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2023.5.0)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (3.1.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (3.1.2)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (1.0.5)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (5.9.5)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (6.3.3)\n",
      "Requirement already satisfied: zict>=2.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (3.0.0)\n",
      "Requirement already satisfied: backports.zoneinfo>=0.2.1 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (0.2.1)\n",
      "Requirement already satisfied: time-machine>=2.6.0 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (2.13.0)\n",
      "Requirement already satisfied: importlib-resources>=5.9.0 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (5.12.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (0.2.2)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (0.6.1)\n",
      "Requirement already satisfied: platformdirs>=1.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (3.10.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.7.1 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (1.0.0)\n",
      "Requirement already satisfied: requests-file>=1.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1 in /home/gautam/.local/lib/python3.8/site-packages (from bokeh>=2.4.2->dask[complete]->thirdai) (1.1.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/gautam/.local/lib/python3.8/site-packages (from bokeh>=2.4.2->dask[complete]->thirdai) (2024.4.0)\n",
      "Requirement already satisfied: pycparser in /home/gautam/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography->simple-salesforce==1.12.5->thirdai) (2.21)\n",
      "Requirement already satisfied: tzlocal in /home/gautam/.local/lib/python3.8/site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura->thirdai) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gautam/.local/lib/python3.8/site-packages (from jinja2>=2.10.3->dask[complete]->thirdai) (2.1.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gautam/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (1.0.0)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git\n",
    "\n",
    "# And install the thirdai package\n",
    "%pip install thirdai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "To ensure that each chunk is semantically coherent, we will split it along class and function boundaries. In addition, for our use case, it's important to know which file, class, and/or function a snippet is taken from. This kind of information is perfect for utilizing NeuralDB's notion of strong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "def apply_to_codebase(path_to_codebase, chunking_strategy):\n",
    "    \"\"\"Traverses entire codebase and applies chunking strategy to all files.\n",
    "    Returns a dataframe with 5 columns: id, chunk, path_to_file, lineno, end_lineno\n",
    "    `lineno` is the line number (not line index) that the chunk starts on.\n",
    "    `end_lineno` is the chunk's last line number (again, not line index).\n",
    "    For example, \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    warn = False\n",
    "    for path_to_file in glob.iglob(f\"{path_to_codebase}/**/*.*\", recursive = True):\n",
    "        if path_to_file.endswith(\".py\"):\n",
    "            try:\n",
    "                script = open(path_to_file).read()\n",
    "                ast_body = ast.parse(script).body\n",
    "                script_lines = script.splitlines(keepends=True)\n",
    "                df = chunking_strategy(ast_body, script_lines)\n",
    "                df[\"path_to_file\"] = [path_to_file for _ in range(len(df))]\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to open\", path_to_file)\n",
    "                print(\"Reason:\", e)\n",
    "                print(\"Skipping...\")\n",
    "        else:\n",
    "            warn = True\n",
    "        \n",
    "    if warn:\n",
    "        warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def split_by_function(ast_body: List[ast.AST], script_lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ast_body: List of elements in the Python script as returned by `ast.parse(script).body`\n",
    "    script_lines: List of lines in the Python script\n",
    "\n",
    "    The script will be split into snippets according to these rules:\n",
    "    - Each function is a chunk\n",
    "    - Each method of a class is a chunk\n",
    "    - Expressions between functions or classes are clubbed together.\n",
    "    - Comments (not docstrings) are clubbed with the next chunk\n",
    "\n",
    "    This chunking method produces a dataframe with four columns:\n",
    "    - snippet: A snippet from the codebase\n",
    "    - trace: The stack trace of the snippet; the class and/or function from which\n",
    "      the snippet is taken\n",
    "    - lineno: The line number where the snippet starts. Note that it is 1-indexed,\n",
    "      which is consistent with the lineno returned by the AST module.\n",
    "    - end_lineno: The line number of the last line of the snippet. Like lineno, \n",
    "      it is 1-indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    start_linenos, end_linenos, traces = _split_by_function(\n",
    "        ast_body=ast_body,\n",
    "        start_lineno=1,\n",
    "        end_lineno=len(script_lines),\n",
    "    )\n",
    "    \n",
    "    # Only keep non-empty lines.\n",
    "\n",
    "    for i in range(len(start_linenos)):\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[start_linenos[i] - 1].strip():\n",
    "            start_linenos[i] += 1\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[end_linenos[i] - 1].strip():\n",
    "            end_linenos[i] -= 1\n",
    "    \n",
    "    snippets = []\n",
    "    final_traces = []\n",
    "    final_linenos = []\n",
    "    final_end_linenos = []\n",
    "\n",
    "    for lineno, end_lineno, snippet_trace in zip(start_linenos, end_linenos, traces):\n",
    "        if lineno <= end_lineno:\n",
    "            snippets.append(\"\".join(script_lines[lineno - 1: end_lineno]))\n",
    "            final_traces.append(snippet_trace)\n",
    "            final_linenos.append(lineno)\n",
    "            final_end_linenos.append(end_lineno)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"snippet\": snippets,\n",
    "        \"trace\": final_traces,\n",
    "        \"lineno\": final_linenos,\n",
    "        \"end_lineno\": final_end_linenos,\n",
    "    })\n",
    "\n",
    "def _split_by_function(ast_body: List[ast.AST], start_lineno: int, end_lineno: int):\n",
    "    \"\"\"This helper function allows us to reuse chunking logic within a scope\n",
    "    such as a class.\n",
    "    \"\"\"\n",
    "    start_linenos = []\n",
    "    end_linenos = []\n",
    "    traces = []\n",
    "    can_connect = False\n",
    "\n",
    "    # start_lineno is always the previous end_lineno + 1\n",
    "    # This is because comments are not captured by the AST parser.\n",
    "    # Thus, to keep comments, we must keep all lines between the previous\n",
    "    # element and the current element.\n",
    "\n",
    "    for elem in ast_body:\n",
    "        if isinstance(elem, ast.FunctionDef):\n",
    "            # A function block is always its own chunk\n",
    "            start_linenos.append(start_lineno)\n",
    "            end_linenos.append(elem.end_lineno)\n",
    "            # Add function name to trace.\n",
    "            traces.append(f\"function name: {elem.name}\")\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        elif isinstance(elem, ast.ClassDef):\n",
    "            # A class is treated as a mini-script;\n",
    "            # functions/methods inside a class are their own snippets.\n",
    "            class_start_linenos, class_end_linenos, class_trace = _split_by_function(\n",
    "                ast_body=elem.body,\n",
    "                start_lineno=start_lineno,\n",
    "                end_lineno=elem.end_lineno,\n",
    "            )\n",
    "            start_linenos.extend(class_start_linenos)\n",
    "            end_linenos.extend(class_end_linenos)\n",
    "            # Prepend class name to the trace of every snippet in the class.\n",
    "            traces.extend([f\"class name: {elem.name}. {trace}\" for trace in class_trace])\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        else:\n",
    "            # Group expressions in the global scope that are neither functions\n",
    "            # nor classes.\n",
    "            if can_connect:\n",
    "                end_linenos[-1] = elem.end_lineno\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "            else:\n",
    "                start_linenos.append(start_lineno)\n",
    "                end_linenos.append(elem.end_lineno)\n",
    "                # Append an empty string so `traces`` is always the same length\n",
    "                # as start_linenos and end_linenos\n",
    "                traces.append(\"\")\n",
    "                can_connect = True\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "    start_linenos.append(start_lineno)\n",
    "    end_linenos.append(end_lineno)\n",
    "\n",
    "    return start_linenos, end_linenos, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open ./langchain/libs/langchain/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361461/476235626.py:33: RuntimeWarning: Found non-Python files in the codebase. This script only snippets python code.\n",
      "  warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "langchain_snippets = apply_to_codebase(\"./langchain\", split_by_function)\n",
    "langchain_snippets.to_csv(\"langchain.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build NeuralDB**\n",
    "As previously mentioned, NeuralDB has a notion of strong and weak columns.\n",
    "\n",
    "`strong_columns` are columns in your CSV file that contains “strong” signals; words or strings that you want exact matches with, such as keywords, brands, categories, or a stack trace.\n",
    "\n",
    "`weak_columns` contain “weak” signals; phrases or passages that you want rough or semantic matches with, such as product descriptions, chunks of an essay, or code snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36b2f067e847f4e87999080101509b3b69f8cde0']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "\n",
    "# TODO: Your ThirdAI key goes here\n",
    "licensing.activate(\"YOUR-THIRDAI-KEY\")\n",
    "\n",
    "doc = ndb.CSV(\n",
    "    \"langchain.csv\",\n",
    "    # Path to file and stack trace are strong signals.\n",
    "    strong_columns=[\"path_to_file\", \"trace\"],\n",
    "    # Code snippets contain weak signals\n",
    "    weak_columns=[\"snippet\"],\n",
    "    reference_columns=[\"snippet\"],\n",
    ")\n",
    "\n",
    "db = ndb.NeuralDB()\n",
    "\n",
    "db.insert([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's test it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/langchain_community/retrievers/arxiv.py \n",
      "\n",
      "trace: class name: ArxivRetriever.  \n",
      "\n",
      "snippet: class ArxivRetriever(BaseRetriever, ArxivAPIWrapper):\n",
      "    \"\"\"`Arxiv` retriever.\n",
      "\n",
      "    It wraps load() to get_relevant_documents().\n",
      "    It uses all ArxivAPIWrapper arguments without any change.\n",
      "    \"\"\"\n",
      "\n",
      "    get_full_documents: bool = False\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/templates/retrieval-agent/retrieval_agent/chain.py \n",
      "\n",
      "trace: class name: ArxivRetriever. function name: _get_relevant_documents \n",
      "\n",
      "snippet:     def _get_relevant_documents(\n",
      "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        try:\n",
      "            if self.is_arxiv_identifier(query):\n",
      "                results = self.arxiv_search(\n",
      "                    id_list=query.split(),\n",
      "                    max_results=self.top_k_results,\n",
      "                ).results()\n",
      "            else:\n",
      "                results = self.arxiv_search(  # type: ignore\n",
      "                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.top_k_results\n",
      "                ).results()\n",
      "        except self.arxiv_exceptions as ex:\n",
      "            return [Document(page_content=f\"Arxiv exception: {ex}\")]\n",
      "        docs = [\n",
      "            Document(\n",
      "                page_content=result.summary,\n",
      "                metadata={\n",
      "                    \"Published\": result.updated.date(),\n",
      "                    \"Title\": result.title,\n",
      "                    \"Authors\": \", \".join(a.name for a in result.authors),\n",
      "                },\n",
      "            )\n",
      "            for result in results\n",
      "        ]\n",
      "        return docs\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"get relevant documents in arxiv retriever\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/experimental/langchain_experimental/retrievers/__init__.py \n",
      "\n",
      "trace:  \n",
      "\n",
      "snippet: \"\"\"**Retriever** class returns Documents given a text **query**.\n",
      "\n",
      "It is more general than a vector store. A retriever does not need to be able to\n",
      "store documents, only to return (or retrieve) it.\n",
      "\"\"\"\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/langchain_community/vectorstores/neo4j_vector.py \n",
      "\n",
      "trace: class name: Neo4jVector. function name: from_documents \n",
      "\n",
      "snippet:     @classmethod\n",
      "    def from_existing_index(\n",
      "        cls: Type[Neo4jVector],\n",
      "        embedding: Embeddings,\n",
      "        index_name: str,\n",
      "        search_type: SearchType = DEFAULT_SEARCH_TYPE,\n",
      "        keyword_index_name: Optional[str] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Neo4jVector:\n",
      "        \"\"\"\n",
      "        Get instance of an existing Neo4j vector index. This method will\n",
      "        return the instance of the store without inserting any new\n",
      "        embeddings.\n",
      "        Neo4j credentials are required in the form of `url`, `username`,\n",
      "        and `password` and optional `database` parameters along with\n",
      "        the `index_name` definition.\n",
      "        \"\"\"\n",
      "\n",
      "        if search_type == SearchType.HYBRID and not keyword_index_name:\n",
      "            raise ValueError(\n",
      "                \"keyword_index name has to be specified \"\n",
      "                \"when using hybrid search option\"\n",
      "            )\n",
      "\n",
      "        store = cls(\n",
      "            embedding=embedding,\n",
      "            index_name=index_name,\n",
      "            keyword_index_name=keyword_index_name,\n",
      "            search_type=search_type,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        embedding_dimension, index_type = store.retrieve_existing_index()\n",
      "\n",
      "        # Raise error if relationship index type\n",
      "        if index_type == \"RELATIONSHIP\":\n",
      "            raise ValueError(\n",
      "                \"Relationship vector index is not supported with \"\n",
      "                \"`from_existing_index` method. Please use the \"\n",
      "                \"`from_existing_relationship_index` method.\"\n",
      "            )\n",
      "\n",
      "        if not embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The specified vector index name does not exist. \"\n",
      "                \"Make sure to check if you spelled it correctly\"\n",
      "            )\n",
      "\n",
      "        # Check if embedding function and vector index dimensions match\n",
      "        if not store.embedding_dimension == embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The provided embedding function and vector index \"\n",
      "                \"dimensions do not match.\\n\"\n",
      "                f\"Embedding function dimension: {store.embedding_dimension}\\n\"\n",
      "                f\"Vector index dimension: {embedding_dimension}\"\n",
      "            )\n",
      "\n",
      "        if search_type == SearchType.HYBRID:\n",
      "            fts_node_label = store.retrieve_existing_fts_index()\n",
      "            # If the FTS index doesn't exist yet\n",
      "            if not fts_node_label:\n",
      "                raise ValueError(\n",
      "                    \"The specified keyword index name does not exist. \"\n",
      "                    \"Make sure to check if you spelled it correctly\"\n",
      "                )\n",
      "            else:  # Validate that FTS and Vector index use the same information\n",
      "                if not fts_node_label == store.node_label:\n",
      "                    raise ValueError(\n",
      "                        \"Vector and keyword index don't index the same node label\"\n",
      "                    )\n",
      "\n",
      "        return store\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"base class for retriever that does not use vector store\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts_with_bm25_params \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts_with_bm25_params() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(\n",
      "        texts=input_texts, bm25_params={\"epsilon\": 10}\n",
      "    )\n",
      "    # should count only multiple words (have, pan)\n",
      "    assert bm25_retriever.vectorizer.epsilon == 10\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\n",
      "    assert len(bm25_retriever.docs) == 3\n",
      "    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"bm25 retriever test\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain.ndb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.save(\"langchain.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A full copilot system with _Chain of Thought_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a copilot system that is powerful enough to answer a question like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I want to integrate a retriever called MyRetriever with this open source codebase. \"\n",
    "    \"It does not use a vector store. \"\n",
    "    \"Create a skeleton for a class that wraps MyRetriever and inherits the right interface. \"\n",
    "    \"(Don't implement the methods, just write #TODOs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model query script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Your OpenAI key goes here\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR-OPENAI-KEY\"\n",
    "openai_client = OpenAI() # defaults to os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def query_gpt(query=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and retreive the relevant code snippet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None, print_metadata=False):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        if (print_metadata):\n",
    "            print(f\"Reference {i + 1} \\n{result.text}\")\n",
    "        if radius:\n",
    "            references.append(f\"```{result.context(radius=radius)}```\")\n",
    "        else:\n",
    "            references.append(f\"```{result.text}```\")\n",
    "    return references\n",
    "\n",
    "def get_context(query, radius=None, print_metadata=False):\n",
    "    references = get_references(str(query), radius=radius, print_metadata=print_metadata)\n",
    "    context = \"\\n\\n\".join(references[:5])\n",
    "    return context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial thoughts\n",
    "Action items required to accomplish the above given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes that wrap external retrievers in the codebase.\n",
      "3. Examples of how other retrievers are integrated into the codebase.\n",
      "4. Any specific requirements or conventions for integrating external retrievers.\n",
      "5. Any dependencies or configurations needed for integrating external retrievers.\n"
     ]
    }
   ],
   "source": [
    "def initial_thoughts(task):\n",
    "    prompt = (\n",
    "        \"Act as a software engineer who is the expert in an unnamed open source codebase. \"\n",
    "        f\"You are asked to do the following:\\n\\n{task}\\n\\n\"\n",
    "        \"You have access to an oracle that can give you snippets and examples from \"\n",
    "        \"this open source codebase, and only from this open source codebase. \"\n",
    "        \"What pieces of information would you want to get from the oracle to complete the task? \"\n",
    "        \"List them in separate lines.\"\n",
    "    )\n",
    "    # Only return non-empty lines.\n",
    "    return [query for query in query_gpt(prompt).split(\"\\n\") if query]\n",
    "\n",
    "for query in initial_thoughts(task):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_thoughts(task, context, previous_answer=\"\"):\n",
    "    prompt = task\n",
    "    prompt += (\n",
    "        f\"Act as an experienced software engineer:\\n\\n\"\n",
    "        f\"Answer the query ```{task}``` , given your previous answers : ```{previous_answer}```\\n\\n\"\n",
    "        f\"modify your answer based on this new information (do not construct \"\n",
    "        f\"your answer from outside the context provided ): ```{context}```\"\n",
    "    )\n",
    "    response = query_gpt(prompt)\n",
    "    return response\n",
    "\n",
    "def copilot(task, radius=None, verbose=False):\n",
    "    queries = initial_thoughts(task)\n",
    "    if verbose:\n",
    "        print(len(queries), \"queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    draft_answer = \"\"\n",
    "\n",
    "    for query in queries:\n",
    "        if verbose:\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Retrieved references:\")\n",
    "        retrieved_info = get_context(query, radius=radius, print_metadata=verbose) # retrieve neural db response for current thought\n",
    "        # LLM modifies answer based on the previous answer and current ndb results\n",
    "        draft_answer = refine_thoughts(\n",
    "            task,\n",
    "            context=f\"Answers to the query '{query}':\\n\\n{retrieved_info}\",\n",
    "            previous_answer=draft_answer,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"Draft Answer:\")\n",
    "            print(draft_answer)\n",
    "            print(\"=\" * 100)\n",
    "    return draft_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get this task done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 queries:\n",
      "1. The interface that the open source codebase uses for retrievers\n",
      "2. Any existing classes or modules that handle retrievers in the codebase\n",
      "3. Any specific requirements or constraints for integrating external retrievers\n",
      "4. Examples of how other retrievers are integrated in the codebase\n",
      "5. Any relevant documentation or guidelines for extending the codebase with new retrievers\n",
      "\n",
      "\n",
      "Query: 1. The interface that the open source codebase uses for retrievers\n",
      "Retrieved references:\n",
      "Reference 1 \n",
      "snippet: import glob\n",
      "import os\n",
      "import re\n",
      "import shutil\n",
      "import sys\n",
      "from pathlib import Path\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    intermediate_dir = Path(sys.argv[1])\n",
      "\n",
      "    templates_source_dir = Path(os.path.abspath(__file__)).parents[2] / \"templates\"\n",
      "    templates_intermediate_dir = intermediate_dir / \"templates\"\n",
      "\n",
      "    readmes = list(glob.glob(str(templates_source_dir) + \"/*/README.md\"))\n",
      "    destinations = [\n",
      "        readme[len(str(templates_source_dir)) + 1 : -10] + \".md\" for readme in readmes\n",
      "    ]\n",
      "    for source, destination in zip(readmes, destinations):\n",
      "        full_destination = templates_intermediate_dir / destination\n",
      "        shutil.copyfile(source, full_destination)\n",
      "        with open(full_destination, \"r\") as f:\n",
      "            content = f.read()\n",
      "        # remove images\n",
      "        content = re.sub(r\"\\!\\[.*?\\]\\((.*?)\\)\", \"\", content)\n",
      "        with open(full_destination, \"w\") as f:\n",
      "            f.write(content)\n",
      "\n",
      "    sidebar_hidden = \"\"\"---\n",
      "sidebar_class_name: hidden\n",
      "---\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "    # handle index file\n",
      "    templates_index_source = templates_source_dir / \"docs\" / \"INDEX.md\"\n",
      "    templates_index_intermediate = templates_intermediate_dir / \"index.md\"\n",
      "\n",
      "    with open(templates_index_source, \"r\") as f:\n",
      "        content = f.read()\n",
      "\n",
      "    # replace relative links\n",
      "    content = re.sub(r\"\\]\\(\\.\\.\\/\", \"](/docs/templates/\", content)\n",
      "\n",
      "    with open(templates_index_intermediate, \"w\") as f:\n",
      "        f.write(sidebar_hidden + content)\n",
      "\n",
      "Reference 2 \n",
      "snippet: class _SparkLLMClient:\n",
      "    \"\"\"\n",
      "    Use websocket-client to call the SparkLLM interface provided by Xfyun,\n",
      "    which is the iFlyTek's open platform for AI capabilities\n",
      "    \"\"\"\n",
      "\n",
      "Reference 3 \n",
      "snippet: class _SparkLLMClient:\n",
      "    \"\"\"\n",
      "    Use websocket-client to call the SparkLLM interface provided by Xfyun,\n",
      "    which is the iFlyTek's open platform for AI capabilities\n",
      "    \"\"\"\n",
      "\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the right interface\n",
      "class MyRetrieverWrapper:\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever here\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to interact with MyRetriever\n",
      "\n",
      "    def retrieve(self, document_id):\n",
      "        # TODO: Implement retrieve method to interact with MyRetriever\n",
      "\n",
      "    def update_index(self, documents):\n",
      "        # TODO: Implement update_index method to interact with MyRetriever\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 2. Any existing classes or modules that handle retrievers in the codebase\n",
      "Retrieved references:\n",
      "Reference 1 \n",
      "snippet: def create_importer(\n",
      "    package: str,\n",
      "    *,\n",
      "    module_lookup: Optional[Dict[str, str]] = None,\n",
      "    deprecated_lookups: Optional[Dict[str, str]] = None,\n",
      "    fallback_module: Optional[str] = None,\n",
      ") -> Callable[[str], Any]:\n",
      "    \"\"\"Create a function that helps retrieve objects from their new locations.\n",
      "\n",
      "    The goal of this function is to help users transition from deprecated\n",
      "    imports to new imports.\n",
      "\n",
      "    The function will raise deprecation warning on loops using\n",
      "    deprecated_lookups or fallback_module.\n",
      "\n",
      "    Module lookups will import without deprecation warnings (used to speed\n",
      "    up imports from large namespaces like llms or chat models).\n",
      "\n",
      "    This function should ideally only be used with deprecated imports not with\n",
      "    existing imports that are valid, as in addition to raising deprecation warnings\n",
      "    the dynamic imports can create other issues for developers (e.g.,\n",
      "    loss of type information, IDE support for going to definition etc).\n",
      "\n",
      "    Args:\n",
      "        package: current package. Use __package__\n",
      "        module_lookup: maps name of object to the module where it is defined.\n",
      "            e.g.,\n",
      "            {\n",
      "                \"MyDocumentLoader\": (\n",
      "                    \"langchain_community.document_loaders.my_document_loader\"\n",
      "                )\n",
      "            }\n",
      "        deprecated_lookups: same as module look up, but will raise\n",
      "            deprecation warnings.\n",
      "        fallback_module: module to import from if the object is not found in\n",
      "            module_lookup or if module_lookup is not provided.\n",
      "\n",
      "    Returns:\n",
      "        A function that imports objects from the specified modules.\n",
      "    \"\"\"\n",
      "    all_module_lookup = {**(deprecated_lookups or {}), **(module_lookup or {})}\n",
      "\n",
      "    def import_by_name(name: str) -> Any:\n",
      "        \"\"\"Import stores from langchain_community.\"\"\"\n",
      "        # If not in interactive env, raise warning.\n",
      "        if all_module_lookup and name in all_module_lookup:\n",
      "            new_module = all_module_lookup[name]\n",
      "            if new_module.split(\".\")[0] not in ALLOWED_TOP_LEVEL_PKGS:\n",
      "                raise AssertionError(\n",
      "                    f\"Importing from {new_module} is not allowed. \"\n",
      "                    f\"Allowed top-level packages are: {ALLOWED_TOP_LEVEL_PKGS}\"\n",
      "                )\n",
      "\n",
      "            try:\n",
      "                module = importlib.import_module(new_module)\n",
      "            except ModuleNotFoundError as e:\n",
      "                if new_module.startswith(\"langchain_community\"):\n",
      "                    raise ModuleNotFoundError(\n",
      "                        f\"Module {new_module} not found. \"\n",
      "                        \"Please install langchain-community to access this module. \"\n",
      "                        \"You can install it using `pip install -U langchain-community`\"\n",
      "                    ) from e\n",
      "                raise\n",
      "\n",
      "            try:\n",
      "                result = getattr(module, name)\n",
      "                if (\n",
      "                    not is_interactive_env()\n",
      "                    and deprecated_lookups\n",
      "                    and name in deprecated_lookups\n",
      "                ):\n",
      "                    # Depth 3:\n",
      "                    # internal.py\n",
      "                    # module_import.py\n",
      "                    # Module in langchain that uses this function\n",
      "                    # [calling code] whose frame we want to inspect.\n",
      "                    if not internal.is_caller_internal(depth=3):\n",
      "                        warn_deprecated(\n",
      "                            since=\"0.1\",\n",
      "                            pending=False,\n",
      "                            removal=\"0.4\",\n",
      "                            message=(\n",
      "                                f\"Importing {name} from {package} is deprecated. \"\n",
      "                                f\"Please replace deprecated imports:\\n\\n\"\n",
      "                                f\">> from {package} import {name}\\n\\n\"\n",
      "                                \"with new imports of:\\n\\n\"\n",
      "                                f\">> from {new_module} import {name}\\n\"\n",
      "                                \"You can use the langchain cli to **automatically** \"\n",
      "                                \"upgrade many imports. Please see documentation here \"\n",
      "                                \"https://python.langchain.com/v0.2/docs/versions/v0_2/ \"\n",
      "                            ),\n",
      "                        )\n",
      "                return result\n",
      "            except Exception as e:\n",
      "                raise AttributeError(\n",
      "                    f\"module {new_module} has no attribute {name}\"\n",
      "                ) from e\n",
      "\n",
      "        if fallback_module:\n",
      "            try:\n",
      "                module = importlib.import_module(fallback_module)\n",
      "                result = getattr(module, name)\n",
      "                if not is_interactive_env():\n",
      "                    # Depth 3:\n",
      "                    # internal.py\n",
      "                    # module_import.py\n",
      "                    # Module in langchain that uses this function\n",
      "                    # [calling code] whose frame we want to inspect.\n",
      "                    if not internal.is_caller_internal(depth=3):\n",
      "                        warn_deprecated(\n",
      "                            since=\"0.1\",\n",
      "                            pending=False,\n",
      "                            removal=\"0.4\",\n",
      "                            message=(\n",
      "                                f\"Importing {name} from {package} is deprecated. \"\n",
      "                                f\"Please replace deprecated imports:\\n\\n\"\n",
      "                                f\">> from {package} import {name}\\n\\n\"\n",
      "                                \"with new imports of:\\n\\n\"\n",
      "                                f\">> from {fallback_module} import {name}\\n\"\n",
      "                                \"You can use the langchain cli to **automatically** \"\n",
      "                                \"upgrade many imports. Please see documentation here \"\n",
      "                                \"https://python.langchain.com/v0.2/docs/versions/v0_2/ \"\n",
      "                            ),\n",
      "                        )\n",
      "                return result\n",
      "\n",
      "            except Exception as e:\n",
      "                raise AttributeError(\n",
      "                    f\"module {fallback_module} has no attribute {name}\"\n",
      "                ) from e\n",
      "\n",
      "        raise AttributeError(f\"module {package} has no attribute {name}\")\n",
      "\n",
      "    return import_by_name\n",
      "\n",
      "Reference 2 \n",
      "snippet: def import_all_modules(package_name: str) -> dict:\n",
      "    package = importlib.import_module(package_name)\n",
      "    classes: dict = {}\n",
      "\n",
      "    def _handle_module(module: ModuleType) -> None:\n",
      "        # Iterate over all members of the module\n",
      "\n",
      "        names = dir(module)\n",
      "\n",
      "        if hasattr(module, \"__all__\"):\n",
      "            names += list(module.__all__)\n",
      "\n",
      "        names = sorted(set(names))\n",
      "\n",
      "        for name in names:\n",
      "            # Check if it's a class or function\n",
      "            attr = getattr(module, name)\n",
      "\n",
      "            if not inspect.isclass(attr):\n",
      "                continue\n",
      "\n",
      "            if not hasattr(attr, \"is_lc_serializable\") or not isinstance(attr, type):\n",
      "                continue\n",
      "\n",
      "            if (\n",
      "                isinstance(attr.is_lc_serializable(), bool)  # type: ignore\n",
      "                and attr.is_lc_serializable()  # type: ignore\n",
      "            ):\n",
      "                key = tuple(attr.lc_id())  # type: ignore\n",
      "                value = tuple(attr.__module__.split(\".\") + [attr.__name__])\n",
      "                if key in classes and classes[key] != value:\n",
      "                    raise ValueError\n",
      "                classes[key] = value\n",
      "\n",
      "    _handle_module(package)\n",
      "\n",
      "    for importer, modname, ispkg in pkgutil.walk_packages(\n",
      "        package.__path__, package.__name__ + \".\"\n",
      "    ):\n",
      "        try:\n",
      "            module = importlib.import_module(modname)\n",
      "        except ModuleNotFoundError:\n",
      "            continue\n",
      "        _handle_module(module)\n",
      "\n",
      "    return classes\n",
      "\n",
      "Reference 3 \n",
      "snippet: def generate_top_level_imports(pkg: str) -> List[Tuple[str, str]]:\n",
      "    \"\"\"This code will look at all the top level modules in langchain_community.\n",
      "\n",
      "    It'll attempt to import everything from each __init__ file\n",
      "\n",
      "    for example,\n",
      "\n",
      "    langchain_community/\n",
      "        chat_models/\n",
      "            __init__.py # <-- import everything from here\n",
      "        llm/\n",
      "            __init__.py # <-- import everything from here\n",
      "\n",
      "\n",
      "    It'll collect all the imports, import the classes / functions it can find\n",
      "    there. It'll return a list of 2-tuples\n",
      "\n",
      "    Each tuple will contain the fully qualified path of the class / function to where\n",
      "    its logic is defined\n",
      "    (e.g., langchain_community.chat_models.xyz_implementation.ver2.XYZ)\n",
      "    and the second tuple will contain the path\n",
      "    to importing it from the top level namespaces\n",
      "    (e.g., langchain_community.chat_models.XYZ)\n",
      "    \"\"\"\n",
      "    package = importlib.import_module(pkg)\n",
      "\n",
      "    items = []\n",
      "\n",
      "    # Function to handle importing from modules\n",
      "    def handle_module(module, module_name):\n",
      "        if hasattr(module, \"__all__\"):\n",
      "            all_objects = getattr(module, \"__all__\")\n",
      "            for name in all_objects:\n",
      "                # Attempt to fetch each object declared in __all__\n",
      "                obj = getattr(module, name, None)\n",
      "                if obj and (inspect.isclass(obj) or inspect.isfunction(obj)):\n",
      "                    # Capture the fully qualified name of the object\n",
      "                    original_module = obj.__module__\n",
      "                    original_name = obj.__name__\n",
      "                    # Form the new import path from the top-level namespace\n",
      "                    top_level_import = f\"{module_name}.{name}\"\n",
      "                    # Append the tuple with original and top-level paths\n",
      "                    items.append(\n",
      "                        (f\"{original_module}.{original_name}\", top_level_import)\n",
      "                    )\n",
      "\n",
      "    # Handle the package itself (root level)\n",
      "    handle_module(package, pkg)\n",
      "\n",
      "    # Only iterate through top-level modules/packages\n",
      "    for finder, modname, ispkg in pkgutil.iter_modules(\n",
      "        package.__path__, package.__name__ + \".\"\n",
      "    ):\n",
      "        if ispkg:\n",
      "            try:\n",
      "                module = importlib.import_module(modname)\n",
      "                handle_module(module, modname)\n",
      "            except ModuleNotFoundError:\n",
      "                continue\n",
      "\n",
      "    return items\n",
      "\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the right interface\n",
      "class MyRetrieverWrapper:\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever here\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to interact with MyRetriever\n",
      "\n",
      "    def retrieve(self, document_id):\n",
      "        # TODO: Implement retrieve method to interact with MyRetriever\n",
      "\n",
      "    def update_index(self, documents):\n",
      "        # TODO: Implement update_index method to interact with MyRetriever\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 3. Any specific requirements or constraints for integrating external retrievers\n",
      "Retrieved references:\n",
      "Reference 1 \n",
      "snippet: \"\"\"Test Xinference embeddings.\"\"\"\n",
      "\n",
      "import time\n",
      "from typing import AsyncGenerator, Tuple\n",
      "\n",
      "import pytest_asyncio\n",
      "\n",
      "from langchain_community.embeddings import XinferenceEmbeddings\n",
      "\n",
      "\n",
      "@pytest_asyncio.fixture\n",
      "async def setup() -> AsyncGenerator[Tuple[str, str], None]:\n",
      "    import xoscar as xo\n",
      "    from xinference.deploy.supervisor import start_supervisor_components\n",
      "    from xinference.deploy.utils import create_worker_actor_pool\n",
      "    from xinference.deploy.worker import start_worker_components\n",
      "\n",
      "    pool = await create_worker_actor_pool(\n",
      "        f\"test://127.0.0.1:{xo.utils.get_next_port()}\"\n",
      "    )\n",
      "    print(f\"Pool running on localhost:{pool.external_address}\")  # noqa: T201\n",
      "\n",
      "    endpoint = await start_supervisor_components(\n",
      "        pool.external_address, \"127.0.0.1\", xo.utils.get_next_port()\n",
      "    )\n",
      "    await start_worker_components(\n",
      "        address=pool.external_address, supervisor_address=pool.external_address\n",
      "    )\n",
      "\n",
      "    # wait for the api.\n",
      "    time.sleep(3)\n",
      "    async with pool:\n",
      "        yield endpoint, pool.external_address\n",
      "\n",
      "Reference 2 \n",
      "snippet: \"\"\"Test Xinference wrapper.\"\"\"\n",
      "\n",
      "import time\n",
      "from typing import AsyncGenerator, Tuple\n",
      "\n",
      "import pytest_asyncio\n",
      "\n",
      "from langchain_community.llms import Xinference\n",
      "\n",
      "\n",
      "@pytest_asyncio.fixture\n",
      "async def setup() -> AsyncGenerator[Tuple[str, str], None]:\n",
      "    import xoscar as xo\n",
      "    from xinference.deploy.supervisor import start_supervisor_components\n",
      "    from xinference.deploy.utils import create_worker_actor_pool\n",
      "    from xinference.deploy.worker import start_worker_components\n",
      "\n",
      "    pool = await create_worker_actor_pool(\n",
      "        f\"test://127.0.0.1:{xo.utils.get_next_port()}\"\n",
      "    )\n",
      "    print(f\"Pool running on localhost:{pool.external_address}\")  # noqa: T201\n",
      "\n",
      "    endpoint = await start_supervisor_components(\n",
      "        pool.external_address, \"127.0.0.1\", xo.utils.get_next_port()\n",
      "    )\n",
      "    await start_worker_components(\n",
      "        address=pool.external_address, supervisor_address=pool.external_address\n",
      "    )\n",
      "\n",
      "    # wait for the api.\n",
      "    time.sleep(3)\n",
      "    async with pool:\n",
      "        yield endpoint, pool.external_address\n",
      "\n",
      "Reference 3 \n",
      "snippet: @pytest.mark.requires(\"kay\")\n",
      "def test_kay_retriever() -> None:\n",
      "    retriever = KayAiRetriever.create(\n",
      "        dataset_id=\"company\",\n",
      "        data_types=[\"10-K\", \"10-Q\", \"8-K\", \"PressRelease\"],\n",
      "        num_contexts=3,\n",
      "    )\n",
      "    docs = retriever.invoke(\n",
      "        \"What were the biggest strategy changes and partnerships made by Roku \"\n",
      "        \"in 2023?\",\n",
      "    )\n",
      "    assert len(docs) == 3\n",
      "    for doc in docs:\n",
      "        assert isinstance(doc, Document)\n",
      "        assert doc.page_content\n",
      "        assert doc.metadata\n",
      "        assert len(list(doc.metadata.items())) > 0\n",
      "\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the right interface\n",
      "class MyRetrieverWrapper:\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever here\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to interact with MyRetriever\n",
      "\n",
      "    def retrieve(self, document_id):\n",
      "        # TODO: Implement retrieve method to interact with MyRetriever\n",
      "\n",
      "    def update_index(self, documents):\n",
      "        # TODO: Implement update_index method to interact with MyRetriever\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 4. Examples of how other retrievers are integrated in the codebase\n",
      "Retrieved references:\n",
      "Reference 1 \n",
      "snippet: async def test_chat_from_role_strings() -> None:\n",
      "    \"\"\"Test instantiation of chat template from role strings.\"\"\"\n",
      "    with pytest.warns(LangChainPendingDeprecationWarning):\n",
      "        template = ChatPromptTemplate.from_role_strings(\n",
      "            [\n",
      "                (\"system\", \"You are a bot.\"),\n",
      "                (\"assistant\", \"hello!\"),\n",
      "                (\"human\", \"{question}\"),\n",
      "                (\"other\", \"{quack}\"),\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    expected = [\n",
      "        ChatMessage(content=\"You are a bot.\", role=\"system\"),\n",
      "        ChatMessage(content=\"hello!\", role=\"assistant\"),\n",
      "        ChatMessage(content=\"How are you?\", role=\"human\"),\n",
      "        ChatMessage(content=\"duck\", role=\"other\"),\n",
      "    ]\n",
      "\n",
      "    messages = template.format_messages(question=\"How are you?\", quack=\"duck\")\n",
      "    assert messages == expected\n",
      "\n",
      "    messages = await template.aformat_messages(question=\"How are you?\", quack=\"duck\")\n",
      "    assert messages == expected\n",
      "\n",
      "Reference 2 \n",
      "snippet: def test_google_documentai_warehoure_retriever() -> None:\n",
      "    \"\"\"In order to run this test, you should provide a project_id and user_ldap.\n",
      "\n",
      "    Example:\n",
      "    export USER_LDAP=...\n",
      "    export PROJECT_NUMBER=...\n",
      "    \"\"\"\n",
      "    project_number = os.environ[\"PROJECT_NUMBER\"]\n",
      "    user_ldap = os.environ[\"USER_LDAP\"]\n",
      "    docai_wh_retriever = GoogleDocumentAIWarehouseRetriever(\n",
      "        project_number=project_number\n",
      "    )\n",
      "    documents = docai_wh_retriever.invoke(\n",
      "        \"What are Alphabet's Other Bets?\", user_ldap=user_ldap\n",
      "    )\n",
      "    assert len(documents) > 0\n",
      "    for doc in documents:\n",
      "        assert isinstance(doc, Document)\n",
      "\n",
      "Reference 3 \n",
      "snippet: class FewShotChatMessagePromptTemplate(\n",
      "    BaseChatPromptTemplate, _FewShotPromptTemplateMixin\n",
      "):\n",
      "    \"\"\"Chat prompt template that supports few-shot examples.\n",
      "\n",
      "    The high level structure of produced by this prompt template is a list of messages\n",
      "    consisting of prefix message(s), example message(s), and suffix message(s).\n",
      "\n",
      "    This structure enables creating a conversation with intermediate examples like:\n",
      "\n",
      "        System: You are a helpful AI Assistant\n",
      "        Human: What is 2+2?\n",
      "        AI: 4\n",
      "        Human: What is 2+3?\n",
      "        AI: 5\n",
      "        Human: What is 4+4?\n",
      "\n",
      "    This prompt template can be used to generate a fixed list of examples or else\n",
      "    to dynamically select examples based on the input.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        Prompt template with a fixed list of examples (matching the sample\n",
      "        conversation above):\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain_core.prompts import (\n",
      "                FewShotChatMessagePromptTemplate,\n",
      "                ChatPromptTemplate\n",
      "            )\n",
      "\n",
      "            examples = [\n",
      "                {\"input\": \"2+2\", \"output\": \"4\"},\n",
      "                {\"input\": \"2+3\", \"output\": \"5\"},\n",
      "            ]\n",
      "\n",
      "            example_prompt = ChatPromptTemplate.from_messages(\n",
      "                [('human', '{input}'), ('ai', '{output}')]\n",
      "            )\n",
      "\n",
      "            few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
      "                examples=examples,\n",
      "                # This is a prompt template used to format each individual example.\n",
      "                example_prompt=example_prompt,\n",
      "            )\n",
      "\n",
      "            final_prompt = ChatPromptTemplate.from_messages(\n",
      "                [\n",
      "                    ('system', 'You are a helpful AI Assistant'),\n",
      "                    few_shot_prompt,\n",
      "                    ('human', '{input}'),\n",
      "                ]\n",
      "            )\n",
      "            final_prompt.format(input=\"What is 4+4?\")\n",
      "\n",
      "        Prompt template with dynamically selected examples:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain_core.prompts import SemanticSimilarityExampleSelector\n",
      "            from langchain_core.embeddings import OpenAIEmbeddings\n",
      "            from langchain_core.vectorstores import Chroma\n",
      "\n",
      "            examples = [\n",
      "                {\"input\": \"2+2\", \"output\": \"4\"},\n",
      "                {\"input\": \"2+3\", \"output\": \"5\"},\n",
      "                {\"input\": \"2+4\", \"output\": \"6\"},\n",
      "                # ...\n",
      "            ]\n",
      "\n",
      "            to_vectorize = [\n",
      "                \" \".join(example.values())\n",
      "                for example in examples\n",
      "            ]\n",
      "            embeddings = OpenAIEmbeddings()\n",
      "            vectorstore = Chroma.from_texts(\n",
      "                to_vectorize, embeddings, metadatas=examples\n",
      "            )\n",
      "            example_selector = SemanticSimilarityExampleSelector(\n",
      "                vectorstore=vectorstore\n",
      "            )\n",
      "\n",
      "            from langchain_core import SystemMessage\n",
      "            from langchain_core.prompts import HumanMessagePromptTemplate\n",
      "            from langchain_core.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
      "\n",
      "            few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
      "                # Which variable(s) will be passed to the example selector.\n",
      "                input_variables=[\"input\"],\n",
      "                example_selector=example_selector,\n",
      "                # Define how each example will be formatted.\n",
      "                # In this case, each example will become 2 messages:\n",
      "                # 1 human, and 1 AI\n",
      "                example_prompt=(\n",
      "                    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
      "                    + AIMessagePromptTemplate.from_template(\"{output}\")\n",
      "                ),\n",
      "            )\n",
      "            # Define the overall prompt.\n",
      "            final_prompt = (\n",
      "                SystemMessagePromptTemplate.from_template(\n",
      "                    \"You are a helpful AI Assistant\"\n",
      "                )\n",
      "                + few_shot_prompt\n",
      "                + HumanMessagePromptTemplate.from_template(\"{input}\")\n",
      "            )\n",
      "            # Show the prompt\n",
      "            print(final_prompt.format_messages(input=\"What's 3+3?\"))  # noqa: T201\n",
      "\n",
      "            # Use within an LLM\n",
      "            from langchain_core.chat_models import ChatAnthropic\n",
      "            chain = final_prompt | ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
      "            chain.invoke({\"input\": \"What's 3+3?\"})\n",
      "    \"\"\"\n",
      "\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the right interface\n",
      "class MyRetrieverWrapper:\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever here\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to interact with MyRetriever\n",
      "\n",
      "    def retrieve(self, document_id):\n",
      "        # TODO: Implement retrieve method to interact with MyRetriever\n",
      "\n",
      "    def update_index(self, documents):\n",
      "        # TODO: Implement update_index method to interact with MyRetriever\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 5. Any relevant documentation or guidelines for extending the codebase with new retrievers\n",
      "Retrieved references:\n",
      "Reference 1 \n",
      "snippet:     def as_retriever(self, **kwargs: Any) -> AzureSearchVectorStoreRetriever:  # type: ignore\n",
      "        \"\"\"Return AzureSearchVectorStoreRetriever initialized from this VectorStore.\n",
      "\n",
      "        Args:\n",
      "            search_type (Optional[str]): Defines the type of search that\n",
      "                the Retriever should perform.\n",
      "                Can be \"similarity\" (default), \"hybrid\", or\n",
      "                    \"semantic_hybrid\".\n",
      "            search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      "                search function. Can include things like:\n",
      "                    score_threshold: Minimum relevance threshold\n",
      "                        for similarity_score_threshold\n",
      "                    fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      "                    lambda_mult: Diversity of results returned by MMR;\n",
      "                        1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      "                    filter: Filter by document metadata\n",
      "\n",
      "        Returns:\n",
      "            AzureSearchVectorStoreRetriever: Retriever class for VectorStore.\n",
      "        \"\"\"\n",
      "        tags = kwargs.pop(\"tags\", None) or []\n",
      "        tags.extend(self._get_retriever_tags())\n",
      "        return AzureSearchVectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)\n",
      "\n",
      "Reference 2 \n",
      "snippet:     def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:\n",
      "        \"\"\"Return VectorStoreRetriever initialized from this VectorStore.\n",
      "\n",
      "        Args:\n",
      "            search_type (Optional[str]): Defines the type of search that\n",
      "                the Retriever should perform.\n",
      "                Can be \"similarity\" (default), \"mmr\", or\n",
      "                \"similarity_score_threshold\".\n",
      "            search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      "                search function. Can include things like:\n",
      "                    k: Amount of documents to return (Default: 4)\n",
      "                    score_threshold: Minimum relevance threshold\n",
      "                        for similarity_score_threshold\n",
      "                    fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      "                    lambda_mult: Diversity of results returned by MMR;\n",
      "                        1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      "                    filter: Filter by document metadata\n",
      "\n",
      "        Returns:\n",
      "            VectorStoreRetriever: Retriever class for VectorStore.\n",
      "\n",
      "        Examples:\n",
      "\n",
      "        .. code-block:: python\n",
      "\n",
      "            # Retrieve more documents with higher diversity\n",
      "            # Useful if your dataset has many similar documents\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"mmr\",\n",
      "                search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      "            )\n",
      "\n",
      "            # Fetch more documents for the MMR algorithm to consider\n",
      "            # But only return the top 5\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"mmr\",\n",
      "                search_kwargs={'k': 5, 'fetch_k': 50}\n",
      "            )\n",
      "\n",
      "            # Only retrieve documents that have a relevance score\n",
      "            # Above a certain threshold\n",
      "            docsearch.as_retriever(\n",
      "                search_type=\"similarity_score_threshold\",\n",
      "                search_kwargs={'score_threshold': 0.8}\n",
      "            )\n",
      "\n",
      "            # Only get the single most similar document from the dataset\n",
      "            docsearch.as_retriever(search_kwargs={'k': 1})\n",
      "\n",
      "            # Use a filter to only retrieve documents from a specific paper\n",
      "            docsearch.as_retriever(\n",
      "                search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      "            )\n",
      "        \"\"\"\n",
      "        tags = kwargs.pop(\"tags\", None) or []\n",
      "        tags.extend(self._get_retriever_tags())\n",
      "        return VectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)\n",
      "\n",
      "Reference 3 \n",
      "snippet:     def add_texts(\n",
      "        self,\n",
      "        texts: Iterable[str],\n",
      "        metadatas: Optional[List[dict]] = None,\n",
      "        ids: Optional[List[str]] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> List[str]:\n",
      "        _texts = list(texts)\n",
      "        _ids = ids or [str(uuid4()) for _ in _texts]\n",
      "        self._texts.extend(_texts)\n",
      "        self._embeddings.extend(self._embedding_function.embed_documents(_texts))\n",
      "        self._metadatas.extend(metadatas or ([{}] * len(_texts)))\n",
      "        self._ids.extend(_ids)\n",
      "        self._update_neighbors()\n",
      "        return _ids\n",
      "\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the right interface\n",
      "class MyRetrieverWrapper(VectorStore):\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever here\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to interact with MyRetriever\n",
      "\n",
      "    def retrieve(self, document_id):\n",
      "        # TODO: Implement retrieve method to interact with MyRetriever\n",
      "\n",
      "    def update_index(self, documents):\n",
      "        # TODO: Implement update_index method to interact with MyRetriever\n",
      "```\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "answer = copilot(task=task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
