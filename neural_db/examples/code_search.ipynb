{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Code Search System**\n",
    "We will build a code search system based on the Langchain codebase. Our system will be able to answer queries such as:\n",
    "* \"get relevant documents in arxiv retriever\"\n",
    "* \"base class for retriever that does not use vector store\"\n",
    "* \"bm25 retriever test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clone the Langchain Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 152182, done.\u001b[K\n",
      "remote: Counting objects: 100% (456/456), done.\u001b[K\n",
      "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
      "remote: Total 152182 (delta 223), reused 311 (delta 140), pack-reused 151726\u001b[K\n",
      "Receiving objects: 100% (152182/152182), 215.03 MiB | 14.90 MiB/s, done.\n",
      "Resolving deltas: 100% (113011/113011), done.\n",
      "Updating files: 100% (7117/7117), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git\n",
    "\n",
    "# And install the thirdai package\n",
    "%pip install thirdai==0.7.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "To ensure that each chunk is semantically coherent, we will split it along class and function boundaries. In addition, for our use case, it's important to know which file, class, and/or function a snippet is taken from. This kind of information is perfect for utilizing NeuralDB's notion of strong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def apply_to_codebase(path_to_codebase, chunking_strategy):\n",
    "    \"\"\"Traverses entire codebase and applies chunking strategy to all files.\n",
    "    Returns a dataframe with 5 columns: id, chunk, path_to_file, lineno, end_lineno\n",
    "    `lineno` is the line number (not line index) that the chunk starts on.\n",
    "    `end_lineno` is the chunk's last line number (again, not line index).\n",
    "    For example, \n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    dfs = []\n",
    "    warn = False\n",
    "    for root, _, files in os.walk(path_to_codebase):\n",
    "        for name in files:\n",
    "            path_to_file = os.path.join(root, name)\n",
    "            if name.endswith(\".py\"):\n",
    "                try:\n",
    "                    script = open(path_to_file).read()\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to open\", path_to_file)\n",
    "                    print(\"Reason:\", e)\n",
    "                    print(\"Skipping...\")\n",
    "                ast_body = ast.parse(script).body\n",
    "                script_lines = script.splitlines(keepends=True)\n",
    "                df = chunking_strategy(ast_body, script_lines)\n",
    "                df[\"id\"] = range(offset, offset + len(df))\n",
    "                df[\"path_to_file\"] = [path_to_file for _ in range(len(df))]\n",
    "                offset += len(df)\n",
    "                dfs.append(df)\n",
    "            else:\n",
    "                warn = True\n",
    "    if warn:\n",
    "        print(f\"Warning: Found non-Python files in the codebase. This script only snippets Python code.\")\n",
    "    df = pd.concat(dfs)\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def split_by_function(ast_body: List[ast.AST], script_lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ast_body: List of elements in the Python script as returned by `ast.parse(script).body`\n",
    "    script_lines: List of lines in the Python script\n",
    "\n",
    "    The script will be split into snippets according to these rules:\n",
    "    - Each function is a chunk\n",
    "    - Each method of a class is a chunk\n",
    "    - Expressions between functions or classes are clubbed together.\n",
    "    - Comments (not docstrings) are clubbed with the next chunk\n",
    "\n",
    "    This chunking method produces a dataframe with four columns:\n",
    "    - snippet: A snippet from the codebase\n",
    "    - trace: The stack trace of the snippet; the class and/or function from which\n",
    "      the snippet is taken\n",
    "    - lineno: The line number where the snippet starts. Note that it is 1-indexed,\n",
    "      which is consistent with the lineno returned by the AST module.\n",
    "    - end_lineno: The line number of the last line of the snippet. Like lineno, \n",
    "      it is 1-indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    start_linenos, end_linenos, traces = _split_by_function(\n",
    "        ast_body=ast_body,\n",
    "        start_lineno=1,\n",
    "        end_lineno=len(script_lines),\n",
    "    )\n",
    "    \n",
    "    # Only keep non-empty lines.\n",
    "\n",
    "    for i in range(len(start_linenos)):\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[start_linenos[i] - 1].strip():\n",
    "            start_linenos[i] += 1\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[end_linenos[i] - 1].strip():\n",
    "            end_linenos[i] -= 1\n",
    "    \n",
    "    snippets = []\n",
    "    final_traces = []\n",
    "    final_linenos = []\n",
    "    final_end_linenos = []\n",
    "\n",
    "    for lineno, end_lineno, snippet_trace in zip(start_linenos, end_linenos, traces):\n",
    "        if lineno <= end_lineno:\n",
    "            snippets.append(\"\".join(script_lines[lineno - 1: end_lineno]))\n",
    "            final_traces.append(snippet_trace)\n",
    "            final_linenos.append(lineno)\n",
    "            final_end_linenos.append(end_lineno)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"snippet\": snippets,\n",
    "        \"trace\": final_traces,\n",
    "        \"lineno\": final_linenos,\n",
    "        \"end_lineno\": final_end_linenos,\n",
    "    })\n",
    "\n",
    "def _split_by_function(ast_body: List[ast.AST], start_lineno: int, end_lineno: int):\n",
    "    \"\"\"This helper function allows us to reuse chunking logic within a scope\n",
    "    such as a class.\n",
    "    \"\"\"\n",
    "    start_linenos = []\n",
    "    end_linenos = []\n",
    "    traces = []\n",
    "    can_connect = False\n",
    "\n",
    "    # start_lineno is always the previous end_lineno + 1\n",
    "    # This is because comments are not captured by the AST parser.\n",
    "    # Thus, to keep comments, we must keep all lines between the previous\n",
    "    # element and the current element.\n",
    "\n",
    "    for elem in ast_body:\n",
    "        if isinstance(elem, ast.FunctionDef):\n",
    "            # A function block is always its own chunk\n",
    "            start_linenos.append(start_lineno)\n",
    "            end_linenos.append(elem.end_lineno)\n",
    "            # Add function name to trace.\n",
    "            traces.append(f\"function name: {elem.name}\")\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        elif isinstance(elem, ast.ClassDef):\n",
    "            # A class is treated as a mini-script;\n",
    "            # functions/methods inside a class are their own snippets.\n",
    "            class_start_linenos, class_end_linenos, class_trace = _split_by_function(\n",
    "                ast_body=elem.body,\n",
    "                start_lineno=start_lineno,\n",
    "                end_lineno=elem.end_lineno,\n",
    "            )\n",
    "            start_linenos.extend(class_start_linenos)\n",
    "            end_linenos.extend(class_end_linenos)\n",
    "            # Prepend class name to the trace of every snippet in the class.\n",
    "            traces.extend([f\"class name: {elem.name}. {trace}\" for trace in class_trace])\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        else:\n",
    "            # Group expressions in the global scope that are neither functions\n",
    "            # nor classes.\n",
    "            if can_connect:\n",
    "                end_linenos[-1] = elem.end_lineno\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "            else:\n",
    "                start_linenos.append(start_lineno)\n",
    "                end_linenos.append(elem.end_lineno)\n",
    "                # Append an empty string so `traces`` is always the same length\n",
    "                # as start_linenos and end_linenos\n",
    "                traces.append(\"\")\n",
    "                can_connect = True\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "    start_linenos.append(start_lineno)\n",
    "    end_linenos.append(end_lineno)\n",
    "\n",
    "    return start_linenos, end_linenos, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open ./langchain/libs/langchain/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Warning: Found non-Python files in the codebase. This script only snippets Python code.\n"
     ]
    }
   ],
   "source": [
    "langchain_snippets = apply_to_codebase(\"./langchain\", split_by_function)\n",
    "langchain_snippets.to_csv(\"langchain.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build NeuralDB**\n",
    "As previously mentioned, NeuralDB has a notion of strong and weak columns.\n",
    "\n",
    "`strong_columns` are columns in your CSV file that contains “strong” signals; words or strings that you want exact matches with, such as keywords, brands, categories, or a stack trace.\n",
    "\n",
    "`weak_columns` contain “weak” signals; phrases or passages that you want rough or semantic matches with, such as product descriptions, chunks of an essay, or code snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benitogeordie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['9dd193841eb3c4c1c80d6d4639cdf36a2e10e561']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "\n",
    "# TODO: Your ThirdAI key goes here\n",
    "licensing.activate(\"YOUR-THIRDAI-KEY\")\n",
    "\n",
    "doc = ndb.CSV(\n",
    "    \"langchain.csv\",\n",
    "    # Path to file and stack trace are strong signals.\n",
    "    strong_columns=[\"path_to_file\", \"trace\"],\n",
    "    # Code snippets contain weak signals\n",
    "    weak_columns=[\"snippet\"],\n",
    "    reference_columns=[\"snippet\"],\n",
    ")\n",
    "\n",
    "db = ndb.NeuralDB()\n",
    "\n",
    "db.insert([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's test it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/langchain_community/retrievers/arxiv.py \n",
      "\n",
      "trace: class name: ArxivRetriever.  \n",
      "\n",
      "snippet: class ArxivRetriever(BaseRetriever, ArxivAPIWrapper):\n",
      "    \"\"\"`Arxiv` retriever.\n",
      "\n",
      "    It wraps load() to get_relevant_documents().\n",
      "    It uses all ArxivAPIWrapper arguments without any change.\n",
      "    \"\"\"\n",
      "\n",
      "    get_full_documents: bool = False\n",
      " \n",
      "\n",
      "===================================================================\n",
      "file: ./langchain/templates/retrieval-agent/retrieval_agent/chain.py \n",
      "\n",
      "trace: class name: ArxivRetriever. function name: _get_relevant_documents \n",
      "\n",
      "snippet:     def _get_relevant_documents(\n",
      "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        try:\n",
      "            if self.is_arxiv_identifier(query):\n",
      "                results = self.arxiv_search(\n",
      "                    id_list=query.split(),\n",
      "                    max_results=self.top_k_results,\n",
      "                ).results()\n",
      "            else:\n",
      "                results = self.arxiv_search(  # type: ignore\n",
      "                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.top_k_results\n",
      "                ).results()\n",
      "        except self.arxiv_exceptions as ex:\n",
      "            return [Document(page_content=f\"Arxiv exception: {ex}\")]\n",
      "        docs = [\n",
      "            Document(\n",
      "                page_content=result.summary,\n",
      "                metadata={\n",
      "                    \"Published\": result.updated.date(),\n",
      "                    \"Title\": result.title,\n",
      "                    \"Authors\": \", \".join(a.name for a in result.authors),\n",
      "                },\n",
      "            )\n",
      "            for result in results\n",
      "        ]\n",
      "        return docs\n",
      " \n",
      "\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"get relevant documents in arxiv retriever\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"===================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/experimental/langchain_experimental/retrievers/__init__.py \n",
      "\n",
      "trace:  \n",
      "\n",
      "snippet: \"\"\"**Retriever** class returns Documents given a text **query**.\n",
      "\n",
      "It is more general than a vector store. A retriever does not need to be able to\n",
      "store documents, only to return (or retrieve) it.\n",
      "\"\"\"\n",
      " \n",
      "\n",
      "===================================================================\n",
      "file: ./langchain/libs/community/langchain_community/vectorstores/neo4j_vector.py \n",
      "\n",
      "trace: class name: Neo4jVector. function name: from_documents \n",
      "\n",
      "snippet:     @classmethod\n",
      "    def from_existing_index(\n",
      "        cls: Type[Neo4jVector],\n",
      "        embedding: Embeddings,\n",
      "        index_name: str,\n",
      "        search_type: SearchType = DEFAULT_SEARCH_TYPE,\n",
      "        keyword_index_name: Optional[str] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Neo4jVector:\n",
      "        \"\"\"\n",
      "        Get instance of an existing Neo4j vector index. This method will\n",
      "        return the instance of the store without inserting any new\n",
      "        embeddings.\n",
      "        Neo4j credentials are required in the form of `url`, `username`,\n",
      "        and `password` and optional `database` parameters along with\n",
      "        the `index_name` definition.\n",
      "        \"\"\"\n",
      "\n",
      "        if search_type == SearchType.HYBRID and not keyword_index_name:\n",
      "            raise ValueError(\n",
      "                \"keyword_index name has to be specified \"\n",
      "                \"when using hybrid search option\"\n",
      "            )\n",
      "\n",
      "        store = cls(\n",
      "            embedding=embedding,\n",
      "            index_name=index_name,\n",
      "            keyword_index_name=keyword_index_name,\n",
      "            search_type=search_type,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        embedding_dimension = store.retrieve_existing_index()\n",
      "\n",
      "        if not embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The specified vector index name does not exist. \"\n",
      "                \"Make sure to check if you spelled it correctly\"\n",
      "            )\n",
      "\n",
      "        # Check if embedding function and vector index dimensions match\n",
      "        if not store.embedding_dimension == embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The provided embedding function and vector index \"\n",
      "                \"dimensions do not match.\\n\"\n",
      "                f\"Embedding function dimension: {store.embedding_dimension}\\n\"\n",
      "                f\"Vector index dimension: {embedding_dimension}\"\n",
      "            )\n",
      "\n",
      "        if search_type == SearchType.HYBRID:\n",
      "            fts_node_label = store.retrieve_existing_fts_index()\n",
      "            # If the FTS index doesn't exist yet\n",
      "            if not fts_node_label:\n",
      "                raise ValueError(\n",
      "                    \"The specified keyword index name does not exist. \"\n",
      "                    \"Make sure to check if you spelled it correctly\"\n",
      "                )\n",
      "            else:  # Validate that FTS and Vector index use the same information\n",
      "                if not fts_node_label == store.node_label:\n",
      "                    raise ValueError(\n",
      "                        \"Vector and keyword index don't index the same node label\"\n",
      "                    )\n",
      "\n",
      "        return store\n",
      " \n",
      "\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"base class for retriever that does not use vector store\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"===================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts_with_bm25_params \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts_with_bm25_params() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(\n",
      "        texts=input_texts, bm25_params={\"epsilon\": 10}\n",
      "    )\n",
      "    # should count only multiple words (have, pan)\n",
      "    assert bm25_retriever.vectorizer.epsilon == 10\n",
      " \n",
      "\n",
      "===================================================================\n",
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\n",
      "    assert len(bm25_retriever.docs) == 3\n",
      "    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\n",
      " \n",
      "\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"bm25 retriever test\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"===================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain.ndb'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.save(\"langchain.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A full copilot system with Chain of Thought**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a copilot system that is powerful enough to answer a question like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I want to integrate a retriever called MyRetriever with this open source codebase. \"\n",
    "    \"It does not use a vector store. \"\n",
    "    \"Create a skeleton for a class that wraps MyRetriever and inherits the right interface. \"\n",
    "    \"(Don't implement the methods, just write #TODOs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Your OpenAI key goes here\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR-OPENAI-KEY\"\n",
    "openai_client = OpenAI() # defaults to os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gpt(query=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None, print_metadata=False):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    sources = []\n",
    "    for result in search_results:\n",
    "        if (print_metadata):\n",
    "            print(result.metadata)\n",
    "        if radius:\n",
    "            references.append(result.metadata['source'] + '\\n' + f\"```{result.context(radius=radius)}```\")\n",
    "        else:\n",
    "            references.append(result.metadata['source'] + '\\n' + f\"```{result.text}```\")\n",
    "    return references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query, radius=None, print_metadata=False):\n",
    "    references = get_references(str(query), radius=radius, print_metadata=print_metadata)\n",
    "    context = \"\\n\\n\".join(references[:5])\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_thoughts(task):\n",
    "    prompt = (\n",
    "        \"Act as a software engineer who is the expert in an unnamed open source codebase. \"\n",
    "        f\"You are asked to do the following:\\n\\n{task}\\n\\n\"\n",
    "        \"You have access to an oracle that that can give you snippets and examples from \"\n",
    "        \"this open source codebase, and only from this open source codebase. \"\n",
    "        \"What pieces of information would you want to get from the oracle to complete the task? \"\n",
    "        \"List them in separate lines.\"\n",
    "    )\n",
    "    # Only return non-empty lines.\n",
    "    return [query for query in query_gpt(prompt).split(\"\\n\") if query]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes or wrappers that are used for integrating external retrievers.\n",
      "3. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "4. Examples of how other retrievers are integrated into the codebase for reference.\n",
      "5. Any guidelines or best practices for implementing the methods in the wrapper class.\n"
     ]
    }
   ],
   "source": [
    "for query in initial_thoughts(task):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_thoughts(task, context, previous_answer=\"\"):\n",
    "    prompt = task\n",
    "    prompt += (\n",
    "        f\"Act as an experienced software engineer:\\n\\n\"\n",
    "        f\"Answer the query ```{task}``` , given your previous answers : ```{previous_answer}```\\n\\n\"\n",
    "        f\"modify your answer based on this new information (do not construct \"\n",
    "        f\"your answer from outside the context provided ): ```{context}```\"\n",
    "    )\n",
    "    response = query_gpt(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copilot(task, radius=None, verbose=False):\n",
    "    queries = initial_thoughts(task)\n",
    "    if verbose:\n",
    "        print(len(queries), \"queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "    draft_answer = \"\"\n",
    "\n",
    "    for query in queries:\n",
    "        if verbose:\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Retrieved references:\")\n",
    "        retrieved_info = get_context(query, radius=radius, print_metadata=verbose) # retrieve neural db response for current thought\n",
    "        # LLM modifies answer based on the previous answer and current ndb results\n",
    "        draft_answer = refine_thoughts(\n",
    "            task,\n",
    "            context=f\"Answers to the query '{query}':\\n\\n{retrieved_info}\",\n",
    "            previous_answer=draft_answer,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"Draft Answer:\")\n",
    "            print(draft_answer)\n",
    "            print(\"========================================\")\n",
    "    return draft_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 queries:\n",
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes or wrappers that integrate external retrievers.\n",
      "3. Examples of how other retrievers are integrated into the codebase.\n",
      "4. Any specific requirements or constraints for integrating external retrievers.\n",
      "5. Any relevant documentation or guidelines for extending the codebase with new retrievers.\n",
      "Query: 1. The interface that the open source codebase uses for retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: __init__', 'lineno': 177.0, 'end_lineno': 181.0, 'id': 20548, 'path_to_file': './langchain/libs/community/langchain_community/llms/sparkllm.py', 'thirdai_index': 20548, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: _create_url', 'lineno': 285.0, 'end_lineno': 289.0, 'id': 15583, 'path_to_file': './langchain/libs/community/langchain_community/chat_models/sparkllm.py', 'thirdai_index': 15583, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class BaseRetriever(RunnableSerializable[RetrieverInput, RetrieverOutput], ABC):\\n    \"\"\"Abstract base class for a Document retrieval system.\\n\\n\\n    A retrieval system is defined as something that can take string queries and return\\n    the most \\'relevant\\' Documents from some source.\\n\\n    Usage:\\n\\n    A retriever follows the standard Runnable interface, and should be used\\n    via the standard runnable methods of `invoke`, `ainvoke`, `batch`, `abatch`.\\n\\n    Implementation:\\n\\n    When implementing a custom retriever, the class should implement\\n    the `_get_relevant_documents` method to define the logic for retrieving documents.\\n\\n    Optionally, an async native implementations can be provided by overriding the\\n    `_aget_relevant_documents` method.\\n\\n    Example: A retriever that returns the first 5 documents from a list of documents\\n\\n        .. code-block:: python\\n\\n            from langchain_core import Document, BaseRetriever\\n            from typing import List\\n\\n            class SimpleRetriever(BaseRetriever):\\n                docs: List[Document]\\n                k: int = 5\\n\\n                def _get_relevant_documents(self, query: str) -> List[Document]:\\n                    \\\\\"\\\\\"\\\\\"Return the first k documents from the list of documents\\\\\"\\\\\"\\\\\"\\n                    return self.docs[:self.k]\\n\\n                async def _aget_relevant_documents(self, query: str) -> List[Document]:\\n                    \\\\\"\\\\\"\\\\\"(Optional) async native implementation.\\\\\"\\\\\"\\\\\"\\n                    return self.docs[:self.k]\\n\\n    Example: A simple retriever based on a scitkit learn vectorizer\\n\\n        .. code-block:: python\\n\\n            from sklearn.metrics.pairwise import cosine_similarity\\n\\n            class TFIDFRetriever(BaseRetriever, BaseModel):\\n                vectorizer: Any\\n                docs: List[Document]\\n                tfidf_array: Any\\n                k: int = 4\\n\\n                class Config:\\n                    arbitrary_types_allowed = True\\n\\n                def _get_relevant_documents(self, query: str) -> List[Document]:\\n                    # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\\n                    query_vec = self.vectorizer.transform([query])\\n                    # Op -- (n_docs,1) -- Cosine Sim with each doc\\n                    results = cosine_similarity(self.tfidf_array, query_vec).reshape((-1,))\\n                    return [self.docs[i] for i in results.argsort()[-self.k :][::-1]]\\n    \"\"\"  # noqa: E501\\n', 'trace': 'class name: BaseRetriever. ', 'lineno': 51.0, 'end_lineno': 111.0, 'id': 1998, 'path_to_file': './langchain/libs/core/langchain_core/retrievers.py', 'thirdai_index': 1998, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "========================================\n",
      "Query: 2. Any existing classes or wrappers that integrate external retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': '\"\"\"All integration tests (tests that call out to an external API).\"\"\"\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 1.0, 'id': 7886, 'path_to_file': './langchain/libs/langchain/tests/integration_tests/__init__.py', 'thirdai_index': 7886, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"Test Xinference wrapper.\"\"\"\\n\\nimport time\\nfrom typing import AsyncGenerator, Tuple\\n\\nimport pytest_asyncio\\n\\nfrom langchain_community.llms import Xinference\\n\\n\\n@pytest_asyncio.fixture\\nasync def setup() -> AsyncGenerator[Tuple[str, str], None]:\\n    import xoscar as xo\\n    from xinference.deploy.supervisor import start_supervisor_components\\n    from xinference.deploy.utils import create_worker_actor_pool\\n    from xinference.deploy.worker import start_worker_components\\n\\n    pool = await create_worker_actor_pool(\\n        f\"test://127.0.0.1:{xo.utils.get_next_port()}\"\\n    )\\n    print(f\"Pool running on localhost:{pool.external_address}\")  # noqa: T201\\n\\n    endpoint = await start_supervisor_components(\\n        pool.external_address, \"127.0.0.1\", xo.utils.get_next_port()\\n    )\\n    await start_worker_components(\\n        address=pool.external_address, supervisor_address=pool.external_address\\n    )\\n\\n    # wait for the api.\\n    time.sleep(3)\\n    async with pool:\\n        yield endpoint, pool.external_address\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 33.0, 'id': 13054, 'path_to_file': './langchain/libs/community/tests/integration_tests/llms/test_xinference.py', 'thirdai_index': 13054, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"Agent toolkits contain integrations with various resources and services.\\n\\nLangChain has a large ecosystem of integrations with various external resources\\nlike local and remote file systems, APIs and databases.\\n\\nThese integrations allow developers to create versatile applications that combine the\\npower of LLMs with the ability to access, interact with and manipulate external\\nresources.\\n\\nWhen developing an application, developers should inspect the capabilities and\\npermissions of the tools that underlie the given agent toolkit, and determine\\nwhether permissions of the given toolkit are appropriate for the application.\\n\\nSee [Security](https://python.langchain.com/docs/security) for more information.\\n\"\"\"\\nimport warnings\\nfrom pathlib import Path\\nfrom typing import Any\\n\\nfrom langchain_core._api import LangChainDeprecationWarning\\nfrom langchain_core._api.path import as_import_path\\n\\nfrom langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import (\\n    create_conversational_retrieval_agent,\\n)\\nfrom langchain.agents.agent_toolkits.vectorstore.base import (\\n    create_vectorstore_agent,\\n    create_vectorstore_router_agent,\\n)\\nfrom langchain.agents.agent_toolkits.vectorstore.toolkit import (\\n    VectorStoreInfo,\\n    VectorStoreRouterToolkit,\\n    VectorStoreToolkit,\\n)\\nfrom langchain.tools.retriever import create_retriever_tool\\nfrom langchain.utils.interactive_env import is_interactive_env\\n\\nDEPRECATED_AGENTS = [\\n    \"create_csv_agent\",\\n    \"create_pandas_dataframe_agent\",\\n    \"create_xorbits_agent\",\\n    \"create_python_agent\",\\n    \"create_spark_dataframe_agent\",\\n]\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 44.0, 'id': 4934, 'path_to_file': './langchain/libs/langchain/langchain/agents/agent_toolkits/__init__.py', 'thirdai_index': 4934, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "========================================\n",
      "Query: 3. Examples of how other retrievers are integrated into the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': 'async def test_chat_from_role_strings() -> None:\\n    \"\"\"Test instantiation of chat template from role strings.\"\"\"\\n    with pytest.warns(LangChainPendingDeprecationWarning):\\n        template = ChatPromptTemplate.from_role_strings(\\n            [\\n                (\"system\", \"You are a bot.\"),\\n                (\"assistant\", \"hello!\"),\\n                (\"human\", \"{question}\"),\\n                (\"other\", \"{quack}\"),\\n            ]\\n        )\\n\\n    expected = [\\n        ChatMessage(content=\"You are a bot.\", role=\"system\"),\\n        ChatMessage(content=\"hello!\", role=\"assistant\"),\\n        ChatMessage(content=\"How are you?\", role=\"human\"),\\n        ChatMessage(content=\"duck\", role=\"other\"),\\n    ]\\n\\n    messages = template.format_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n\\n    messages = await template.aformat_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n', 'trace': '', 'lineno': 302.0, 'end_lineno': 325.0, 'id': 1819, 'path_to_file': './langchain/libs/core/tests/unit_tests/prompts/test_chat.py', 'thirdai_index': 1819, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class ConversationalRetrievalChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for having a conversation based on retrieved documents.\\n\\n    This chain takes in chat history (a list of messages) and new questions,\\n    and then returns an answer to that question.\\n    The algorithm for this chain consists of three parts:\\n\\n    1. Use the chat history and the new question to create a \"standalone question\".\\n    This is done so that this question can be passed into the retrieval step to fetch\\n    relevant documents. If only the new question was passed in, then relevant context\\n    may be lacking. If the whole conversation was passed into retrieval, there may\\n    be unnecessary information there that would distract from retrieval.\\n\\n    2. This new question is passed to the retriever and relevant documents are\\n    returned.\\n\\n    3. The retrieved documents are passed to an LLM along with either the new question\\n    (default behavior) or the original question and chat history to generate a final\\n    response.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            combine_docs_chain = StuffDocumentsChain(...)\\n            vectorstore = ...\\n            retriever = vectorstore.as_retriever()\\n\\n            # This controls how the standalone question is generated.\\n            # Should take `chat_history` and `question` as input variables.\\n            template = (\\n                \"Combine the chat history and follow up question into \"\\n                \"a standalone question. Chat History: {chat_history}\"\\n                \"Follow up question: {question}\"\\n            )\\n            prompt = PromptTemplate.from_template(template)\\n            llm = OpenAI()\\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = ConversationalRetrievalChain(\\n                combine_docs_chain=combine_docs_chain,\\n                retriever=retriever,\\n                question_generator=question_generator_chain,\\n            )\\n    \"\"\"\\n\\n    retriever: BaseRetriever\\n    \"\"\"Retriever to use to fetch documents.\"\"\"\\n    max_tokens_limit: Optional[int] = None\\n    \"\"\"If set, enforces that the documents returned are less than this limit.\\n    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\\n', 'trace': 'class name: ConversationalRetrievalChain. ', 'lineno': 236.0, 'end_lineno': 290.0, 'id': 6700, 'path_to_file': './langchain/libs/langchain/langchain/chains/conversational_retrieval/base.py', 'thirdai_index': 6700, 'source': 'langchain.csv'}\n",
      "{'snippet': 'def get_buffer_string(\\n    messages: Sequence[BaseMessage], human_prefix: str = \"Human\", ai_prefix: str = \"AI\"\\n) -> str:\\n    \"\"\"Convert a sequence of Messages to strings and concatenate them into one string.\\n\\n    Args:\\n        messages: Messages to be converted to strings.\\n        human_prefix: The prefix to prepend to contents of HumanMessages.\\n        ai_prefix: THe prefix to prepend to contents of AIMessages.\\n\\n    Returns:\\n        A single string concatenation of all input messages.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain_core import AIMessage, HumanMessage\\n\\n            messages = [\\n                HumanMessage(content=\"Hi, how are you?\"),\\n                AIMessage(content=\"Good, how are you?\"),\\n            ]\\n            get_buffer_string(messages)\\n            # -> \"Human: Hi, how are you?\\\\nAI: Good, how are you?\"\\n    \"\"\"\\n    string_messages = []\\n    for m in messages:\\n        if isinstance(m, HumanMessage):\\n            role = human_prefix\\n        elif isinstance(m, AIMessage):\\n            role = ai_prefix\\n        elif isinstance(m, SystemMessage):\\n            role = \"System\"\\n        elif isinstance(m, FunctionMessage):\\n            role = \"Function\"\\n        elif isinstance(m, ToolMessage):\\n            role = \"Tool\"\\n        elif isinstance(m, ChatMessage):\\n            role = m.role\\n        else:\\n            raise ValueError(f\"Got unsupported message type: {m}\")\\n        message = f\"{role}: {m.content}\"\\n        if isinstance(m, AIMessage) and \"function_call\" in m.additional_kwargs:\\n            message += f\"{m.additional_kwargs[\\'function_call\\']}\"\\n        string_messages.append(message)\\n\\n    return \"\\\\n\".join(string_messages)\\n', 'trace': 'function name: get_buffer_string', 'lineno': 22.0, 'end_lineno': 68.0, 'id': 2276, 'path_to_file': './langchain/libs/core/langchain_core/messages/utils.py', 'thirdai_index': 2276, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "========================================\n",
      "Query: 4. Any specific requirements or constraints for integrating external retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': '@pytest.mark.requires(\"rank_bm25\")\\ndef test_from_texts() -> None:\\n    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\\n    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\\n    assert len(bm25_retriever.docs) == 3\\n    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\\n', 'trace': 'function name: test_from_texts', 'lineno': 7.0, 'end_lineno': 12.0, 'id': 9651, 'path_to_file': './langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py', 'thirdai_index': 9651, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"All integration tests (tests that call out to an external API).\"\"\"\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 1.0, 'id': 7886, 'path_to_file': './langchain/libs/langchain/tests/integration_tests/__init__.py', 'thirdai_index': 7886, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def add_constraint(self, constraint: str) -> None:\\n        \"\"\"\\n        Add a constraint to the constraints list.\\n\\n        Args:\\n            constraint (str): The constraint to be added.\\n        \"\"\"\\n        self.constraints.append(constraint)\\n', 'trace': 'class name: PromptGenerator. function name: add_constraint', 'lineno': 36.0, 'end_lineno': 43.0, 'id': 515, 'path_to_file': './langchain/libs/experimental/langchain_experimental/autonomous_agents/autogpt/prompt_generator.py', 'thirdai_index': 515, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "Based on the new information provided, the skeleton for the class that wraps MyRetriever and inherits the right interface would be as follows:\n",
      "\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "\n",
      "This skeleton class provides a structure for integrating MyRetriever with the open source codebase. The methods `_get_relevant_documents` and `_aget_relevant_documents` are placeholders for implementing the logic to retrieve relevant documents using MyRetriever.\n",
      "========================================\n",
      "Query: 5. Any relevant documentation or guidelines for extending the codebase with new retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': '    def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:\\n        \"\"\"Return VectorStoreRetriever initialized from this VectorStore.\\n\\n        Args:\\n            search_type (Optional[str]): Defines the type of search that\\n                the Retriever should perform.\\n                Can be \"similarity\" (default), \"mmr\", or\\n                \"similarity_score_threshold\".\\n            search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n                search function. Can include things like:\\n                    k: Amount of documents to return (Default: 4)\\n                    score_threshold: Minimum relevance threshold\\n                        for similarity_score_threshold\\n                    fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n                    lambda_mult: Diversity of results returned by MMR;\\n                        1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n                    filter: Filter by document metadata\\n\\n        Returns:\\n            VectorStoreRetriever: Retriever class for VectorStore.\\n\\n        Examples:\\n\\n        .. code-block:: python\\n\\n            # Retrieve more documents with higher diversity\\n            # Useful if your dataset has many similar documents\\n            docsearch.as_retriever(\\n                search_type=\"mmr\",\\n                search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n            )\\n\\n            # Fetch more documents for the MMR algorithm to consider\\n            # But only return the top 5\\n            docsearch.as_retriever(\\n                search_type=\"mmr\",\\n                search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n            )\\n\\n            # Only retrieve documents that have a relevance score\\n            # Above a certain threshold\\n            docsearch.as_retriever(\\n                search_type=\"similarity_score_threshold\",\\n                search_kwargs={\\'score_threshold\\': 0.8}\\n            )\\n\\n            # Only get the single most similar document from the dataset\\n            docsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n            # Use a filter to only retrieve documents from a specific paper\\n            docsearch.as_retriever(\\n                search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n            )\\n        \"\"\"\\n        tags = kwargs.pop(\"tags\", None) or []\\n        tags.extend(self._get_retriever_tags())\\n        return VectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)\\n', 'trace': 'class name: VectorStore. function name: as_retriever', 'lineno': 595.0, 'end_lineno': 651.0, 'id': 1975, 'path_to_file': './langchain/libs/core/langchain_core/vectorstores.py', 'thirdai_index': 1975, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        _texts = list(texts)\\n        _ids = ids or [str(uuid4()) for _ in _texts]\\n        self._texts.extend(_texts)\\n        self._embeddings.extend(self._embedding_function.embed_documents(_texts))\\n        self._metadatas.extend(metadatas or ([{}] * len(_texts)))\\n        self._ids.extend(_ids)\\n        self._update_neighbors()\\n        return _ids\\n', 'trace': 'class name: SKLearnVectorStore. function name: _similarity_search_with_relevance_scores', 'lineno': 197.0, 'end_lineno': 211.0, 'id': 18342, 'path_to_file': './langchain/libs/community/langchain_community/vectorstores/sklearn.py', 'thirdai_index': 18342, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def retrieve_documents(\\n        self, queries: List[str], run_manager: CallbackManagerForRetrieverRun\\n    ) -> List[Document]:\\n        \"\"\"Run all LLM generated queries.\\n\\n        Args:\\n            queries: query list\\n\\n        Returns:\\n            List of retrieved Documents\\n        \"\"\"\\n        documents = []\\n        for query in queries:\\n            docs = self.retriever.get_relevant_documents(\\n                query, callbacks=run_manager.get_child()\\n            )\\n            documents.extend(docs)\\n        return documents\\n', 'trace': 'class name: MultiQueryRetriever. function name: unique_union', 'lineno': 186.0, 'end_lineno': 203.0, 'id': 3729, 'path_to_file': './langchain/libs/langchain/langchain/retrievers/multi_query.py', 'thirdai_index': 3729, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "Based on the new information provided, the skeleton for the class that wraps MyRetriever and inherits the right interface would be as follows:\n",
      "\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "\n",
      "This skeleton class provides a structure for integrating MyRetriever with the open source codebase. The methods `_get_relevant_documents` and `_aget_relevant_documents` are placeholders for implementing the logic to retrieve relevant documents using MyRetriever.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "answer = copilot(task=task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the new information provided, the skeleton for the class that wraps MyRetriever and inherits the right interface would be as follows:\n",
      "\n",
      "```python\n",
      "from langchain_core import BaseRetriever\n",
      "from typing import List\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "\n",
      "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
      "        # TODO: Implement async logic to retrieve relevant documents using MyRetriever\n",
      "        pass\n",
      "```\n",
      "\n",
      "This skeleton class provides a structure for integrating MyRetriever with the open source codebase. The methods `_get_relevant_documents` and `_aget_relevant_documents` are placeholders for implementing the logic to retrieve relevant documents using MyRetriever.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
