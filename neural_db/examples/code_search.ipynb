{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Code Search System**\n",
    "We will build a code search system based on the Langchain codebase. Our system will be able to answer queries such as:\n",
    "* get relevant documents in arxiv retriever\n",
    "* base class for retriever that does not use vector store\n",
    "* bm25 retriever test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clone the Langchain Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git\n",
    "\n",
    "# And install the thirdai package\n",
    "%pip install thirdai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "To ensure that each chunk is semantically coherent, we will split it along class and function boundaries. In addition, for our use case, it's important to know which file, class, and/or function a snippet is taken from. This kind of information is perfect for utilizing NeuralDB's notion of strong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "def apply_to_codebase(path_to_codebase, chunking_strategy):\n",
    "    \"\"\"Traverses entire codebase and applies chunking strategy to all files.\n",
    "    Returns a dataframe with 5 columns: id, chunk, path_to_file, lineno, end_lineno\n",
    "    `lineno` is the line number (not line index) that the chunk starts on.\n",
    "    `end_lineno` is the chunk's last line number (again, not line index).\n",
    "    For example, \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    warn = False\n",
    "    for path_to_file in glob.iglob(f\"{path_to_codebase}/**/*.*\", recursive = True):\n",
    "        if path_to_file.endswith(\".py\"):\n",
    "            try:\n",
    "                script = open(path_to_file).read()\n",
    "                ast_body = ast.parse(script).body\n",
    "                script_lines = script.splitlines(keepends=True)\n",
    "                df = chunking_strategy(ast_body, script_lines)\n",
    "                df[\"path_to_file\"] = [path_to_file for _ in range(len(df))]\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to open\", path_to_file)\n",
    "                print(\"Reason:\", e)\n",
    "                print(\"Skipping...\")\n",
    "        else:\n",
    "            warn = True\n",
    "        \n",
    "    if warn:\n",
    "        warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def split_by_function(ast_body: List[ast.AST], script_lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ast_body: List of elements in the Python script as returned by `ast.parse(script).body`\n",
    "    script_lines: List of lines in the Python script\n",
    "\n",
    "    The script will be split into snippets according to these rules:\n",
    "    - Each function is a chunk\n",
    "    - Each method of a class is a chunk\n",
    "    - Expressions between functions or classes are clubbed together.\n",
    "    - Comments (not docstrings) are clubbed with the next chunk\n",
    "\n",
    "    This chunking method produces a dataframe with four columns:\n",
    "    - snippet: A snippet from the codebase\n",
    "    - trace: The stack trace of the snippet; the class and/or function from which\n",
    "      the snippet is taken\n",
    "    - lineno: The line number where the snippet starts. Note that it is 1-indexed,\n",
    "      which is consistent with the lineno returned by the AST module.\n",
    "    - end_lineno: The line number of the last line of the snippet. Like lineno, \n",
    "      it is 1-indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    start_linenos, end_linenos, traces = _split_by_function(\n",
    "        ast_body=ast_body,\n",
    "        start_lineno=1,\n",
    "        end_lineno=len(script_lines),\n",
    "    )\n",
    "    \n",
    "    # Only keep non-empty lines.\n",
    "\n",
    "    for i in range(len(start_linenos)):\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[start_linenos[i] - 1].strip():\n",
    "            start_linenos[i] += 1\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[end_linenos[i] - 1].strip():\n",
    "            end_linenos[i] -= 1\n",
    "    \n",
    "    snippets = []\n",
    "    final_traces = []\n",
    "    final_linenos = []\n",
    "    final_end_linenos = []\n",
    "\n",
    "    for lineno, end_lineno, snippet_trace in zip(start_linenos, end_linenos, traces):\n",
    "        if lineno <= end_lineno:\n",
    "            snippets.append(\"\".join(script_lines[lineno - 1: end_lineno]))\n",
    "            final_traces.append(snippet_trace)\n",
    "            final_linenos.append(lineno)\n",
    "            final_end_linenos.append(end_lineno)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"snippet\": snippets,\n",
    "        \"trace\": final_traces,\n",
    "        \"lineno\": final_linenos,\n",
    "        \"end_lineno\": final_end_linenos,\n",
    "    })\n",
    "\n",
    "def _split_by_function(ast_body: List[ast.AST], start_lineno: int, end_lineno: int):\n",
    "    \"\"\"This helper function allows us to reuse chunking logic within a scope\n",
    "    such as a class.\n",
    "    \"\"\"\n",
    "    start_linenos = []\n",
    "    end_linenos = []\n",
    "    traces = []\n",
    "    can_connect = False\n",
    "\n",
    "    # start_lineno is always the previous end_lineno + 1\n",
    "    # This is because comments are not captured by the AST parser.\n",
    "    # Thus, to keep comments, we must keep all lines between the previous\n",
    "    # element and the current element.\n",
    "\n",
    "    for elem in ast_body:\n",
    "        if isinstance(elem, ast.FunctionDef):\n",
    "            # A function block is always its own chunk\n",
    "            start_linenos.append(start_lineno)\n",
    "            end_linenos.append(elem.end_lineno)\n",
    "            # Add function name to trace.\n",
    "            traces.append(f\"function name: {elem.name}\")\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        elif isinstance(elem, ast.ClassDef):\n",
    "            # A class is treated as a mini-script;\n",
    "            # functions/methods inside a class are their own snippets.\n",
    "            class_start_linenos, class_end_linenos, class_trace = _split_by_function(\n",
    "                ast_body=elem.body,\n",
    "                start_lineno=start_lineno,\n",
    "                end_lineno=elem.end_lineno,\n",
    "            )\n",
    "            start_linenos.extend(class_start_linenos)\n",
    "            end_linenos.extend(class_end_linenos)\n",
    "            # Prepend class name to the trace of every snippet in the class.\n",
    "            traces.extend([f\"class name: {elem.name}. {trace}\" for trace in class_trace])\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        else:\n",
    "            # Group expressions in the global scope that are neither functions\n",
    "            # nor classes.\n",
    "            if can_connect:\n",
    "                end_linenos[-1] = elem.end_lineno\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "            else:\n",
    "                start_linenos.append(start_lineno)\n",
    "                end_linenos.append(elem.end_lineno)\n",
    "                # Append an empty string so `traces`` is always the same length\n",
    "                # as start_linenos and end_linenos\n",
    "                traces.append(\"\")\n",
    "                can_connect = True\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "    start_linenos.append(start_lineno)\n",
    "    end_linenos.append(end_lineno)\n",
    "\n",
    "    return start_linenos, end_linenos, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_snippets = apply_to_codebase(\"./langchain\", split_by_function)\n",
    "langchain_snippets.to_csv(\"langchain.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build NeuralDB**\n",
    "As previously mentioned, NeuralDB has a notion of strong and weak columns.\n",
    "\n",
    "`strong_columns` are columns in your CSV file that contains “strong” signals; words or strings that you want exact matches with, such as keywords, brands, categories, or a stack trace.\n",
    "\n",
    "`weak_columns` contain “weak” signals; phrases or passages that you want rough or semantic matches with, such as product descriptions, chunks of an essay, or code snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "\n",
    "# TODO: Your ThirdAI key goes here\n",
    "licensing.activate(\"YOUR-THIRDAI-KEY\")\n",
    "\n",
    "doc = ndb.CSV(\n",
    "    \"langchain.csv\",\n",
    "    # Path to file and stack trace are strong signals.\n",
    "    strong_columns=[\"path_to_file\", \"trace\"],\n",
    "    # Code snippets contain weak signals\n",
    "    weak_columns=[\"snippet\"],\n",
    "    reference_columns=[\"snippet\"],\n",
    ")\n",
    "\n",
    "db = ndb.NeuralDB()\n",
    "\n",
    "db.insert([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's test it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in db.search(\"get relevant documents in arxiv retriever\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in db.search(\"base class for retriever that does not use vector store\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in db.search(\"bm25 retriever test\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save(\"langchain.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A full copilot system with _Chain of Thought_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a copilot system that is powerful enough to answer a question like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I want to integrate a retriever called MyRetriever with this open source codebase. \"\n",
    "    \"It does not use a vector store. \"\n",
    "    \"Create a skeleton for a class that wraps MyRetriever and inherits the right interface. \"\n",
    "    \"(Don't implement the methods, just write #TODOs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model query script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Your OpenAI key goes here\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR-OPENAI-KEY\"\n",
    "openai_client = OpenAI() # defaults to os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def query_gpt(query=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and retreive the relevant code snippet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None, print_metadata=False):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        if (print_metadata):\n",
    "            print(f\"Reference {i + 1} \\n{result.text}\")\n",
    "        if radius:\n",
    "            references.append(f\"```{result.context(radius=radius)}```\")\n",
    "        else:\n",
    "            references.append(f\"```{result.text}```\")\n",
    "    return references\n",
    "\n",
    "def get_context(query, radius=None, print_metadata=False):\n",
    "    references = get_references(str(query), radius=radius, print_metadata=print_metadata)\n",
    "    context = \"\\n\\n\".join(references[:5])\n",
    "    return context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial thoughts\n",
    "Action items required to accomplish the above given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_thoughts(task):\n",
    "    prompt = (\n",
    "        \"Act as a software engineer who is the expert in an unnamed open source codebase. \"\n",
    "        f\"You are asked to do the following:\\n\\n{task}\\n\\n\"\n",
    "        \"You have access to an oracle that can give you snippets and examples from \"\n",
    "        \"this open source codebase, and only from this open source codebase. \"\n",
    "        \"What pieces of information would you want to get from the oracle to complete the task? \"\n",
    "        \"List them in separate lines.\"\n",
    "    )\n",
    "    # Only return non-empty lines.\n",
    "    return [query for query in query_gpt(prompt).split(\"\\n\") if query]\n",
    "\n",
    "for query in initial_thoughts(task):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_thoughts(task, context, previous_answer=\"\"):\n",
    "    prompt = task\n",
    "    prompt += (\n",
    "        f\"Act as an experienced software engineer:\\n\\n\"\n",
    "        f\"Answer the query ```{task}``` , given your previous answers : ```{previous_answer}```\\n\\n\"\n",
    "        f\"modify your answer based on this new information (do not construct \"\n",
    "        f\"your answer from outside the context provided ): ```{context}```\"\n",
    "    )\n",
    "    response = query_gpt(prompt)\n",
    "    return response\n",
    "\n",
    "def copilot(task, radius=None, verbose=False):\n",
    "    queries = initial_thoughts(task)\n",
    "    if verbose:\n",
    "        print(len(queries), \"queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    draft_answer = \"\"\n",
    "\n",
    "    for query in queries:\n",
    "        if verbose:\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Retrieved references:\")\n",
    "        retrieved_info = get_context(query, radius=radius, print_metadata=verbose) # retrieve neural db response for current thought\n",
    "        # LLM modifies answer based on the previous answer and current ndb results\n",
    "        draft_answer = refine_thoughts(\n",
    "            task,\n",
    "            context=f\"Answers to the query '{query}':\\n\\n{retrieved_info}\",\n",
    "            previous_answer=draft_answer,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"Draft Answer:\")\n",
    "            print(draft_answer)\n",
    "            print(\"=\" * 100)\n",
    "    return draft_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get this task done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = copilot(task=task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
