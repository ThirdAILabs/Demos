{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Code Search System**\n",
    "We will build a code search system based on the Langchain codebase. Our system will be able to answer queries such as:\n",
    "* get relevant documents in arxiv retriever\n",
    "* base class for retriever that does not use vector store\n",
    "* bm25 retriever test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clone the Langchain Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 171026, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 171026 (delta 0), reused 0 (delta 0), pack-reused 171025\u001b[K\n",
      "Receiving objects: 100% (171026/171026), 231.80 MiB | 37.50 MiB/s, done.\n",
      "Resolving deltas: 100% (127816/127816), done.\n",
      "Updating files: 100% (7421/7421), done.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: thirdai in /home/gautam/.local/lib/python3.8/site-packages (0.8.4)\n",
      "Requirement already satisfied: Office365-REST-Python-Client==2.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.5.1)\n",
      "Requirement already satisfied: PyMuPDF==1.23.26 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.23.26)\n",
      "Requirement already satisfied: PyTrie in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.4.0)\n",
      "Requirement already satisfied: SQLAlchemy>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.0.17)\n",
      "Requirement already satisfied: bs4 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.0.1)\n",
      "Requirement already satisfied: dask[complete] in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2023.5.0)\n",
      "Requirement already satisfied: ipython in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (8.12.2)\n",
      "Requirement already satisfied: langchain in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.1.16)\n",
      "Requirement already satisfied: langchain-community in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.0.33)\n",
      "Requirement already satisfied: lxml[html_clean] in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.9.3)\n",
      "Requirement already satisfied: nltk in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.24.4)\n",
      "Requirement already satisfied: openai in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.19.0)\n",
      "Requirement already satisfied: pandas<=2.1.4,>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.0.3)\n",
      "Requirement already satisfied: pdfplumber in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.10.3)\n",
      "Requirement already satisfied: pydantic in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.4.2)\n",
      "Requirement already satisfied: python-docx in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (0.8.11)\n",
      "Requirement already satisfied: requests in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.2.2)\n",
      "Requirement already satisfied: simple-salesforce==1.12.5 in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.12.5)\n",
      "Requirement already satisfied: sortedcontainers in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (2.4.0)\n",
      "Requirement already satisfied: tqdm in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.65.0)\n",
      "Requirement already satisfied: trafilatura in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (4.11.0)\n",
      "Requirement already satisfied: unidecode in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.3.6)\n",
      "Requirement already satisfied: url-normalize in /home/gautam/.local/lib/python3.8/site-packages (from thirdai) (1.4.3)\n",
      "Requirement already satisfied: msal in /home/gautam/.local/lib/python3.8/site-packages (from Office365-REST-Python-Client==2.5.1->thirdai) (1.24.1)\n",
      "Requirement already satisfied: pytz in /home/gautam/.local/lib/python3.8/site-packages (from Office365-REST-Python-Client==2.5.1->thirdai) (2023.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/gautam/.local/lib/python3.8/site-packages (from PyMuPDF==1.23.26->thirdai) (1.23.22)\n",
      "Requirement already satisfied: cryptography in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (42.0.1)\n",
      "Requirement already satisfied: zeep in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (4.2.1)\n",
      "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from simple-salesforce==1.12.5->thirdai) (1.7.1)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from simple-salesforce==1.12.5->thirdai) (4.2.0)\n",
      "Requirement already satisfied: pendulum in /home/gautam/.local/lib/python3.8/site-packages (from simple-salesforce==1.12.5->thirdai) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gautam/.local/lib/python3.8/site-packages (from pandas<=2.1.4,>=2.0.0->thirdai) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/gautam/.local/lib/python3.8/site-packages (from pandas<=2.1.4,>=2.0.0->thirdai) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gautam/.local/lib/python3.8/site-packages (from requests->thirdai) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/gautam/.local/lib/python3.8/site-packages (from SQLAlchemy>=2.0.0->thirdai) (2.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/gautam/.local/lib/python3.8/site-packages (from bs4->thirdai) (4.8.2)\n",
      "Requirement already satisfied: click>=8.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (23.2)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/lib/python3/dist-packages (from dask[complete]->thirdai) (5.3.1)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (6.7.0)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (12.0.1)\n",
      "Requirement already satisfied: lz4>=4.3.2 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (4.3.3)\n",
      "Requirement already satisfied: backcall in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /home/gautam/.local/lib/python3.8/site-packages (from ipython->thirdai) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython->thirdai) (4.6.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (3.8.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.5.14)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.1.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (0.1.48)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/gautam/.local/lib/python3.8/site-packages (from langchain->thirdai) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from pydantic->thirdai) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /home/gautam/.local/lib/python3.8/site-packages (from pydantic->thirdai) (2.10.1)\n",
      "\u001b[33mWARNING: lxml 4.9.3 does not provide the extra 'html-clean'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: joblib in /home/gautam/.local/lib/python3.8/site-packages (from nltk->thirdai) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from nltk->thirdai) (2023.8.8)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/gautam/.local/lib/python3.8/site-packages (from openai->thirdai) (1.3.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (20221105)\n",
      "Requirement already satisfied: Pillow>=9.1 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (10.2.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /home/gautam/.local/lib/python3.8/site-packages (from pdfplumber->thirdai) (4.26.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/gautam/.local/lib/python3.8/site-packages (from scikit-learn->thirdai) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from scikit-learn->thirdai) (3.1.0)\n",
      "Requirement already satisfied: courlan>=0.9.4 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (0.9.4)\n",
      "Requirement already satisfied: htmldate>=1.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (1.5.1)\n",
      "Requirement already satisfied: justext>=3.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from trafilatura->thirdai) (3.0.0)\n",
      "Requirement already satisfied: six in /home/gautam/.local/lib/python3.8/site-packages (from url-normalize->thirdai) (1.12.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gautam/.local/lib/python3.8/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gautam/.local/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai->thirdai) (1.1.3)\n",
      "Requirement already satisfied: langcodes>=3.3.0 in /home/gautam/.local/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura->thirdai) (3.3.0)\n",
      "Requirement already satisfied: tld>=0.13 in /home/gautam/.local/lib/python3.8/site-packages (from courlan>=0.9.4->trafilatura->thirdai) (0.13)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/gautam/.local/lib/python3.8/site-packages (from cryptography->simple-salesforce==1.12.5->thirdai) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gautam/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (0.9.0)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in /home/gautam/.local/lib/python3.8/site-packages (from htmldate>=1.5.1->trafilatura->thirdai) (1.1.8)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gautam/.local/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai->thirdai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gautam/.local/lib/python3.8/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->thirdai) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gautam/.local/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask[complete]->thirdai) (3.17.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/gautam/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython->thirdai) (0.8.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/lib/python3/dist-packages (from jsonpatch<2.0,>=1.33->langchain->thirdai) (2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/gautam/.local/lib/python3.8/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->thirdai) (3.10.1)\n",
      "Requirement already satisfied: locket in /home/gautam/.local/lib/python3.8/site-packages (from partd>=1.2.0->dask[complete]->thirdai) (1.0.0)\n",
      "Requirement already satisfied: wcwidth in /home/gautam/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->thirdai) (0.2.6)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/gautam/.local/lib/python3.8/site-packages (from beautifulsoup4->bs4->thirdai) (2.5)\n",
      "Requirement already satisfied: distributed==2023.5.0 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (2023.5.0)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (3.1.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in /home/gautam/.local/lib/python3.8/site-packages (from dask[complete]->thirdai) (3.1.2)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (1.0.5)\n",
      "Requirement already satisfied: psutil>=5.7.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (5.9.5)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (6.3.3)\n",
      "Requirement already satisfied: zict>=2.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from distributed==2023.5.0->dask[complete]->thirdai) (3.0.0)\n",
      "Requirement already satisfied: backports.zoneinfo>=0.2.1 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (0.2.1)\n",
      "Requirement already satisfied: time-machine>=2.6.0 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (2.13.0)\n",
      "Requirement already satisfied: importlib-resources>=5.9.0 in /home/gautam/.local/lib/python3.8/site-packages (from pendulum->simple-salesforce==1.12.5->thirdai) (5.12.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/gautam/.local/lib/python3.8/site-packages (from stack-data->ipython->thirdai) (0.2.2)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (0.6.1)\n",
      "Requirement already satisfied: platformdirs>=1.4.0 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (3.10.0)\n",
      "Requirement already satisfied: requests-toolbelt>=0.7.1 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (1.0.0)\n",
      "Requirement already satisfied: requests-file>=1.5.1 in /home/gautam/.local/lib/python3.8/site-packages (from zeep->simple-salesforce==1.12.5->thirdai) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1 in /home/gautam/.local/lib/python3.8/site-packages (from bokeh>=2.4.2->dask[complete]->thirdai) (1.1.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/gautam/.local/lib/python3.8/site-packages (from bokeh>=2.4.2->dask[complete]->thirdai) (2024.4.0)\n",
      "Requirement already satisfied: pycparser in /home/gautam/.local/lib/python3.8/site-packages (from cffi>=1.12->cryptography->simple-salesforce==1.12.5->thirdai) (2.21)\n",
      "Requirement already satisfied: tzlocal in /home/gautam/.local/lib/python3.8/site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura->thirdai) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gautam/.local/lib/python3.8/site-packages (from jinja2>=2.10.3->dask[complete]->thirdai) (2.1.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gautam/.local/lib/python3.8/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->thirdai) (1.0.0)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git\n",
    "\n",
    "# And install the thirdai package\n",
    "%pip install thirdai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "To ensure that each chunk is semantically coherent, we will split it along class and function boundaries. In addition, for our use case, it's important to know which file, class, and/or function a snippet is taken from. This kind of information is perfect for utilizing NeuralDB's notion of strong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "def apply_to_codebase(path_to_codebase, chunking_strategy):\n",
    "    \"\"\"Traverses entire codebase and applies chunking strategy to all files.\n",
    "    Returns a dataframe with 5 columns: id, chunk, path_to_file, lineno, end_lineno\n",
    "    `lineno` is the line number (not line index) that the chunk starts on.\n",
    "    `end_lineno` is the chunk's last line number (again, not line index).\n",
    "    For example, \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    warn = False\n",
    "    for path_to_file in glob.iglob(f\"{path_to_codebase}/**/*.*\", recursive = True):\n",
    "        if path_to_file.endswith(\".py\"):\n",
    "            try:\n",
    "                script = open(path_to_file).read()\n",
    "                ast_body = ast.parse(script).body\n",
    "                script_lines = script.splitlines(keepends=True)\n",
    "                df = chunking_strategy(ast_body, script_lines)\n",
    "                df[\"path_to_file\"] = [path_to_file for _ in range(len(df))]\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to open\", path_to_file)\n",
    "                print(\"Reason:\", e)\n",
    "                print(\"Skipping...\")\n",
    "        else:\n",
    "            warn = True\n",
    "        \n",
    "    if warn:\n",
    "        warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def split_by_function(ast_body: List[ast.AST], script_lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ast_body: List of elements in the Python script as returned by `ast.parse(script).body`\n",
    "    script_lines: List of lines in the Python script\n",
    "\n",
    "    The script will be split into snippets according to these rules:\n",
    "    - Each function is a chunk\n",
    "    - Each method of a class is a chunk\n",
    "    - Expressions between functions or classes are clubbed together.\n",
    "    - Comments (not docstrings) are clubbed with the next chunk\n",
    "\n",
    "    This chunking method produces a dataframe with four columns:\n",
    "    - snippet: A snippet from the codebase\n",
    "    - trace: The stack trace of the snippet; the class and/or function from which\n",
    "      the snippet is taken\n",
    "    - lineno: The line number where the snippet starts. Note that it is 1-indexed,\n",
    "      which is consistent with the lineno returned by the AST module.\n",
    "    - end_lineno: The line number of the last line of the snippet. Like lineno, \n",
    "      it is 1-indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    start_linenos, end_linenos, traces = _split_by_function(\n",
    "        ast_body=ast_body,\n",
    "        start_lineno=1,\n",
    "        end_lineno=len(script_lines),\n",
    "    )\n",
    "    \n",
    "    # Only keep non-empty lines.\n",
    "\n",
    "    for i in range(len(start_linenos)):\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[start_linenos[i] - 1].strip():\n",
    "            start_linenos[i] += 1\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[end_linenos[i] - 1].strip():\n",
    "            end_linenos[i] -= 1\n",
    "    \n",
    "    snippets = []\n",
    "    final_traces = []\n",
    "    final_linenos = []\n",
    "    final_end_linenos = []\n",
    "\n",
    "    for lineno, end_lineno, snippet_trace in zip(start_linenos, end_linenos, traces):\n",
    "        if lineno <= end_lineno:\n",
    "            snippets.append(\"\".join(script_lines[lineno - 1: end_lineno]))\n",
    "            final_traces.append(snippet_trace)\n",
    "            final_linenos.append(lineno)\n",
    "            final_end_linenos.append(end_lineno)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"snippet\": snippets,\n",
    "        \"trace\": final_traces,\n",
    "        \"lineno\": final_linenos,\n",
    "        \"end_lineno\": final_end_linenos,\n",
    "    })\n",
    "\n",
    "def _split_by_function(ast_body: List[ast.AST], start_lineno: int, end_lineno: int):\n",
    "    \"\"\"This helper function allows us to reuse chunking logic within a scope\n",
    "    such as a class.\n",
    "    \"\"\"\n",
    "    start_linenos = []\n",
    "    end_linenos = []\n",
    "    traces = []\n",
    "    can_connect = False\n",
    "\n",
    "    # start_lineno is always the previous end_lineno + 1\n",
    "    # This is because comments are not captured by the AST parser.\n",
    "    # Thus, to keep comments, we must keep all lines between the previous\n",
    "    # element and the current element.\n",
    "\n",
    "    for elem in ast_body:\n",
    "        if isinstance(elem, ast.FunctionDef):\n",
    "            # A function block is always its own chunk\n",
    "            start_linenos.append(start_lineno)\n",
    "            end_linenos.append(elem.end_lineno)\n",
    "            # Add function name to trace.\n",
    "            traces.append(f\"function name: {elem.name}\")\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        elif isinstance(elem, ast.ClassDef):\n",
    "            # A class is treated as a mini-script;\n",
    "            # functions/methods inside a class are their own snippets.\n",
    "            class_start_linenos, class_end_linenos, class_trace = _split_by_function(\n",
    "                ast_body=elem.body,\n",
    "                start_lineno=start_lineno,\n",
    "                end_lineno=elem.end_lineno,\n",
    "            )\n",
    "            start_linenos.extend(class_start_linenos)\n",
    "            end_linenos.extend(class_end_linenos)\n",
    "            # Prepend class name to the trace of every snippet in the class.\n",
    "            traces.extend([f\"class name: {elem.name}. {trace}\" for trace in class_trace])\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        else:\n",
    "            # Group expressions in the global scope that are neither functions\n",
    "            # nor classes.\n",
    "            if can_connect:\n",
    "                end_linenos[-1] = elem.end_lineno\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "            else:\n",
    "                start_linenos.append(start_lineno)\n",
    "                end_linenos.append(elem.end_lineno)\n",
    "                # Append an empty string so `traces`` is always the same length\n",
    "                # as start_linenos and end_linenos\n",
    "                traces.append(\"\")\n",
    "                can_connect = True\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "    start_linenos.append(start_lineno)\n",
    "    end_linenos.append(end_lineno)\n",
    "\n",
    "    return start_linenos, end_linenos, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open ./langchain/libs/langchain/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_361461/476235626.py:33: RuntimeWarning: Found non-Python files in the codebase. This script only snippets python code.\n",
      "  warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "langchain_snippets = apply_to_codebase(\"./langchain\", split_by_function)\n",
    "langchain_snippets.to_csv(\"langchain.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build NeuralDB**\n",
    "As previously mentioned, NeuralDB has a notion of strong and weak columns.\n",
    "\n",
    "`strong_columns` are columns in your CSV file that contains “strong” signals; words or strings that you want exact matches with, such as keywords, brands, categories, or a stack trace.\n",
    "\n",
    "`weak_columns` contain “weak” signals; phrases or passages that you want rough or semantic matches with, such as product descriptions, chunks of an essay, or code snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gautam/.local/lib/python3.8/site-packages/thirdai/demos/beir_download_utils.py:9: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "WARNING: calling 'licensing.activate' on package built without license checks enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['36b2f067e847f4e87999080101509b3b69f8cde0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "\n",
    "# TODO: Your ThirdAI key goes here\n",
    "licensing.activate(\"YOUR-THIRDAI-KEY\")\n",
    "\n",
    "doc = ndb.CSV(\n",
    "    \"langchain.csv\",\n",
    "    # Path to file and stack trace are strong signals.\n",
    "    strong_columns=[\"path_to_file\", \"trace\"],\n",
    "    # Code snippets contain weak signals\n",
    "    weak_columns=[\"snippet\"],\n",
    "    reference_columns=[\"snippet\"],\n",
    ")\n",
    "\n",
    "db = ndb.NeuralDB()\n",
    "\n",
    "db.insert([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's test it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/langchain_community/retrievers/arxiv.py \n",
      "\n",
      "trace: class name: ArxivRetriever.  \n",
      "\n",
      "snippet: class ArxivRetriever(BaseRetriever, ArxivAPIWrapper):\n",
      "    \"\"\"`Arxiv` retriever.\n",
      "\n",
      "    It wraps load() to get_relevant_documents().\n",
      "    It uses all ArxivAPIWrapper arguments without any change.\n",
      "    \"\"\"\n",
      "\n",
      "    get_full_documents: bool = False\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/templates/retrieval-agent/retrieval_agent/chain.py \n",
      "\n",
      "trace: class name: ArxivRetriever. function name: _get_relevant_documents \n",
      "\n",
      "snippet:     def _get_relevant_documents(\n",
      "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        try:\n",
      "            if self.is_arxiv_identifier(query):\n",
      "                results = self.arxiv_search(\n",
      "                    id_list=query.split(),\n",
      "                    max_results=self.top_k_results,\n",
      "                ).results()\n",
      "            else:\n",
      "                results = self.arxiv_search(  # type: ignore\n",
      "                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.top_k_results\n",
      "                ).results()\n",
      "        except self.arxiv_exceptions as ex:\n",
      "            return [Document(page_content=f\"Arxiv exception: {ex}\")]\n",
      "        docs = [\n",
      "            Document(\n",
      "                page_content=result.summary,\n",
      "                metadata={\n",
      "                    \"Published\": result.updated.date(),\n",
      "                    \"Title\": result.title,\n",
      "                    \"Authors\": \", \".join(a.name for a in result.authors),\n",
      "                },\n",
      "            )\n",
      "            for result in results\n",
      "        ]\n",
      "        return docs\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"get relevant documents in arxiv retriever\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/experimental/langchain_experimental/retrievers/__init__.py \n",
      "\n",
      "trace:  \n",
      "\n",
      "snippet: \"\"\"**Retriever** class returns Documents given a text **query**.\n",
      "\n",
      "It is more general than a vector store. A retriever does not need to be able to\n",
      "store documents, only to return (or retrieve) it.\n",
      "\"\"\"\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/langchain_community/vectorstores/neo4j_vector.py \n",
      "\n",
      "trace: class name: Neo4jVector. function name: from_documents \n",
      "\n",
      "snippet:     @classmethod\n",
      "    def from_existing_index(\n",
      "        cls: Type[Neo4jVector],\n",
      "        embedding: Embeddings,\n",
      "        index_name: str,\n",
      "        search_type: SearchType = DEFAULT_SEARCH_TYPE,\n",
      "        keyword_index_name: Optional[str] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Neo4jVector:\n",
      "        \"\"\"\n",
      "        Get instance of an existing Neo4j vector index. This method will\n",
      "        return the instance of the store without inserting any new\n",
      "        embeddings.\n",
      "        Neo4j credentials are required in the form of `url`, `username`,\n",
      "        and `password` and optional `database` parameters along with\n",
      "        the `index_name` definition.\n",
      "        \"\"\"\n",
      "\n",
      "        if search_type == SearchType.HYBRID and not keyword_index_name:\n",
      "            raise ValueError(\n",
      "                \"keyword_index name has to be specified \"\n",
      "                \"when using hybrid search option\"\n",
      "            )\n",
      "\n",
      "        store = cls(\n",
      "            embedding=embedding,\n",
      "            index_name=index_name,\n",
      "            keyword_index_name=keyword_index_name,\n",
      "            search_type=search_type,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        embedding_dimension, index_type = store.retrieve_existing_index()\n",
      "\n",
      "        # Raise error if relationship index type\n",
      "        if index_type == \"RELATIONSHIP\":\n",
      "            raise ValueError(\n",
      "                \"Relationship vector index is not supported with \"\n",
      "                \"`from_existing_index` method. Please use the \"\n",
      "                \"`from_existing_relationship_index` method.\"\n",
      "            )\n",
      "\n",
      "        if not embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The specified vector index name does not exist. \"\n",
      "                \"Make sure to check if you spelled it correctly\"\n",
      "            )\n",
      "\n",
      "        # Check if embedding function and vector index dimensions match\n",
      "        if not store.embedding_dimension == embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The provided embedding function and vector index \"\n",
      "                \"dimensions do not match.\\n\"\n",
      "                f\"Embedding function dimension: {store.embedding_dimension}\\n\"\n",
      "                f\"Vector index dimension: {embedding_dimension}\"\n",
      "            )\n",
      "\n",
      "        if search_type == SearchType.HYBRID:\n",
      "            fts_node_label = store.retrieve_existing_fts_index()\n",
      "            # If the FTS index doesn't exist yet\n",
      "            if not fts_node_label:\n",
      "                raise ValueError(\n",
      "                    \"The specified keyword index name does not exist. \"\n",
      "                    \"Make sure to check if you spelled it correctly\"\n",
      "                )\n",
      "            else:  # Validate that FTS and Vector index use the same information\n",
      "                if not fts_node_label == store.node_label:\n",
      "                    raise ValueError(\n",
      "                        \"Vector and keyword index don't index the same node label\"\n",
      "                    )\n",
      "\n",
      "        return store\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"base class for retriever that does not use vector store\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts_with_bm25_params \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts_with_bm25_params() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(\n",
      "        texts=input_texts, bm25_params={\"epsilon\": 10}\n",
      "    )\n",
      "    # should count only multiple words (have, pan)\n",
      "    assert bm25_retriever.vectorizer.epsilon == 10\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\n",
      "    assert len(bm25_retriever.docs) == 3\n",
      "    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"bm25 retriever test\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain.ndb'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.save(\"langchain.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A full copilot system with _Chain of Thought_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a copilot system that is powerful enough to answer a question like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I want to integrate a retriever called MyRetriever with this open source codebase. \"\n",
    "    \"It does not use a vector store. \"\n",
    "    \"Create a skeleton for a class that wraps MyRetriever and inherits the right interface. \"\n",
    "    \"(Don't implement the methods, just write #TODOs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model query script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Your OpenAI key goes here\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR-OPENAI-KEY\"\n",
    "openai_client = OpenAI() # defaults to os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def query_gpt(query=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and retreive the relevant code snippet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None, print_metadata=False):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        if (print_metadata):\n",
    "            print(result.metadata)\n",
    "        if radius:\n",
    "            references.append(f\"```{result.context(radius=radius)}```\")\n",
    "        else:\n",
    "            references.append(f\"```{result.text}```\")\n",
    "    return references\n",
    "\n",
    "def get_context(query, radius=None, print_metadata=False):\n",
    "    references = get_references(str(query), radius=radius, print_metadata=print_metadata)\n",
    "    context = \"\\n\\n\".join(references[:5])\n",
    "    return context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial thoughts\n",
    "Action items required to accomplish the above given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes that wrap external retrievers in the codebase.\n",
      "3. Examples of how other retrievers are integrated into the codebase.\n",
      "4. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "5. Any relevant configuration settings or parameters that need to be set for retrievers in the codebase.\n"
     ]
    }
   ],
   "source": [
    "def initial_thoughts(task):\n",
    "    prompt = (\n",
    "        \"Act as a software engineer who is the expert in an unnamed open source codebase. \"\n",
    "        f\"You are asked to do the following:\\n\\n{task}\\n\\n\"\n",
    "        \"You have access to an oracle that can give you snippets and examples from \"\n",
    "        \"this open source codebase, and only from this open source codebase. \"\n",
    "        \"What pieces of information would you want to get from the oracle to complete the task? \"\n",
    "        \"List them in separate lines.\"\n",
    "    )\n",
    "    # Only return non-empty lines.\n",
    "    return [query for query in query_gpt(prompt).split(\"\\n\") if query]\n",
    "\n",
    "for query in initial_thoughts(task):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_thoughts(task, context, previous_answer=\"\"):\n",
    "    prompt = task\n",
    "    prompt += (\n",
    "        f\"Act as an experienced software engineer:\\n\\n\"\n",
    "        f\"Answer the query ```{task}``` , given your previous answers : ```{previous_answer}```\\n\\n\"\n",
    "        f\"modify your answer based on this new information (do not construct \"\n",
    "        f\"your answer from outside the context provided ): ```{context}```\"\n",
    "    )\n",
    "    response = query_gpt(prompt)\n",
    "    return response\n",
    "\n",
    "def copilot(task, radius=None, verbose=False):\n",
    "    queries = initial_thoughts(task)\n",
    "    if verbose:\n",
    "        print(len(queries), \"queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    draft_answer = \"\"\n",
    "\n",
    "    for query in queries:\n",
    "        if verbose:\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Retrieved references:\")\n",
    "        retrieved_info = get_context(query, radius=radius, print_metadata=verbose) # retrieve neural db response for current thought\n",
    "        # LLM modifies answer based on the previous answer and current ndb results\n",
    "        draft_answer = refine_thoughts(\n",
    "            task,\n",
    "            context=f\"Answers to the query '{query}':\\n\\n{retrieved_info}\",\n",
    "            previous_answer=draft_answer,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"Draft Answer:\")\n",
    "            print(draft_answer)\n",
    "            print(\"=\" * 100)\n",
    "    return draft_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get this task done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 queries:\n",
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes or wrappers that are used for integrating external retrievers.\n",
      "3. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "4. Examples of how other retrievers are integrated into the codebase for reference.\n",
      "5. Any guidelines or best practices for implementing new retriever classes in the codebase.\n",
      "\n",
      "\n",
      "Query: 1. The interface that the open source codebase uses for retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': 'import glob\\nimport os\\nimport re\\nimport shutil\\nimport sys\\nfrom pathlib import Path\\n\\nif __name__ == \"__main__\":\\n    intermediate_dir = Path(sys.argv[1])\\n\\n    templates_source_dir = Path(os.path.abspath(__file__)).parents[2] / \"templates\"\\n    templates_intermediate_dir = intermediate_dir / \"templates\"\\n\\n    readmes = list(glob.glob(str(templates_source_dir) + \"/*/README.md\"))\\n    destinations = [\\n        readme[len(str(templates_source_dir)) + 1 : -10] + \".md\" for readme in readmes\\n    ]\\n    for source, destination in zip(readmes, destinations):\\n        full_destination = templates_intermediate_dir / destination\\n        shutil.copyfile(source, full_destination)\\n        with open(full_destination, \"r\") as f:\\n            content = f.read()\\n        # remove images\\n        content = re.sub(r\"\\\\!\\\\[.*?\\\\]\\\\((.*?)\\\\)\", \"\", content)\\n        with open(full_destination, \"w\") as f:\\n            f.write(content)\\n\\n    sidebar_hidden = \"\"\"---\\nsidebar_class_name: hidden\\n---\\n\\n\"\"\"\\n\\n    # handle index file\\n    templates_index_source = templates_source_dir / \"docs\" / \"INDEX.md\"\\n    templates_index_intermediate = templates_intermediate_dir / \"index.md\"\\n\\n    with open(templates_index_source, \"r\") as f:\\n        content = f.read()\\n\\n    # replace relative links\\n    content = re.sub(r\"\\\\]\\\\(\\\\.\\\\.\\\\/\", \"](/docs/templates/\", content)\\n\\n    with open(templates_index_intermediate, \"w\") as f:\\n        f.write(sidebar_hidden + content)\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 45.0, 'path_to_file': './langchain/docs/scripts/copy_templates.py', 'id': 25231, 'thirdai_index': 25231, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: __init__', 'lineno': 177.0, 'end_lineno': 181.0, 'path_to_file': './langchain/libs/community/langchain_community/llms/sparkllm.py', 'id': 19547, 'thirdai_index': 19547, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: _create_url', 'lineno': 285.0, 'end_lineno': 289.0, 'path_to_file': './langchain/libs/community/langchain_community/chat_models/sparkllm.py', 'id': 18478, 'thirdai_index': 18478, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Import MyRetriever\n",
      "\n",
      "class MyRetrieverWrapper(MyRetriever):\n",
      "    \"\"\"\n",
      "    A class that wraps MyRetriever and inherits the interface used by the open source codebase.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to retrieve results from MyRetriever\n",
      "\n",
      "    def index(self, data):\n",
      "        # TODO: Implement index method to add data to MyRetriever\n",
      "\n",
      "    def delete(self, id):\n",
      "        # TODO: Implement delete method to remove data from MyRetriever\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 2. Any existing classes or wrappers that are used for integrating external retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': '    @property\\n    def output_keys(self) -> List[str]:\\n        \"\"\"\\n        Returns a list of output keys.\\n\\n        This method defines the output keys that will be used to access the output\\n        values produced by the chain or function. It ensures that the specified keys\\n        are available to access the outputs.\\n\\n        Returns:\\n            List[str]: A list of output keys.\\n\\n        Note:\\n            This method is considered private and may not be intended for direct\\n            external use.\\n\\n        \"\"\"\\n        return [self.output_key]\\n', 'trace': 'class name: AmazonComprehendModerationChain. function name: output_keys', 'lineno': 115.0, 'end_lineno': 132.0, 'path_to_file': './langchain/libs/experimental/langchain_experimental/comprehend_moderation/amazon_comprehend_moderation.py', 'id': 5330, 'thirdai_index': 5330, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"All integration tests (tests that call out to an external API).\"\"\"\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 1.0, 'path_to_file': './langchain/libs/langchain/tests/integration_tests/__init__.py', 'id': 6533, 'thirdai_index': 6533, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def delete(\\n        self, ids: Optional[List[str]] = None, timestamp: int = 0, **kwargs: Any\\n    ) -> Optional[bool]:\\n        \"\"\"Delete by vector ID or other criteria.\\n\\n        Args:\\n            ids: List of ids to delete.\\n            timestamp: Optional timestamp to delete with.\\n            **kwargs: Other keyword arguments that subclasses might use.\\n\\n        Returns:\\n            Optional[bool]: True if deletion is successful,\\n            False otherwise, None if not implemented.\\n        \"\"\"\\n\\n        external_ids = np.array(ids).astype(np.uint64)\\n        self.vector_index.delete_batch(\\n            external_ids=external_ids, timestamp=timestamp if timestamp != 0 else None\\n        )\\n        return True\\n', 'trace': 'class name: TileDB. function name: delete', 'lineno': 613.0, 'end_lineno': 632.0, 'path_to_file': './langchain/libs/community/langchain_community/vectorstores/tiledb.py', 'id': 16704, 'thirdai_index': 16704, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Import MyRetriever\n",
      "\n",
      "class MyRetrieverWrapper(MyRetriever):\n",
      "    \"\"\"\n",
      "    A class that wraps MyRetriever and inherits the interface used by the open source codebase.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to retrieve results from MyRetriever\n",
      "\n",
      "    def index(self, data):\n",
      "        # TODO: Implement index method to add data to MyRetriever\n",
      "\n",
      "    def delete(self, id):\n",
      "        # TODO: Implement delete method to remove data from MyRetriever\n",
      "\n",
      "    @property\n",
      "    def output_keys(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of output keys.\n",
      "\n",
      "        This method defines the output keys that will be used to access the output\n",
      "        values produced by the chain or function. It ensures that the specified keys\n",
      "        are available to access the outputs.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of output keys.\n",
      "\n",
      "        Note:\n",
      "            This method is considered private and may not be intended for direct\n",
      "            external use.\n",
      "\n",
      "        \"\"\"\n",
      "        return [self.output_key]\n",
      "``````\n",
      "====================================================================================================\n",
      "Query: 3. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': '\"\"\"Test Xinference wrapper.\"\"\"\\n\\nimport time\\nfrom typing import AsyncGenerator, Tuple\\n\\nimport pytest_asyncio\\n\\nfrom langchain_community.llms import Xinference\\n\\n\\n@pytest_asyncio.fixture\\nasync def setup() -> AsyncGenerator[Tuple[str, str], None]:\\n    import xoscar as xo\\n    from xinference.deploy.supervisor import start_supervisor_components\\n    from xinference.deploy.utils import create_worker_actor_pool\\n    from xinference.deploy.worker import start_worker_components\\n\\n    pool = await create_worker_actor_pool(\\n        f\"test://127.0.0.1:{xo.utils.get_next_port()}\"\\n    )\\n    print(f\"Pool running on localhost:{pool.external_address}\")  # noqa: T201\\n\\n    endpoint = await start_supervisor_components(\\n        pool.external_address, \"127.0.0.1\", xo.utils.get_next_port()\\n    )\\n    await start_worker_components(\\n        address=pool.external_address, supervisor_address=pool.external_address\\n    )\\n\\n    # wait for the api.\\n    time.sleep(3)\\n    async with pool:\\n        yield endpoint, pool.external_address\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 33.0, 'path_to_file': './langchain/libs/community/tests/integration_tests/llms/test_xinference.py', 'id': 14595, 'thirdai_index': 14595, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"Test Xinference embeddings.\"\"\"\\n\\nimport time\\nfrom typing import AsyncGenerator, Tuple\\n\\nimport pytest_asyncio\\n\\nfrom langchain_community.embeddings import XinferenceEmbeddings\\n\\n\\n@pytest_asyncio.fixture\\nasync def setup() -> AsyncGenerator[Tuple[str, str], None]:\\n    import xoscar as xo\\n    from xinference.deploy.supervisor import start_supervisor_components\\n    from xinference.deploy.utils import create_worker_actor_pool\\n    from xinference.deploy.worker import start_worker_components\\n\\n    pool = await create_worker_actor_pool(\\n        f\"test://127.0.0.1:{xo.utils.get_next_port()}\"\\n    )\\n    print(f\"Pool running on localhost:{pool.external_address}\")  # noqa: T201\\n\\n    endpoint = await start_supervisor_components(\\n        pool.external_address, \"127.0.0.1\", xo.utils.get_next_port()\\n    )\\n    await start_worker_components(\\n        address=pool.external_address, supervisor_address=pool.external_address\\n    )\\n\\n    # wait for the api.\\n    time.sleep(3)\\n    async with pool:\\n        yield endpoint, pool.external_address\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 33.0, 'path_to_file': './langchain/libs/community/tests/integration_tests/embeddings/test_xinference.py', 'id': 15672, 'thirdai_index': 15672, 'source': 'langchain.csv'}\n",
      "{'snippet': '@pytest.mark.requires(\"kay\")\\ndef test_kay_retriever() -> None:\\n    retriever = KayAiRetriever.create(\\n        dataset_id=\"company\",\\n        data_types=[\"10-K\", \"10-Q\", \"8-K\", \"PressRelease\"],\\n        num_contexts=3,\\n    )\\n    docs = retriever.invoke(\\n        \"What were the biggest strategy changes and partnerships made by Roku \"\\n        \"in 2023?\",\\n    )\\n    assert len(docs) == 3\\n    for doc in docs:\\n        assert isinstance(doc, Document)\\n        assert doc.page_content\\n        assert doc.metadata\\n        assert len(list(doc.metadata.items())) > 0\\n', 'trace': 'function name: test_kay_retriever', 'lineno': 8.0, 'end_lineno': 24.0, 'path_to_file': './langchain/libs/community/tests/integration_tests/retrievers/test_kay.py', 'id': 15110, 'thirdai_index': 15110, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Import MyRetriever\n",
      "\n",
      "class MyRetrieverWrapper(MyRetriever):\n",
      "    \"\"\"\n",
      "    A class that wraps MyRetriever and inherits the interface used by the open source codebase.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to retrieve results from MyRetriever\n",
      "\n",
      "    def index(self, data):\n",
      "        # TODO: Implement index method to add data to MyRetriever\n",
      "\n",
      "    def delete(self, id):\n",
      "        # TODO: Implement delete method to remove data from MyRetriever\n",
      "\n",
      "    @property\n",
      "    def output_keys(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of output keys.\n",
      "\n",
      "        This method defines the output keys that will be used to access the output\n",
      "        values produced by the chain or function. It ensures that the specified keys\n",
      "        are available to access the outputs.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of output keys.\n",
      "\n",
      "        Note:\n",
      "            This method is considered private and may not be intended for direct\n",
      "            external use.\n",
      "\n",
      "        \"\"\"\n",
      "        return [self.output_key]\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 4. Examples of how other retrievers are integrated into the codebase for reference.\n",
      "Retrieved references:\n",
      "{'snippet': 'async def test_chat_from_role_strings() -> None:\\n    \"\"\"Test instantiation of chat template from role strings.\"\"\"\\n    with pytest.warns(LangChainPendingDeprecationWarning):\\n        template = ChatPromptTemplate.from_role_strings(\\n            [\\n                (\"system\", \"You are a bot.\"),\\n                (\"assistant\", \"hello!\"),\\n                (\"human\", \"{question}\"),\\n                (\"other\", \"{quack}\"),\\n            ]\\n        )\\n\\n    expected = [\\n        ChatMessage(content=\"You are a bot.\", role=\"system\"),\\n        ChatMessage(content=\"hello!\", role=\"assistant\"),\\n        ChatMessage(content=\"How are you?\", role=\"human\"),\\n        ChatMessage(content=\"duck\", role=\"other\"),\\n    ]\\n\\n    messages = template.format_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n\\n    messages = await template.aformat_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n', 'trace': '', 'lineno': 302.0, 'end_lineno': 325.0, 'path_to_file': './langchain/libs/core/tests/unit_tests/prompts/test_chat.py', 'id': 2060, 'thirdai_index': 2060, 'source': 'langchain.csv'}\n",
      "{'snippet': '# flake8: noqa\\nfrom langchain_core.prompts.prompt import PromptTemplate\\n\\n_DEFAULT_ENTITY_EXTRACTION_TEMPLATE = \"\"\"You are an AI assistant reading the transcript of a conversation between an AI and a human. Extract all of the proper nouns from the last line of conversation. As a guideline, a proper noun is generally capitalized. You should definitely extract all names and places.\\n\\nThe conversation history is provided just in case of a coreference (e.g. \"What do you know about him\" where \"him\" is defined in a previous line) -- ignore items mentioned there that are not in the last line.\\n\\nReturn the output as a single comma-separated list, or NONE if there is nothing of note to return (e.g. the user is just issuing a greeting or having a simple conversation).\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff.\\nOutput: Langchain\\nEND OF EXAMPLE\\n\\nEXAMPLE\\nConversation history:\\nPerson #1: how\\'s it going today?\\nAI: \"It\\'s going great! How about you?\"\\nPerson #1: good! busy working on Langchain. lots to do.\\nAI: \"That sounds like a lot of work! What kind of things are you doing to make Langchain better?\"\\nLast line:\\nPerson #1: i\\'m trying to improve Langchain\\'s interfaces, the UX, its integrations with various products the user might want ... a lot of stuff. I\\'m working with Person #2.\\nOutput: Langchain, Person #2\\nEND OF EXAMPLE\\n\\nConversation history (for reference only):\\n{history}\\nLast line of conversation (for extraction):\\nHuman: {input}\\n\\nOutput:\"\"\"\\nENTITY_EXTRACTION_PROMPT = PromptTemplate(\\n    input_variables=[\"history\", \"input\"], template=_DEFAULT_ENTITY_EXTRACTION_TEMPLATE\\n)\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 40.0, 'path_to_file': './langchain/libs/langchain/langchain/indexes/prompts/entity_extraction.py', 'id': 6660, 'thirdai_index': 6660, 'source': 'langchain.csv'}\n",
      "{'snippet': '@deprecated(\\n    since=\"0.1.17\",\\n    alternative=(\\n        \"create_history_aware_retriever together with create_retrieval_chain \"\\n        \"(see example in docstring)\"\\n    ),\\n    removal=\"0.3.0\",\\n)\\nclass ConversationalRetrievalChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for having a conversation based on retrieved documents.\\n\\n    This class is deprecated. See below for an example implementation using\\n    `create_retrieval_chain`. Additional walkthroughs can be found at\\n    https://python.langchain.com/docs/use_cases/question_answering/chat_history\\n\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                create_history_aware_retriever,\\n                create_retrieval_chain,\\n            )\\n            from langchain.chains.combine_documents import create_stuff_documents_chain\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n            from langchain_openai import ChatOpenAI\\n\\n\\n            retriever = ...  # Your retriever\\n\\n            llm = ChatOpenAI()\\n\\n            # Contextualize question\\n            contextualize_q_system_prompt = (\\n                \"Given a chat history and the latest user question \"\\n                \"which might reference context in the chat history, \"\\n                \"formulate a standalone question which can be understood \"\\n                \"without the chat history. Do NOT answer the question, just \"\\n                \"reformulate it if needed and otherwise return it as is.\"\\n            )\\n            contextualize_q_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", contextualize_q_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            history_aware_retriever = create_history_aware_retriever(\\n                llm, retriever, contextualize_q_prompt\\n            )\\n\\n            # Answer question\\n            qa_system_prompt = (\\n                \"You are an assistant for question-answering tasks. Use \"\\n                \"the following pieces of retrieved context to answer the \"\\n                \"question. If you don\\'t know the answer, just say that you \"\\n                \"don\\'t know. Use three sentences maximum and keep the answer \"\\n                \"concise.\"\\n                \"\\\\n\\\\n\"\\n                \"{context}\"\\n            )\\n            qa_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", qa_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            # Below we use create_stuff_documents_chain to feed all retrieved context\\n            # into the LLM. Note that we can also use StuffDocumentsChain and other\\n            # instances of BaseCombineDocumentsChain.\\n            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\\n            rag_chain = create_retrieval_chain(\\n                history_aware_retriever, question_answer_chain\\n            )\\n\\n            # Usage:\\n            chat_history = []  # Collect chat history here (a sequence of messages)\\n            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\\n\\n    This chain takes in chat history (a list of messages) and new questions,\\n    and then returns an answer to that question.\\n    The algorithm for this chain consists of three parts:\\n\\n    1. Use the chat history and the new question to create a \"standalone question\".\\n    This is done so that this question can be passed into the retrieval step to fetch\\n    relevant documents. If only the new question was passed in, then relevant context\\n    may be lacking. If the whole conversation was passed into retrieval, there may\\n    be unnecessary information there that would distract from retrieval.\\n\\n    2. This new question is passed to the retriever and relevant documents are\\n    returned.\\n\\n    3. The retrieved documents are passed to an LLM along with either the new question\\n    (default behavior) or the original question and chat history to generate a final\\n    response.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            combine_docs_chain = StuffDocumentsChain(...)\\n            vectorstore = ...\\n            retriever = vectorstore.as_retriever()\\n\\n            # This controls how the standalone question is generated.\\n            # Should take `chat_history` and `question` as input variables.\\n            template = (\\n                \"Combine the chat history and follow up question into \"\\n                \"a standalone question. Chat History: {chat_history}\"\\n                \"Follow up question: {question}\"\\n            )\\n            prompt = PromptTemplate.from_template(template)\\n            llm = OpenAI()\\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = ConversationalRetrievalChain(\\n                combine_docs_chain=combine_docs_chain,\\n                retriever=retriever,\\n                question_generator=question_generator_chain,\\n            )\\n    \"\"\"\\n\\n    retriever: BaseRetriever\\n    \"\"\"Retriever to use to fetch documents.\"\"\"\\n    max_tokens_limit: Optional[int] = None\\n    \"\"\"If set, enforces that the documents returned are less than this limit.\\n    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\\n', 'trace': 'class name: ConversationalRetrievalChain. ', 'lineno': 237.0, 'end_lineno': 366.0, 'path_to_file': './langchain/libs/langchain/langchain/chains/conversational_retrieval/base.py', 'id': 7903, 'thirdai_index': 7903, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Import MyRetriever\n",
      "\n",
      "class MyRetrieverWrapper(MyRetriever):\n",
      "    \"\"\"\n",
      "    A class that wraps MyRetriever and inherits the interface used by the open source codebase.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to retrieve results from MyRetriever\n",
      "\n",
      "    def index(self, data):\n",
      "        # TODO: Implement index method to add data to MyRetriever\n",
      "\n",
      "    def delete(self, id):\n",
      "        # TODO: Implement delete method to remove data from MyRetriever\n",
      "\n",
      "    @property\n",
      "    def output_keys(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of output keys.\n",
      "\n",
      "        This method defines the output keys that will be used to access the output\n",
      "        values produced by the chain or function. It ensures that the specified keys\n",
      "        are available to access the outputs.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of output keys.\n",
      "\n",
      "        Note:\n",
      "            This method is considered private and may not be intended for direct\n",
      "            external use.\n",
      "\n",
      "        \"\"\"\n",
      "        return [self.output_key]\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 5. Any guidelines or best practices for implementing new retriever classes in the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': 'class BaseChatMessageHistory(ABC):\\n    \"\"\"Abstract base class for storing chat message history.\\n\\n    Implementations guidelines:\\n\\n    Implementations are expected to over-ride all or some of the following methods:\\n\\n    * add_messages: sync variant for bulk addition of messages\\n    * aadd_messages: async variant for bulk addition of messages\\n    * messages: sync variant for getting messages\\n    * aget_messages: async variant for getting messages\\n    * clear: sync variant for clearing messages\\n    * aclear: async variant for clearing messages\\n\\n    add_messages contains a default implementation that calls add_message\\n    for each message in the sequence. This is provided for backwards compatibility\\n    with existing implementations which only had add_message.\\n\\n    Async variants all have default implementations that call the sync variants.\\n    Implementers can choose to over-ride the async implementations to provide\\n    truly async implementations.\\n\\n    Usage guidelines:\\n\\n    When used for updating history, users should favor usage of `add_messages`\\n    over `add_message` or other variants like `add_user_message` and `add_ai_message`\\n    to avoid unnecessary round-trips to the underlying persistence layer.\\n\\n    Example: Shows a default implementation.\\n\\n        .. code-block:: python\\n\\n            class FileChatMessageHistory(BaseChatMessageHistory):\\n                storage_path:  str\\n                session_id: str\\n\\n               @property\\n               def messages(self):\\n                   with open(os.path.join(storage_path, session_id), \\'r:utf-8\\') as f:\\n                       messages = json.loads(f.read())\\n                    return messages_from_dict(messages)\\n\\n               def add_messages(self, messages: Sequence[BaseMessage]) -> None:\\n                   all_messages = list(self.messages) # Existing messages\\n                   all_messages.extend(messages) # Add new messages\\n\\n                   serialized = [message_to_dict(message) for message in all_messages]\\n                   # Can be further optimized by only writing new messages\\n                   # using append mode.\\n                   with open(os.path.join(storage_path, session_id), \\'w\\') as f:\\n                       json.dump(f, messages)\\n\\n               def clear(self):\\n                   with open(os.path.join(storage_path, session_id), \\'w\\') as f:\\n                       f.write(\"[]\")\\n    \"\"\"\\n\\n    messages: List[BaseMessage]\\n    \"\"\"A property or attribute that returns a list of messages.\\n\\n    In general, getting the messages may involve IO to the underlying\\n    persistence layer, so this operation is expected to incur some\\n    latency.\\n    \"\"\"\\n\\n    async def aget_messages(self) -> List[BaseMessage]:\\n        \"\"\"Async version of getting messages.\\n\\n        Can over-ride this method to provide an efficient async implementation.\\n\\n        In general, fetching messages may involve IO to the underlying\\n        persistence layer.\\n        \"\"\"\\n        return await run_in_executor(None, lambda: self.messages)\\n', 'trace': 'class name: BaseChatMessageHistory. ', 'lineno': 32.0, 'end_lineno': 105.0, 'path_to_file': './langchain/libs/core/langchain_core/chat_history.py', 'id': 2712, 'thirdai_index': 2712, 'source': 'langchain.csv'}\n",
      "{'snippet': '    @classmethod\\n    def from_default(cls, objective: str, **kwargs: Any) -> NatBotChain:\\n        \"\"\"Load with default LLMChain.\"\"\"\\n        raise NotImplementedError(\\n            \"This method is no longer implemented. Please use from_llm.\"\\n            \"llm = OpenAI(temperature=0.5, best_of=10, n=3, max_tokens=50)\"\\n            \"For example, NatBotChain.from_llm(llm, objective)\"\\n        )\\n', 'trace': 'class name: NatBotChain. function name: from_llm', 'lineno': 67.0, 'end_lineno': 74.0, 'path_to_file': './langchain/libs/langchain/langchain/chains/natbot/base.py', 'id': 7954, 'thirdai_index': 7954, 'source': 'langchain.csv'}\n",
      "{'snippet': 'def __getattr__(name: str = \"\") -> Any:\\n    raise AttributeError(\\n        \"This tool has been moved to langchain experiment. \"\\n        \"This tool has access to a python REPL. \"\\n        \"For best practices make sure to sandbox this tool. \"\\n        \"Read https://github.com/langchain-ai/langchain/blob/master/SECURITY.md \"\\n        \"To keep using this code as is, install langchain experimental and \"\\n        \"update relevant imports replacing \\'langchain\\' with \\'langchain_experimental\\'\"\\n    )\\n', 'trace': 'function name: __getattr__', 'lineno': 4.0, 'end_lineno': 12.0, 'path_to_file': './langchain/libs/langchain/langchain/tools/python/__init__.py', 'id': 8445, 'thirdai_index': 8445, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Import MyRetriever\n",
      "\n",
      "class MyRetrieverWrapper(MyRetriever):\n",
      "    \"\"\"\n",
      "    A class that wraps MyRetriever and inherits the interface used by the open source codebase.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        # TODO: Initialize MyRetriever\n",
      "\n",
      "    def search(self, query):\n",
      "        # TODO: Implement search method to retrieve results from MyRetriever\n",
      "\n",
      "    def index(self, data):\n",
      "        # TODO: Implement index method to add data to MyRetriever\n",
      "\n",
      "    def delete(self, id):\n",
      "        # TODO: Implement delete method to remove data from MyRetriever\n",
      "\n",
      "    @property\n",
      "    def output_keys(self) -> List[str]:\n",
      "        \"\"\"\n",
      "        Returns a list of output keys.\n",
      "\n",
      "        This method defines the output keys that will be used to access the output\n",
      "        values produced by the chain or function. It ensures that the specified keys\n",
      "        are available to access the outputs.\n",
      "\n",
      "        Returns:\n",
      "            List[str]: A list of output keys.\n",
      "\n",
      "        Note:\n",
      "            This method is considered private and may not be intended for direct\n",
      "            external use.\n",
      "\n",
      "        \"\"\"\n",
      "        return [self.output_key]\n",
      "\n",
      "    # TODO: Implement the necessary methods to integrate MyRetriever with the open source codebase\n",
      "```\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "answer = copilot(task=task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
