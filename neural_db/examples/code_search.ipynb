{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a Code Search System**\n",
    "We will build a code search system based on the Langchain codebase. Our system will be able to answer queries such as:\n",
    "* get relevant documents in arxiv retriever\n",
    "* base class for retriever that does not use vector store\n",
    "* bm25 retriever test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clone the Langchain Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'langchain'...\n",
      "remote: Enumerating objects: 171026, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 171026 (delta 0), reused 0 (delta 0), pack-reused 171025\u001b[K\n",
      "Receiving objects: 100% (171026/171026), 231.79 MiB | 34.11 MiB/s, done.\n",
      "Resolving deltas: 100% (127823/127823), done.\n",
      "Updating files: 100% (7421/7421), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/langchain-ai/langchain.git\n",
    "\n",
    "# And install the thirdai package\n",
    "%pip install thirdai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "To ensure that each chunk is semantically coherent, we will split it along class and function boundaries. In addition, for our use case, it's important to know which file, class, and/or function a snippet is taken from. This kind of information is perfect for utilizing NeuralDB's notion of strong columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "def apply_to_codebase(path_to_codebase, chunking_strategy):\n",
    "    \"\"\"Traverses entire codebase and applies chunking strategy to all files.\n",
    "    Returns a dataframe with 5 columns: id, chunk, path_to_file, lineno, end_lineno\n",
    "    `lineno` is the line number (not line index) that the chunk starts on.\n",
    "    `end_lineno` is the chunk's last line number (again, not line index).\n",
    "    For example, \n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    warn = False\n",
    "    for path_to_file in glob.iglob(f\"{path_to_codebase}/**/*.*\", recursive = True):\n",
    "        if path_to_file.endswith(\".py\"):\n",
    "            try:\n",
    "                script = open(path_to_file).read()\n",
    "                ast_body = ast.parse(script).body\n",
    "                script_lines = script.splitlines(keepends=True)\n",
    "                df = chunking_strategy(ast_body, script_lines)\n",
    "                df[\"path_to_file\"] = [path_to_file for _ in range(len(df))]\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to open\", path_to_file)\n",
    "                print(\"Reason:\", e)\n",
    "                print(\"Skipping...\")\n",
    "        else:\n",
    "            warn = True\n",
    "        \n",
    "    if warn:\n",
    "        warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    df[\"id\"] = range(len(df))\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def split_by_function(ast_body: List[ast.AST], script_lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    ast_body: List of elements in the Python script as returned by `ast.parse(script).body`\n",
    "    script_lines: List of lines in the Python script\n",
    "\n",
    "    The script will be split into snippets according to these rules:\n",
    "    - Each function is a chunk\n",
    "    - Each method of a class is a chunk\n",
    "    - Expressions between functions or classes are clubbed together.\n",
    "    - Comments (not docstrings) are clubbed with the next chunk\n",
    "\n",
    "    This chunking method produces a dataframe with four columns:\n",
    "    - snippet: A snippet from the codebase\n",
    "    - trace: The stack trace of the snippet; the class and/or function from which\n",
    "      the snippet is taken\n",
    "    - lineno: The line number where the snippet starts. Note that it is 1-indexed,\n",
    "      which is consistent with the lineno returned by the AST module.\n",
    "    - end_lineno: The line number of the last line of the snippet. Like lineno, \n",
    "      it is 1-indexed.\n",
    "    \"\"\"\n",
    "\n",
    "    start_linenos, end_linenos, traces = _split_by_function(\n",
    "        ast_body=ast_body,\n",
    "        start_lineno=1,\n",
    "        end_lineno=len(script_lines),\n",
    "    )\n",
    "    \n",
    "    # Only keep non-empty lines.\n",
    "\n",
    "    for i in range(len(start_linenos)):\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[start_linenos[i] - 1].strip():\n",
    "            start_linenos[i] += 1\n",
    "        while start_linenos[i] <= end_linenos[i] and not script_lines[end_linenos[i] - 1].strip():\n",
    "            end_linenos[i] -= 1\n",
    "    \n",
    "    snippets = []\n",
    "    final_traces = []\n",
    "    final_linenos = []\n",
    "    final_end_linenos = []\n",
    "\n",
    "    for lineno, end_lineno, snippet_trace in zip(start_linenos, end_linenos, traces):\n",
    "        if lineno <= end_lineno:\n",
    "            snippets.append(\"\".join(script_lines[lineno - 1: end_lineno]))\n",
    "            final_traces.append(snippet_trace)\n",
    "            final_linenos.append(lineno)\n",
    "            final_end_linenos.append(end_lineno)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"snippet\": snippets,\n",
    "        \"trace\": final_traces,\n",
    "        \"lineno\": final_linenos,\n",
    "        \"end_lineno\": final_end_linenos,\n",
    "    })\n",
    "\n",
    "def _split_by_function(ast_body: List[ast.AST], start_lineno: int, end_lineno: int):\n",
    "    \"\"\"This helper function allows us to reuse chunking logic within a scope\n",
    "    such as a class.\n",
    "    \"\"\"\n",
    "    start_linenos = []\n",
    "    end_linenos = []\n",
    "    traces = []\n",
    "    can_connect = False\n",
    "\n",
    "    # start_lineno is always the previous end_lineno + 1\n",
    "    # This is because comments are not captured by the AST parser.\n",
    "    # Thus, to keep comments, we must keep all lines between the previous\n",
    "    # element and the current element.\n",
    "\n",
    "    for elem in ast_body:\n",
    "        if isinstance(elem, ast.FunctionDef):\n",
    "            # A function block is always its own chunk\n",
    "            start_linenos.append(start_lineno)\n",
    "            end_linenos.append(elem.end_lineno)\n",
    "            # Add function name to trace.\n",
    "            traces.append(f\"function name: {elem.name}\")\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        elif isinstance(elem, ast.ClassDef):\n",
    "            # A class is treated as a mini-script;\n",
    "            # functions/methods inside a class are their own snippets.\n",
    "            class_start_linenos, class_end_linenos, class_trace = _split_by_function(\n",
    "                ast_body=elem.body,\n",
    "                start_lineno=start_lineno,\n",
    "                end_lineno=elem.end_lineno,\n",
    "            )\n",
    "            start_linenos.extend(class_start_linenos)\n",
    "            end_linenos.extend(class_end_linenos)\n",
    "            # Prepend class name to the trace of every snippet in the class.\n",
    "            traces.extend([f\"class name: {elem.name}. {trace}\" for trace in class_trace])\n",
    "            can_connect = False\n",
    "            start_lineno = elem.end_lineno + 1\n",
    "        else:\n",
    "            # Group expressions in the global scope that are neither functions\n",
    "            # nor classes.\n",
    "            if can_connect:\n",
    "                end_linenos[-1] = elem.end_lineno\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "            else:\n",
    "                start_linenos.append(start_lineno)\n",
    "                end_linenos.append(elem.end_lineno)\n",
    "                # Append an empty string so `traces`` is always the same length\n",
    "                # as start_linenos and end_linenos\n",
    "                traces.append(\"\")\n",
    "                can_connect = True\n",
    "                start_lineno = elem.end_lineno + 1\n",
    "    start_linenos.append(start_lineno)\n",
    "    end_linenos.append(end_lineno)\n",
    "\n",
    "    return start_linenos, end_linenos, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to open ./langchain/libs/langchain/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n",
      "Failed to open ./langchain/libs/community/tests/integration_tests/examples/non-utf8-encoding.py\n",
      "Reason: 'utf-8' codec can't decode byte 0xb1 in position 23: invalid start byte\n",
      "Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360838/476235626.py:33: RuntimeWarning: Found non-Python files in the codebase. This script only snippets python code.\n",
      "  warnings.warn(\"Found non-Python files in the codebase. This script only snippets python code.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "langchain_snippets = apply_to_codebase(\"./langchain\", split_by_function)\n",
    "langchain_snippets.to_csv(\"langchain.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build NeuralDB**\n",
    "As previously mentioned, NeuralDB has a notion of strong and weak columns.\n",
    "\n",
    "`strong_columns` are columns in your CSV file that contains “strong” signals; words or strings that you want exact matches with, such as keywords, brands, categories, or a stack trace.\n",
    "\n",
    "`weak_columns` contain “weak” signals; phrases or passages that you want rough or semantic matches with, such as product descriptions, chunks of an essay, or code snippets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['36b2f067e847f4e87999080101509b3b69f8cde0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "\n",
    "# TODO: Your ThirdAI key goes here\n",
    "licensing.activate(\"YOUR-THIRDAI-KEY\")\n",
    "\n",
    "doc = ndb.CSV(\n",
    "    \"langchain.csv\",\n",
    "    # Path to file and stack trace are strong signals.\n",
    "    strong_columns=[\"path_to_file\", \"trace\"],\n",
    "    # Code snippets contain weak signals\n",
    "    weak_columns=[\"snippet\"],\n",
    "    reference_columns=[\"snippet\"],\n",
    ")\n",
    "\n",
    "db = ndb.NeuralDB()\n",
    "\n",
    "db.insert([doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's test it!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/langchain_community/retrievers/arxiv.py \n",
      "\n",
      "trace: class name: ArxivRetriever.  \n",
      "\n",
      "snippet: class ArxivRetriever(BaseRetriever, ArxivAPIWrapper):\n",
      "    \"\"\"`Arxiv` retriever.\n",
      "\n",
      "    It wraps load() to get_relevant_documents().\n",
      "    It uses all ArxivAPIWrapper arguments without any change.\n",
      "    \"\"\"\n",
      "\n",
      "    get_full_documents: bool = False\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/templates/retrieval-agent/retrieval_agent/chain.py \n",
      "\n",
      "trace: class name: ArxivRetriever. function name: _get_relevant_documents \n",
      "\n",
      "snippet:     def _get_relevant_documents(\n",
      "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
      "    ) -> List[Document]:\n",
      "        try:\n",
      "            if self.is_arxiv_identifier(query):\n",
      "                results = self.arxiv_search(\n",
      "                    id_list=query.split(),\n",
      "                    max_results=self.top_k_results,\n",
      "                ).results()\n",
      "            else:\n",
      "                results = self.arxiv_search(  # type: ignore\n",
      "                    query[: self.ARXIV_MAX_QUERY_LENGTH], max_results=self.top_k_results\n",
      "                ).results()\n",
      "        except self.arxiv_exceptions as ex:\n",
      "            return [Document(page_content=f\"Arxiv exception: {ex}\")]\n",
      "        docs = [\n",
      "            Document(\n",
      "                page_content=result.summary,\n",
      "                metadata={\n",
      "                    \"Published\": result.updated.date(),\n",
      "                    \"Title\": result.title,\n",
      "                    \"Authors\": \", \".join(a.name for a in result.authors),\n",
      "                },\n",
      "            )\n",
      "            for result in results\n",
      "        ]\n",
      "        return docs\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"get relevant documents in arxiv retriever\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/experimental/langchain_experimental/retrievers/__init__.py \n",
      "\n",
      "trace:  \n",
      "\n",
      "snippet: \"\"\"**Retriever** class returns Documents given a text **query**.\n",
      "\n",
      "It is more general than a vector store. A retriever does not need to be able to\n",
      "store documents, only to return (or retrieve) it.\n",
      "\"\"\"\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/langchain_community/vectorstores/neo4j_vector.py \n",
      "\n",
      "trace: class name: Neo4jVector. function name: from_documents \n",
      "\n",
      "snippet:     @classmethod\n",
      "    def from_existing_index(\n",
      "        cls: Type[Neo4jVector],\n",
      "        embedding: Embeddings,\n",
      "        index_name: str,\n",
      "        search_type: SearchType = DEFAULT_SEARCH_TYPE,\n",
      "        keyword_index_name: Optional[str] = None,\n",
      "        **kwargs: Any,\n",
      "    ) -> Neo4jVector:\n",
      "        \"\"\"\n",
      "        Get instance of an existing Neo4j vector index. This method will\n",
      "        return the instance of the store without inserting any new\n",
      "        embeddings.\n",
      "        Neo4j credentials are required in the form of `url`, `username`,\n",
      "        and `password` and optional `database` parameters along with\n",
      "        the `index_name` definition.\n",
      "        \"\"\"\n",
      "\n",
      "        if search_type == SearchType.HYBRID and not keyword_index_name:\n",
      "            raise ValueError(\n",
      "                \"keyword_index name has to be specified \"\n",
      "                \"when using hybrid search option\"\n",
      "            )\n",
      "\n",
      "        store = cls(\n",
      "            embedding=embedding,\n",
      "            index_name=index_name,\n",
      "            keyword_index_name=keyword_index_name,\n",
      "            search_type=search_type,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        embedding_dimension, index_type = store.retrieve_existing_index()\n",
      "\n",
      "        # Raise error if relationship index type\n",
      "        if index_type == \"RELATIONSHIP\":\n",
      "            raise ValueError(\n",
      "                \"Relationship vector index is not supported with \"\n",
      "                \"`from_existing_index` method. Please use the \"\n",
      "                \"`from_existing_relationship_index` method.\"\n",
      "            )\n",
      "\n",
      "        if not embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The specified vector index name does not exist. \"\n",
      "                \"Make sure to check if you spelled it correctly\"\n",
      "            )\n",
      "\n",
      "        # Check if embedding function and vector index dimensions match\n",
      "        if not store.embedding_dimension == embedding_dimension:\n",
      "            raise ValueError(\n",
      "                \"The provided embedding function and vector index \"\n",
      "                \"dimensions do not match.\\n\"\n",
      "                f\"Embedding function dimension: {store.embedding_dimension}\\n\"\n",
      "                f\"Vector index dimension: {embedding_dimension}\"\n",
      "            )\n",
      "\n",
      "        if search_type == SearchType.HYBRID:\n",
      "            fts_node_label = store.retrieve_existing_fts_index()\n",
      "            # If the FTS index doesn't exist yet\n",
      "            if not fts_node_label:\n",
      "                raise ValueError(\n",
      "                    \"The specified keyword index name does not exist. \"\n",
      "                    \"Make sure to check if you spelled it correctly\"\n",
      "                )\n",
      "            else:  # Validate that FTS and Vector index use the same information\n",
      "                if not fts_node_label == store.node_label:\n",
      "                    raise ValueError(\n",
      "                        \"Vector and keyword index don't index the same node label\"\n",
      "                    )\n",
      "\n",
      "        return store\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"base class for retriever that does not use vector store\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts_with_bm25_params \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts_with_bm25_params() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(\n",
      "        texts=input_texts, bm25_params={\"epsilon\": 10}\n",
      "    )\n",
      "    # should count only multiple words (have, pan)\n",
      "    assert bm25_retriever.vectorizer.epsilon == 10\n",
      " \n",
      "\n",
      "====================================================================================================\n",
      "file: ./langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py \n",
      "\n",
      "trace: function name: test_from_texts \n",
      "\n",
      "snippet: @pytest.mark.requires(\"rank_bm25\")\n",
      "def test_from_texts() -> None:\n",
      "    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\n",
      "    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\n",
      "    assert len(bm25_retriever.docs) == 3\n",
      "    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\n",
      " \n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "for res in db.search(\"bm25 retriever test\", top_k=2):\n",
    "    print(\"file:\", res.metadata[\"path_to_file\"], \"\\n\")\n",
    "    print(\"trace:\", res.metadata[\"trace\"], \"\\n\")\n",
    "    print(\"snippet:\", res.metadata[\"snippet\"], \"\\n\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain.ndb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.save(\"langchain.ndb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A full copilot system with _Chain of Thought_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a copilot system that is powerful enough to answer a question like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = (\n",
    "    \"I want to integrate a retriever called MyRetriever with this open source codebase. \"\n",
    "    \"It does not use a vector store. \"\n",
    "    \"Create a skeleton for a class that wraps MyRetriever and inherits the right interface. \"\n",
    "    \"(Don't implement the methods, just write #TODOs)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model query script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# TODO: Your OpenAI key goes here\n",
    "os.environ['OPENAI_API_KEY'] = \"YOUR-OPENAI-KEY\"\n",
    "openai_client = OpenAI() # defaults to os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def query_gpt(query=\"\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"{query}\"}]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and retreive the relevant code snippet(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None, print_metadata=False):\n",
    "    search_results = db.search(query, top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        if (print_metadata):\n",
    "            print(result.metadata)\n",
    "        if radius:\n",
    "            references.append(f\"```{result.context(radius=radius)}```\")\n",
    "        else:\n",
    "            references.append(f\"```{result.text}```\")\n",
    "    return references\n",
    "\n",
    "def get_context(query, radius=None, print_metadata=False):\n",
    "    references = get_references(str(query), radius=radius, print_metadata=print_metadata)\n",
    "    context = \"\\n\\n\".join(references[:5])\n",
    "    return context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial thoughts\n",
    "Action items required to accomplish the above given task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes or modules that handle retrievers in the codebase.\n",
      "3. Any specific requirements or constraints for integrating external retrievers.\n",
      "4. Examples of how other retrievers are integrated within the codebase.\n",
      "5. Any relevant documentation or guidelines for extending the codebase with new retrievers.\n"
     ]
    }
   ],
   "source": [
    "def initial_thoughts(task):\n",
    "    prompt = (\n",
    "        \"Act as a software engineer who is the expert in an unnamed open source codebase. \"\n",
    "        f\"You are asked to do the following:\\n\\n{task}\\n\\n\"\n",
    "        \"You have access to an oracle that can give you snippets and examples from \"\n",
    "        \"this open source codebase, and only from this open source codebase. \"\n",
    "        \"What pieces of information would you want to get from the oracle to complete the task? \"\n",
    "        \"List them in separate lines.\"\n",
    "    )\n",
    "    # Only return non-empty lines.\n",
    "    return [query for query in query_gpt(prompt).split(\"\\n\") if query]\n",
    "\n",
    "for query in initial_thoughts(task):\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_thoughts(task, context, previous_answer=\"\"):\n",
    "    prompt = task\n",
    "    prompt += (\n",
    "        f\"Act as an experienced software engineer:\\n\\n\"\n",
    "        f\"Answer the query ```{task}``` , given your previous answers : ```{previous_answer}```\\n\\n\"\n",
    "        f\"modify your answer based on this new information (do not construct \"\n",
    "        f\"your answer from outside the context provided ): ```{context}```\"\n",
    "    )\n",
    "    response = query_gpt(prompt)\n",
    "    return response\n",
    "\n",
    "def copilot(task, radius=None, verbose=False):\n",
    "    queries = initial_thoughts(task)\n",
    "    if verbose:\n",
    "        print(len(queries), \"queries:\")\n",
    "        for query in queries:\n",
    "            print(query)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    draft_answer = \"\"\n",
    "\n",
    "    for query in queries:\n",
    "        if verbose:\n",
    "            print(\"Query:\", query)\n",
    "            print(\"Retrieved references:\")\n",
    "        retrieved_info = get_context(query, radius=radius, print_metadata=verbose) # retrieve neural db response for current thought\n",
    "        # LLM modifies answer based on the previous answer and current ndb results\n",
    "        draft_answer = refine_thoughts(\n",
    "            task,\n",
    "            context=f\"Answers to the query '{query}':\\n\\n{retrieved_info}\",\n",
    "            previous_answer=draft_answer,\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"Draft Answer:\")\n",
    "            print(draft_answer)\n",
    "            print(\"=\" * 100)\n",
    "    return draft_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get this task done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 queries:\n",
      "1. The interface that the open source codebase uses for retrievers.\n",
      "2. Any existing classes that wrap external retrievers in the codebase.\n",
      "3. Examples of how other retrievers are integrated into the codebase.\n",
      "4. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "5. Any relevant documentation or guidelines for integrating external retrievers.\n",
      "\n",
      "\n",
      "Query: 1. The interface that the open source codebase uses for retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': 'import glob\\nimport os\\nimport re\\nimport shutil\\nimport sys\\nfrom pathlib import Path\\n\\nif __name__ == \"__main__\":\\n    intermediate_dir = Path(sys.argv[1])\\n\\n    templates_source_dir = Path(os.path.abspath(__file__)).parents[2] / \"templates\"\\n    templates_intermediate_dir = intermediate_dir / \"templates\"\\n\\n    readmes = list(glob.glob(str(templates_source_dir) + \"/*/README.md\"))\\n    destinations = [\\n        readme[len(str(templates_source_dir)) + 1 : -10] + \".md\" for readme in readmes\\n    ]\\n    for source, destination in zip(readmes, destinations):\\n        full_destination = templates_intermediate_dir / destination\\n        shutil.copyfile(source, full_destination)\\n        with open(full_destination, \"r\") as f:\\n            content = f.read()\\n        # remove images\\n        content = re.sub(r\"\\\\!\\\\[.*?\\\\]\\\\((.*?)\\\\)\", \"\", content)\\n        with open(full_destination, \"w\") as f:\\n            f.write(content)\\n\\n    sidebar_hidden = \"\"\"---\\nsidebar_class_name: hidden\\n---\\n\\n\"\"\"\\n\\n    # handle index file\\n    templates_index_source = templates_source_dir / \"docs\" / \"INDEX.md\"\\n    templates_index_intermediate = templates_intermediate_dir / \"index.md\"\\n\\n    with open(templates_index_source, \"r\") as f:\\n        content = f.read()\\n\\n    # replace relative links\\n    content = re.sub(r\"\\\\]\\\\(\\\\.\\\\.\\\\/\", \"](/docs/templates/\", content)\\n\\n    with open(templates_index_intermediate, \"w\") as f:\\n        f.write(sidebar_hidden + content)\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 45.0, 'path_to_file': './langchain/docs/scripts/copy_templates.py', 'id': 25231, 'thirdai_index': 25231, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: __init__', 'lineno': 177.0, 'end_lineno': 181.0, 'path_to_file': './langchain/libs/community/langchain_community/llms/sparkllm.py', 'id': 19547, 'thirdai_index': 19547, 'source': 'langchain.csv'}\n",
      "{'snippet': 'class _SparkLLMClient:\\n    \"\"\"\\n    Use websocket-client to call the SparkLLM interface provided by Xfyun,\\n    which is the iFlyTek\\'s open platform for AI capabilities\\n    \"\"\"\\n', 'trace': 'class name: _SparkLLMClient. function name: _create_url', 'lineno': 285.0, 'end_lineno': 289.0, 'path_to_file': './langchain/libs/community/langchain_community/chat_models/sparkllm.py', 'id': 18478, 'thirdai_index': 18478, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the interface used by the open source codebase\n",
      "\n",
      "class MyRetrieverWrapper:\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    # TODO: Implement the method to retrieve data using MyRetriever\n",
      "    def retrieve_data(self, query):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to process the retrieved data\n",
      "    def process_data(self, data):\n",
      "        pass\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 2. Any existing classes that wrap external retrievers in the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': 'class ContextualCompressionRetriever(BaseRetriever):\\n    \"\"\"Retriever that wraps a base retriever and compresses the results.\"\"\"\\n\\n    base_compressor: BaseDocumentCompressor\\n    \"\"\"Compressor for compressing retrieved documents.\"\"\"\\n\\n    base_retriever: BaseRetriever\\n    \"\"\"Base Retriever to use for getting relevant documents.\"\"\"\\n', 'trace': 'class name: ContextualCompressionRetriever. ', 'lineno': 15.0, 'end_lineno': 22.0, 'path_to_file': './langchain/libs/langchain/langchain/retrievers/contextual_compression.py', 'id': 9650, 'thirdai_index': 9650, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def test_create_external_handler(self) -> None:\\n        \"\"\"If we\\'re using a Streamlit that *does* expose its own callback handler,\\n        delegate to that implementation.\\n        \"\"\"\\n\\n        mock_streamlit_module = MagicMock()\\n\\n        def external_import_success(\\n            name: str, globals: Any, locals: Any, fromlist: Any, level: int\\n        ) -> Any:\\n            if name == \"streamlit.external.langchain\":\\n                return mock_streamlit_module\\n            return self.builtins_import(name, globals, locals, fromlist, level)\\n\\n        builtins.__import__ = external_import_success  # type: ignore[assignment]\\n\\n        parent_container = MagicMock()\\n        thought_labeler = MagicMock()\\n        StreamlitCallbackHandler(\\n            parent_container,\\n            max_thought_containers=1,\\n            expand_new_thoughts=True,\\n            collapse_completed_thoughts=False,\\n            thought_labeler=thought_labeler,\\n        )\\n\\n        # Streamlit\\'s handler should be created\\n        mock_streamlit_module.StreamlitCallbackHandler.assert_called_once_with(\\n            parent_container,\\n            max_thought_containers=1,\\n            expand_new_thoughts=True,\\n            collapse_completed_thoughts=False,\\n            thought_labeler=thought_labeler,\\n        )\\n', 'trace': 'class name: TestImport. function name: test_create_external_handler', 'lineno': 55.0, 'end_lineno': 88.0, 'path_to_file': './langchain/libs/community/tests/unit_tests/callbacks/test_streamlit_callback.py', 'id': 12236, 'thirdai_index': 12236, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"All integration tests (tests that call out to an external API).\"\"\"\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 1.0, 'path_to_file': './langchain/libs/langchain/tests/integration_tests/__init__.py', 'id': 6533, 'thirdai_index': 6533, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the interface used by the open source codebase\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    # TODO: Implement the method to retrieve data using MyRetriever\n",
      "    def retrieve_data(self, query):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to process the retrieved data\n",
      "    def process_data(self, data):\n",
      "        pass\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 3. Examples of how other retrievers are integrated into the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': 'async def test_chat_from_role_strings() -> None:\\n    \"\"\"Test instantiation of chat template from role strings.\"\"\"\\n    with pytest.warns(LangChainPendingDeprecationWarning):\\n        template = ChatPromptTemplate.from_role_strings(\\n            [\\n                (\"system\", \"You are a bot.\"),\\n                (\"assistant\", \"hello!\"),\\n                (\"human\", \"{question}\"),\\n                (\"other\", \"{quack}\"),\\n            ]\\n        )\\n\\n    expected = [\\n        ChatMessage(content=\"You are a bot.\", role=\"system\"),\\n        ChatMessage(content=\"hello!\", role=\"assistant\"),\\n        ChatMessage(content=\"How are you?\", role=\"human\"),\\n        ChatMessage(content=\"duck\", role=\"other\"),\\n    ]\\n\\n    messages = template.format_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n\\n    messages = await template.aformat_messages(question=\"How are you?\", quack=\"duck\")\\n    assert messages == expected\\n', 'trace': '', 'lineno': 302.0, 'end_lineno': 325.0, 'path_to_file': './langchain/libs/core/tests/unit_tests/prompts/test_chat.py', 'id': 2060, 'thirdai_index': 2060, 'source': 'langchain.csv'}\n",
      "{'snippet': '@deprecated(\\n    since=\"0.1.17\",\\n    alternative=(\\n        \"create_history_aware_retriever together with create_retrieval_chain \"\\n        \"(see example in docstring)\"\\n    ),\\n    removal=\"0.3.0\",\\n)\\nclass ConversationalRetrievalChain(BaseConversationalRetrievalChain):\\n    \"\"\"Chain for having a conversation based on retrieved documents.\\n\\n    This class is deprecated. See below for an example implementation using\\n    `create_retrieval_chain`. Additional walkthroughs can be found at\\n    https://python.langchain.com/docs/use_cases/question_answering/chat_history\\n\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                create_history_aware_retriever,\\n                create_retrieval_chain,\\n            )\\n            from langchain.chains.combine_documents import create_stuff_documents_chain\\n            from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\\n            from langchain_openai import ChatOpenAI\\n\\n\\n            retriever = ...  # Your retriever\\n\\n            llm = ChatOpenAI()\\n\\n            # Contextualize question\\n            contextualize_q_system_prompt = (\\n                \"Given a chat history and the latest user question \"\\n                \"which might reference context in the chat history, \"\\n                \"formulate a standalone question which can be understood \"\\n                \"without the chat history. Do NOT answer the question, just \"\\n                \"reformulate it if needed and otherwise return it as is.\"\\n            )\\n            contextualize_q_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", contextualize_q_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            history_aware_retriever = create_history_aware_retriever(\\n                llm, retriever, contextualize_q_prompt\\n            )\\n\\n            # Answer question\\n            qa_system_prompt = (\\n                \"You are an assistant for question-answering tasks. Use \"\\n                \"the following pieces of retrieved context to answer the \"\\n                \"question. If you don\\'t know the answer, just say that you \"\\n                \"don\\'t know. Use three sentences maximum and keep the answer \"\\n                \"concise.\"\\n                \"\\\\n\\\\n\"\\n                \"{context}\"\\n            )\\n            qa_prompt = ChatPromptTemplate.from_messages(\\n                [\\n                    (\"system\", qa_system_prompt),\\n                    MessagesPlaceholder(\"chat_history\"),\\n                    (\"human\", \"{input}\"),\\n                ]\\n            )\\n            # Below we use create_stuff_documents_chain to feed all retrieved context\\n            # into the LLM. Note that we can also use StuffDocumentsChain and other\\n            # instances of BaseCombineDocumentsChain.\\n            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\\n            rag_chain = create_retrieval_chain(\\n                history_aware_retriever, question_answer_chain\\n            )\\n\\n            # Usage:\\n            chat_history = []  # Collect chat history here (a sequence of messages)\\n            rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\\n\\n    This chain takes in chat history (a list of messages) and new questions,\\n    and then returns an answer to that question.\\n    The algorithm for this chain consists of three parts:\\n\\n    1. Use the chat history and the new question to create a \"standalone question\".\\n    This is done so that this question can be passed into the retrieval step to fetch\\n    relevant documents. If only the new question was passed in, then relevant context\\n    may be lacking. If the whole conversation was passed into retrieval, there may\\n    be unnecessary information there that would distract from retrieval.\\n\\n    2. This new question is passed to the retriever and relevant documents are\\n    returned.\\n\\n    3. The retrieved documents are passed to an LLM along with either the new question\\n    (default behavior) or the original question and chat history to generate a final\\n    response.\\n\\n    Example:\\n        .. code-block:: python\\n\\n            from langchain.chains import (\\n                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\\n            )\\n            from langchain_core.prompts import PromptTemplate\\n            from langchain_community.llms import OpenAI\\n\\n            combine_docs_chain = StuffDocumentsChain(...)\\n            vectorstore = ...\\n            retriever = vectorstore.as_retriever()\\n\\n            # This controls how the standalone question is generated.\\n            # Should take `chat_history` and `question` as input variables.\\n            template = (\\n                \"Combine the chat history and follow up question into \"\\n                \"a standalone question. Chat History: {chat_history}\"\\n                \"Follow up question: {question}\"\\n            )\\n            prompt = PromptTemplate.from_template(template)\\n            llm = OpenAI()\\n            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\\n            chain = ConversationalRetrievalChain(\\n                combine_docs_chain=combine_docs_chain,\\n                retriever=retriever,\\n                question_generator=question_generator_chain,\\n            )\\n    \"\"\"\\n\\n    retriever: BaseRetriever\\n    \"\"\"Retriever to use to fetch documents.\"\"\"\\n    max_tokens_limit: Optional[int] = None\\n    \"\"\"If set, enforces that the documents returned are less than this limit.\\n    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\\n', 'trace': 'class name: ConversationalRetrievalChain. ', 'lineno': 237.0, 'end_lineno': 366.0, 'path_to_file': './langchain/libs/langchain/langchain/chains/conversational_retrieval/base.py', 'id': 7903, 'thirdai_index': 7903, 'source': 'langchain.csv'}\n",
      "{'snippet': '@deprecated(\\n    since=\"0.0.32\",\\n    removal=\"0.3.0\",\\n    alternative_import=\"langchain_google_community.BigQueryLoader\",\\n)\\nclass BigQueryLoader(BaseLoader):\\n    \"\"\"Load from the Google Cloud Platform `BigQuery`.\\n\\n    Each document represents one row of the result. The `page_content_columns`\\n    are written into the `page_content` of the document. The `metadata_columns`\\n    are written into the `metadata` of the document. By default, all columns\\n    are written into the `page_content` and none into the `metadata`.\\n\\n    \"\"\"\\n', 'trace': 'class name: BigQueryLoader. ', 'lineno': 15.0, 'end_lineno': 28.0, 'path_to_file': './langchain/libs/community/langchain_community/document_loaders/bigquery.py', 'id': 23672, 'thirdai_index': 23672, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the interface used by the open source codebase\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    # TODO: Implement the method to retrieve data using MyRetriever\n",
      "    def retrieve_data(self, query):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to process the retrieved data\n",
      "    def process_data(self, data):\n",
      "        pass\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 4. Any specific requirements or constraints for integrating external retrievers into the codebase.\n",
      "Retrieved references:\n",
      "{'snippet': '@pytest.mark.requires(\"rank_bm25\")\\ndef test_from_texts() -> None:\\n    input_texts = [\"I have a pen.\", \"Do you have a pen?\", \"I have a bag.\"]\\n    bm25_retriever = BM25Retriever.from_texts(texts=input_texts)\\n    assert len(bm25_retriever.docs) == 3\\n    assert bm25_retriever.vectorizer.doc_len == [4, 5, 4]\\n', 'trace': 'function name: test_from_texts', 'lineno': 7.0, 'end_lineno': 12.0, 'path_to_file': './langchain/libs/community/tests/unit_tests/retrievers/test_bm25.py', 'id': 12358, 'thirdai_index': 12358, 'source': 'langchain.csv'}\n",
      "{'snippet': '\"\"\"All integration tests (tests that call out to an external API).\"\"\"\\n', 'trace': '', 'lineno': 1.0, 'end_lineno': 1.0, 'path_to_file': './langchain/libs/langchain/tests/integration_tests/__init__.py', 'id': 6533, 'thirdai_index': 6533, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def add_texts(\\n        self,\\n        texts: Iterable[str],\\n        metadatas: Optional[List[dict]] = None,\\n        ids: Optional[List[str]] = None,\\n        timestamp: int = 0,\\n        **kwargs: Any,\\n    ) -> List[str]:\\n        \"\"\"Run more texts through the embeddings and add to the vectorstore.\\n\\n        Args:\\n            texts: Iterable of strings to add to the vectorstore.\\n            metadatas: Optional list of metadatas associated with the texts.\\n            ids: Optional ids of each text object.\\n            timestamp: Optional timestamp to write new texts with.\\n            kwargs: vectorstore specific parameters\\n\\n        Returns:\\n            List of ids from adding the texts into the vectorstore.\\n        \"\"\"\\n        tiledb = guard_import(\"tiledb\")\\n        embeddings = self.embedding.embed_documents(list(texts))\\n        if ids is None:\\n            ids = [str(random.randint(0, MAX_UINT64 - 1)) for _ in texts]\\n\\n        external_ids = np.array(ids).astype(np.uint64)\\n        vectors = np.empty((len(embeddings)), dtype=\"O\")\\n        for i in range(len(embeddings)):\\n            vectors[i] = np.array(embeddings[i], dtype=np.float32)\\n        self.vector_index.update_batch(\\n            vectors=vectors,\\n            external_ids=external_ids,\\n            timestamp=timestamp if timestamp != 0 else None,\\n        )\\n\\n        docs = {}\\n        docs[\"text\"] = np.array(texts)\\n        if metadatas is not None:\\n            metadata_attr = np.empty([len(metadatas)], dtype=object)\\n            i = 0\\n            for metadata in metadatas:\\n                metadata_attr[i] = np.frombuffer(pickle.dumps(metadata), dtype=np.uint8)\\n                i += 1\\n            docs[\"metadata\"] = metadata_attr\\n\\n        docs_array = tiledb.open(\\n            self.docs_array_uri,\\n            \"w\",\\n            timestamp=timestamp if timestamp != 0 else None,\\n            config=self.config,\\n        )\\n        docs_array[external_ids] = docs\\n        docs_array.close()\\n        return ids\\n', 'trace': 'class name: TileDB. function name: add_texts', 'lineno': 634.0, 'end_lineno': 687.0, 'path_to_file': './langchain/libs/community/langchain_community/vectorstores/tiledb.py', 'id': 16705, 'thirdai_index': 16705, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the interface used by the open source codebase\n",
      "\n",
      "class MyRetrieverWrapper(BaseRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    # TODO: Implement the method to retrieve data using MyRetriever\n",
      "    def retrieve_data(self, query):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to process the retrieved data\n",
      "    def process_data(self, data):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to add texts to the vectorstore\n",
      "    def add_texts(self, texts, metadatas=None, ids=None, timestamp=0, **kwargs):\n",
      "        pass\n",
      "```\n",
      "====================================================================================================\n",
      "Query: 5. Any relevant documentation or guidelines for integrating external retrievers.\n",
      "Retrieved references:\n",
      "{'snippet': 'def test_pgvector_retriever_search_threshold_custom_normalization_fn() -> None:\\n    \"\"\"Test searching with threshold and custom normalization function\"\"\"\\n    texts = [\"foo\", \"bar\", \"baz\"]\\n    metadatas = [{\"page\": str(i)} for i in range(len(texts))]\\n    docsearch = PGVector.from_texts(\\n        texts=texts,\\n        collection_name=\"test_collection\",\\n        embedding=FakeEmbeddingsWithAdaDimension(),\\n        metadatas=metadatas,\\n        connection_string=CONNECTION_STRING,\\n        pre_delete_collection=True,\\n        relevance_score_fn=lambda d: d * 0,\\n    )\\n\\n    retriever = docsearch.as_retriever(\\n        search_type=\"similarity_score_threshold\",\\n        search_kwargs={\"k\": 3, \"score_threshold\": 0.5},\\n    )\\n    output = retriever.invoke(\"foo\")\\n    assert output == []\\n', 'trace': 'function name: test_pgvector_max_marginal_relevance_search', 'lineno': 342.0, 'end_lineno': 361.0, 'path_to_file': './langchain/libs/community/tests/integration_tests/vectorstores/test_pgvector.py', 'id': 13431, 'thirdai_index': 13431, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def _similarity_search_with_relevance_scores(\\n        self,\\n        query: str,\\n        k: int = 4,\\n        **kwargs: Any,\\n    ) -> List[Tuple[Document, float]]:\\n        return [(doc, 0.5) for doc in _get_example_memories()]\\n', 'trace': 'class name: MockVectorStore. function name: _similarity_search_with_relevance_scores', 'lineno': 56.0, 'end_lineno': 62.0, 'path_to_file': './langchain/libs/langchain/tests/unit_tests/retrievers/test_time_weighted_retriever.py', 'id': 6305, 'thirdai_index': 6305, 'source': 'langchain.csv'}\n",
      "{'snippet': '    def as_retriever(self, **kwargs: Any) -> VectorStoreRetriever:\\n        \"\"\"Return VectorStoreRetriever initialized from this VectorStore.\\n\\n        Args:\\n            search_type (Optional[str]): Defines the type of search that\\n                the Retriever should perform.\\n                Can be \"similarity\" (default), \"mmr\", or\\n                \"similarity_score_threshold\".\\n            search_kwargs (Optional[Dict]): Keyword arguments to pass to the\\n                search function. Can include things like:\\n                    k: Amount of documents to return (Default: 4)\\n                    score_threshold: Minimum relevance threshold\\n                        for similarity_score_threshold\\n                    fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\\n                    lambda_mult: Diversity of results returned by MMR;\\n                        1 for minimum diversity and 0 for maximum. (Default: 0.5)\\n                    filter: Filter by document metadata\\n\\n        Returns:\\n            VectorStoreRetriever: Retriever class for VectorStore.\\n\\n        Examples:\\n\\n        .. code-block:: python\\n\\n            # Retrieve more documents with higher diversity\\n            # Useful if your dataset has many similar documents\\n            docsearch.as_retriever(\\n                search_type=\"mmr\",\\n                search_kwargs={\\'k\\': 6, \\'lambda_mult\\': 0.25}\\n            )\\n\\n            # Fetch more documents for the MMR algorithm to consider\\n            # But only return the top 5\\n            docsearch.as_retriever(\\n                search_type=\"mmr\",\\n                search_kwargs={\\'k\\': 5, \\'fetch_k\\': 50}\\n            )\\n\\n            # Only retrieve documents that have a relevance score\\n            # Above a certain threshold\\n            docsearch.as_retriever(\\n                search_type=\"similarity_score_threshold\",\\n                search_kwargs={\\'score_threshold\\': 0.8}\\n            )\\n\\n            # Only get the single most similar document from the dataset\\n            docsearch.as_retriever(search_kwargs={\\'k\\': 1})\\n\\n            # Use a filter to only retrieve documents from a specific paper\\n            docsearch.as_retriever(\\n                search_kwargs={\\'filter\\': {\\'paper_title\\':\\'GPT-4 Technical Report\\'}}\\n            )\\n        \"\"\"\\n        tags = kwargs.pop(\"tags\", None) or []\\n        tags.extend(self._get_retriever_tags())\\n        return VectorStoreRetriever(vectorstore=self, **kwargs, tags=tags)\\n', 'trace': 'class name: VectorStore. function name: as_retriever', 'lineno': 595.0, 'end_lineno': 651.0, 'path_to_file': './langchain/libs/core/langchain_core/vectorstores.py', 'id': 2794, 'thirdai_index': 2794, 'source': 'langchain.csv'}\n",
      "Draft Answer:\n",
      "```python\n",
      "# TODO: Create a class that wraps MyRetriever and inherits the interface used by the open source codebase\n",
      "\n",
      "class MyRetrieverWrapper(VectorStoreRetriever):\n",
      "    def __init__(self, my_retriever):\n",
      "        self.my_retriever = my_retriever\n",
      "\n",
      "    # TODO: Implement the method to retrieve data using MyRetriever\n",
      "    def retrieve_data(self, query):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to process the retrieved data\n",
      "    def process_data(self, data):\n",
      "        pass\n",
      "\n",
      "    # TODO: Implement the method to add texts to the vectorstore\n",
      "    def add_texts(self, texts, metadatas=None, ids=None, timestamp=0, **kwargs):\n",
      "        pass\n",
      "```\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "answer = copilot(task=task, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
