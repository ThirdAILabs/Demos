{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training and fine-tuning using NeuralDB\n",
    "\n",
    "In this notebook, we will pre-train a NeuralDB from scratch on the popular BEIR dataset (https://github.com/beir-cellar/beir) using ThirdAI's NeuralDB. We will use the 'Scifact' dataset to demonstrate how NeuralDB can just pre-train on a small dataset and outperform T5-large model trained on a huge corpus. \n",
    "\n",
    "This demo shows that one-model for all is sub-optimal and pre-training on specific downstream datasets is required to get the best results.\n",
    "\n",
    "Please Note: You can immediately run a version of this notebook in your browser on Google Colab at the following link:\n",
    "\n",
    "https://githubtocolab.com/ThirdAILabs/Demos/blob/main/neural_db_examples/scifact.ipynb\n",
    "\n",
    "This notebook uses an activation key that will only work with this demo. If you want to try us out on your own dataset, you can obtain a free trial license at the following link: https://www.thirdai.com/try-bolt/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import thirdai and activate license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beir\n",
    "# !pip3 install thirdai --upgrade\n",
    "!pip3 install \"thirdai[neural_db]\" --upgrade\n",
    "\n",
    "import os\n",
    "from thirdai import licensing\n",
    "licensing.deactivate()\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process the dataset into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai.demos import download_beir_dataset\n",
    "\n",
    "dataset = \"scifact\"\n",
    "unsup_file, sup_train_file, sup_test_file, n_target_classes = download_beir_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above step, *unsup_file* refers to the corpus file with document id, title and text. We can have even more columns with other metadata for each document. Pre-training with NeuralDB supports two types of columns, strong and weak. For the purpose of this demo, we choose 'title' to be the strong column and 'text' to be the weak column.\n",
    "\n",
    "A couple of sample rows of the *unsup_file* are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOC_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.</td>\n",
       "      <td>Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient  to calculate relative anisotropy  and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development  early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coeffic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Induction of myelodysplasia by myeloid-derived suppressor cells.</td>\n",
       "      <td>Myelodysplastic syndromes (MDS) are age-dependent stem cell malignancies that share biological features of activated adaptive immune response and ineffective hematopoiesis. Here we report that myeloid-derived suppressor cells (MDSC)  which are classically linked to immunosuppression  inflammation  and cancer  were markedly expanded in the bone marrow of MDS patients and played a pathogenetic role in the development of ineffective hematopoiesis. These clonally distinct MDSC overproduce hematopoietic suppressive cytokines and function as potent apoptotic effectors targeting autologous hematopoietic progenitors. Using multiple transfected cell models  we found that MDSC expansion is driven ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DOC_ID  \\\n",
       "0       0   \n",
       "1       1   \n",
       "\n",
       "                                                                                                                                 TITLE  \\\n",
       "0  Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.   \n",
       "1                                                                     Induction of myelodysplasia by myeloid-derived suppressor cells.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          TEXT  \n",
       "0  Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient  to calculate relative anisotropy  and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development  early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coeffic...  \n",
       "1  Myelodysplastic syndromes (MDS) are age-dependent stem cell malignancies that share biological features of activated adaptive immune response and ineffective hematopoiesis. Here we report that myeloid-derived suppressor cells (MDSC)  which are classically linked to immunosuppression  inflammation  and cancer  were markedly expanded in the bone marrow of MDS patients and played a pathogenetic role in the development of ineffective hematopoiesis. These clonally distinct MDSC overproduce hematopoietic suppressive cytokines and function as potent apoptotic effectors targeting autologous hematopoietic progenitors. Using multiple transfected cell models  we found that MDSC expansion is driven ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 700\n",
    "pd.read_csv(unsup_file, nrows=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a NeuralDB from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb\n",
    "db = ndb.NeuralDB(user_id=\"my_user\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the unsupervised documents and create an insertable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "csv_files = [unsup_file]\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = ndb.CSV(\n",
    "        path=file,\n",
    "        id_column=\"DOC_ID\",\n",
    "        strong_columns=[\"TITLE\"],\n",
    "        weak_columns=[\"TEXT\"],  \n",
    "        reference_columns=[\"TITLE\",\"TEXT\"])\n",
    "    #\n",
    "    insertable_docs.append(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train on the *unsup_file*\n",
    "\n",
    "In the following step, we do the pre-training by specifying the strong and weak columns. For this demo, we use 'TITLE' as the strong column and 'TEXT' as the weak column. We can have more columns in either of the lists. The training time and the test accuracies are shown below. We can see that by just pre-traiing on the Scifact dataset, we get 40% precision@1 which beats T5-large's performance on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data | source 'Documents:\n",
      "unsupervised.csv' | vectors 110213 | batches 54 | time 0.542s | complete\n",
      "\n",
      "train | epoch 0 | train_steps 54 | train_hash_precision@5=0.281137  | train_batches 54 | time 9.398s\n",
      "\n",
      "train | epoch 1 | train_steps 108 | train_hash_precision@5=0.87396  | train_batches 54 | time 7.329s\n",
      "\n",
      "train | epoch 2 | train_steps 162 | train_hash_precision@5=0.991335  | train_batches 54 | time 7.318s\n",
      "\n",
      "train | epoch 3 | train_steps 216 | train_hash_precision@5=0.998483  | train_batches 54 | time 7.397s\n",
      "\n",
      "train | epoch 4 | train_steps 270 | train_hash_precision@5=0.999503  | train_batches 54 | time 7.407s\n",
      "\n",
      "train | epoch 5 | train_steps 324 | train_hash_precision@5=0.999849  | train_batches 54 | time 7.499s\n",
      "\n",
      "train | epoch 6 | train_steps 378 | train_hash_precision@5=0.99992  | train_batches 54 | time 7.381s\n",
      "\n",
      "train | epoch 7 | train_steps 432 | train_hash_precision@5=0.999944  | train_batches 54 | time 7.416s\n",
      "\n",
      "train | epoch 8 | train_steps 486 | train_hash_precision@5=0.999967  | train_batches 54 | time 7.387s\n",
      "\n",
      "train | epoch 9 | train_steps 540 | train_hash_precision@5=0.999946  | train_batches 54 | time 7.382s\n",
      "\n",
      "train | epoch 10 | train_steps 594 | train_hash_precision@5=0.999953  | train_batches 54 | time 7.447s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_ids = db.insert(insertable_docs, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate after pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(test_file, db):\n",
    "    test_df = pd.read_csv(sup_test_file)\n",
    "    correct_count = 0\n",
    "    for i in range(test_df.shape[0]):\n",
    "        query = test_df['QUERY'][i]\n",
    "        top_pred = db.search(query=query,top_k=1)[0].id\n",
    "        if str(top_pred) in test_df['DOC_ID'][i].split(\":\"):\n",
    "            correct_count += 1\n",
    "    ##\n",
    "    return correct_count/test_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "print(get_precision(sup_test_file, db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on supervised data (OPTIONAL)\n",
    "\n",
    "If you have supervised data that maps queries to documents, you can further improve the model performance by fine-tuning your pre-trained model on the supervised data.\n",
    "\n",
    "The training time to fine-tune and the final accuracy are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(sup_train_file)\n",
    "\n",
    "if type(train_df['DOC_ID'][0])==str:\n",
    "    train_df['DOC_ID'] = train_df['DOC_ID'].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    train_df.to_csv(sup_train_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data | source 'Supervised training samples' | vectors 809 | batches 1 | time 0.168s | complete\n",
      "\n",
      "train | epoch 0 | train_steps 595 |  | train_batches 1 | time 0.070s    \n",
      "\n",
      "train | epoch 1 | train_steps 596 |  | train_batches 1 | time 0.073s    \n",
      "\n",
      "train | epoch 2 | train_steps 597 |  | train_batches 1 | time 0.064s    \n",
      "\n",
      "train | epoch 3 | train_steps 598 |  | train_batches 1 | time 0.063s    \n",
      "\n",
      "train | epoch 4 | train_steps 599 |  | train_batches 1 | time 0.068s    \n",
      "\n",
      "train | epoch 5 | train_steps 600 |  | train_batches 1 | time 0.063s    \n",
      "\n",
      "train | epoch 6 | train_steps 601 |  | train_batches 1 | time 0.070s    \n",
      "\n",
      "train | epoch 7 | train_steps 602 |  | train_batches 1 | time 0.071s    \n",
      "\n",
      "train | epoch 8 | train_steps 603 |  | train_batches 1 | time 0.067s    \n",
      "\n",
      "train | epoch 9 | train_steps 604 |  | train_batches 1 | time 0.064s    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "db.supervised_train([ndb.Sup(sup_train_file, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0])],learning_rate=0.001, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7433333333333333\n"
     ]
    }
   ],
   "source": [
    "print(get_precision(sup_test_file, db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons against T5\n",
    "\n",
    "| Model | Precision@1 |\n",
    "| --- | --- |\n",
    "| NeuralDB (pre-training + fine-tuning) | 74% |\n",
    "| OpenAI Ada-002 | 63%    |\n",
    "| Instruct-L | 52%    |\n",
    "|  NeuralDB (just pre-training) |     40%     |\n",
    "| T5-large | 39.3%    |\n",
    "| T5-base |  34.7%    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
