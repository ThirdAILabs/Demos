{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training and fine-tuning using NeuralDB\n",
    "\n",
    "In this notebook, we will pre-train a NeuralDB from scratch on the popular BEIR dataset (https://github.com/beir-cellar/beir) using ThirdAI's NeuralDB. We will use the 'Scifact' dataset to demonstrate how NeuralDB can just pre-train on a small dataset and outperform T5-large model trained on a huge corpus. \n",
    "\n",
    "This demo shows that one-model for all is sub-optimal and pre-training on specific downstream datasets is required to get the best results.\n",
    "\n",
    "Please Note: You can immediately run a version of this notebook in your browser on Google Colab at the following link:\n",
    "\n",
    "https://githubtocolab.com/ThirdAILabs/Demos/blob/main/neural_db/examples/scifact.ipynb\n",
    "\n",
    "This notebook uses an activation key that will only work with this demo. If you want to try us out on your own dataset, you can obtain a free trial license at the following link: https://www.thirdai.com/try-bolt/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import thirdai and activate license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beir\n",
    "# !pip3 install thirdai --upgrade\n",
    "!pip3 install \"thirdai[neural_db]\" --upgrade\n",
    "\n",
    "import os\n",
    "from thirdai import licensing\n",
    "licensing.deactivate()\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and process the dataset into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai.demos import download_beir_dataset\n",
    "\n",
    "dataset = \"scifact\"\n",
    "unsup_file, sup_train_file, sup_test_file, n_target_classes = download_beir_dataset(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above step, *unsup_file* refers to the corpus file with document id, title and text. We can have even more columns with other metadata for each document. Pre-training with NeuralDB supports two types of columns, strong and weak. For the purpose of this demo, we choose 'title' to be the strong column and 'text' to be the weak column.\n",
    "\n",
    "A couple of sample rows of the *unsup_file* are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 700\n",
    "pd.read_csv(unsup_file, nrows=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a NeuralDB from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb\n",
    "db = ndb.NeuralDB(user_id=\"my_user\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the unsupervised documents and create an insertable object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insertable_docs = []\n",
    "csv_files = [unsup_file]\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = ndb.CSV(\n",
    "        path=file,\n",
    "        id_column=\"DOC_ID\",\n",
    "        strong_columns=[\"TITLE\"],\n",
    "        weak_columns=[\"TEXT\"],  \n",
    "        reference_columns=[\"TITLE\",\"TEXT\"])\n",
    "    #\n",
    "    insertable_docs.append(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-train on the *unsup_file*\n",
    "\n",
    "In the following step, we do the pre-training by specifying the strong and weak columns. For this demo, we use 'TITLE' as the strong column and 'TEXT' as the weak column. We can have more columns in either of the lists. The training time and the test accuracies are shown below. We can see that by just pre-traiing on the Scifact dataset, we get 40% precision@1 which beats T5-large's performance on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(insertable_docs, train=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate after pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(test_file, db):\n",
    "    test_df = pd.read_csv(sup_test_file)\n",
    "    correct_count = 0\n",
    "    for i in range(test_df.shape[0]):\n",
    "        query = test_df['QUERY'][i]\n",
    "        top_pred = db.search(query=query,top_k=1)[0].id\n",
    "        if str(top_pred) in test_df['DOC_ID'][i].split(\":\"):\n",
    "            correct_count += 1\n",
    "    ##\n",
    "    return correct_count/test_df.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_precision(sup_test_file, db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on supervised data (OPTIONAL)\n",
    "\n",
    "If you have supervised data that maps queries to documents, you can further improve the model performance by fine-tuning your pre-trained model on the supervised data.\n",
    "\n",
    "The training time to fine-tune and the final accuracy are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(sup_train_file)\n",
    "\n",
    "if type(train_df['DOC_ID'][0])==str:\n",
    "    train_df['DOC_ID'] = train_df['DOC_ID'].apply(lambda x: int(x.split(\":\")[0]))\n",
    "    train_df.to_csv(sup_train_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.supervised_train([ndb.Sup(sup_train_file, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0])],learning_rate=0.001, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_precision(sup_test_file, db))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons against T5\n",
    "\n",
    "| Model | Precision@1 |\n",
    "| --- | --- |\n",
    "| NeuralDB (pre-training + fine-tuning) | 77% |\n",
    "| OpenAI Ada-002 | 63%    |\n",
    "|  NeuralDB (just pre-training) |     53%     |\n",
    "| Instruct-L | 52%    |\n",
    "| T5-large | 39.3%    |\n",
    "| T5-base |  34.7%    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
