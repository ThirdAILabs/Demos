{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA INDEX INTEGRATION FOR NEURALDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requiremnts\n",
    "!pip install llama-index\n",
    "!pip install thirdai[neural_db]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your openai key as environment variable.\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-.....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Path to the directory containing all the files\n",
    "document_directory = \"/path/to/the/directory\"\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(document_directory).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thirdai import neural_db as ndb\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    ")\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class NDBretriever(BaseRetriever):\n",
    "    def __init__(self, nodes, storage_context, top_k=5):\n",
    "        self.db = self.constructdb(nodes)\n",
    "        self.storage_context = storage_context\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def constructdb(self, nodes):\n",
    "        db = ndb.NeuralDB()\n",
    "        docs = []\n",
    "        for node in nodes:\n",
    "            doc = ndb.InMemoryText(name=node.node_id,texts=[node.text])\n",
    "            docs.append(doc)\n",
    "        \n",
    "        db.insert(docs)\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        results = self.db.search(query_bundle.query_str, top_k=self.top_k)\n",
    "        node_with_scores: List[NodeWithScore] = []\n",
    "        for result in results:\n",
    "            node = self.storage_context.docstore.get_node(result.source)\n",
    "            node_with_scores.append(NodeWithScore(node=node, score=result.score))\n",
    "        \n",
    "        return node_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# define custom retriever\n",
    "ndb_retriever = NDBretriever(nodes=nodes,storage_context=storage_context,top_k=5)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=ndb_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = custom_query_engine.query(\n",
    "    \"what is lorem_ipsum?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
