{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA INDEX INTEGRATION FOR NEURALDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requiremnts\n",
    "!pip install llama-index\n",
    "!pip install thirdai[neural_db]\n",
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from thirdai import licensing\n",
    "\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"\"  # Enter your OpenAI key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# Path to the directory containing all the files\n",
    "document_directory = \"data/\"\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(document_directory).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "nodes = Settings.node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize storage context (by default it's in-memory)\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional code for Redacting PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can look into https://github.com/ThirdAILabs/Demos/tree/main/universal_deep_transformer/named_entity_recognition for more info about our NER models.\n",
    "import os\n",
    "from thirdai import bolt\n",
    "\n",
    "if not os.path.isdir(\"./models/\"):\n",
    "    os.system(\"mkdir ./models/\")\n",
    "\n",
    "if not os.path.exists(\"./models/pretrained_multilingual.model\"):\n",
    "    os.system(\n",
    "        \"wget -nv -O ./models/pretrained_multilingual.model 'https://www.dropbox.com/scl/fi/z3xo7nqbjpo1xsvl9b0xh/ner_model_new.bolt?rlkey=md3lw409d55krjdm2ao6kjo7o&st=jnv84wtg&dl=0'\"\n",
    "    )\n",
    "    \n",
    "\n",
    "pii_model = bolt.UniversalDeepTransformer.load(\"./models/pretrained_multilingual.model\")\n",
    "    \n",
    "def redact_pii(node):\n",
    "    text = node.text.replace(\"\\n\",\" \")\n",
    "    predicted_tags = pii_model.predict({\"source\": text}, top_k=1)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    redacted_tokens = [\n",
    "        predicted_tags[i][0][0] if predicted_tags[i][0][0] != \"O\" else token\n",
    "        for i, token in enumerate(tokens)\n",
    "    ]\n",
    "    node.text = \" \".join(redacted_tokens)\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Retriever for NeuralDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from thirdai import neural_db as ndb\n",
    "from llama_index.core.retrievers import (\n",
    "    BaseRetriever,\n",
    ")\n",
    "# import NodeWithScore\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class NDBretriever(BaseRetriever):\n",
    "    def __init__(self, nodes, storage_context, top_k=5):\n",
    "        self.db = self.constructdb(nodes)\n",
    "        self.storage_context = storage_context\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def constructdb(self, nodes):\n",
    "        db = ndb.NeuralDB()\n",
    "        docs = []\n",
    "        for node in nodes:\n",
    "            doc = ndb.InMemoryText(name=node.node_id,texts=[node.text])\n",
    "            docs.append(doc)\n",
    "        \n",
    "        db.insert(docs)\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def _retrieve(self, query_bundle):\n",
    "        results = self.db.search(query_bundle.query_str, top_k=self.top_k)\n",
    "        node_with_scores: List[NodeWithScore] = []\n",
    "        for result in results:\n",
    "            node = self.storage_context.docstore.get_node(result.source)\n",
    "            # To Remove PII information from the text before sending to LLM, we can use our NER model to detect and remove PII. Uncomment the following comments to use it.\n",
    "            # node = redact_pii(node)\n",
    "            \n",
    "            # To see what exactly goes into LLM, you can use the following\n",
    "            # print(node.get_content())\n",
    "            node_with_scores.append(NodeWithScore(node=node, score=result.score))\n",
    "        \n",
    "        return node_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# define custom retriever\n",
    "ndb_retriever = NDBretriever(nodes=nodes,storage_context=storage_context,top_k=5)\n",
    "\n",
    "# define response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=ndb_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional code for how to use our RLHF Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into https://github.com/ThirdAILabs/Demos/blob/main/neural_db/main_example.ipynb for all the functionalities of NeuralDB you can use. Lets see some sample examples.\n",
    "\n",
    "# To associate a source query and target\n",
    "ndb_retriever.db.associate(source=\"who are the parties involved\", target=\"made by and between\")\n",
    "\n",
    "# To use our upvote functionality\n",
    "results = ndb_retriever.db.search(query=\"made by and between\",top_k=10)\n",
    "for result in results:\n",
    "    print(result.id)\n",
    "    print(result.text)\n",
    "    # print(result.source)\n",
    "    # print(result.metadata)\n",
    " \n",
    "# If you think that 4th id answer is more relevant and correct for given query, you can teach model by upvoting \n",
    "ndb_retriever.db.text_to_result(\"made by and between\",4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query your application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = custom_query_engine.query(\n",
    "    \"made by and between\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
