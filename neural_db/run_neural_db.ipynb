{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's NeuralDB\n",
    "\n",
    "First let's import the relevant module and initialize a neural db class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb\n",
    "\n",
    "db = ndb.NeuralDB(user_id=\"my_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "At this point, the db is uninitialized. We can either initialize from scratch like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.from_scratch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even build one with a base DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from thirdai import bolt\n",
    "\n",
    "checkpoint = \"qna_1.bolt\"\n",
    "\n",
    "if not os.path.exists(checkpoint):\n",
    "    if checkpoint==\"qna_1.bolt\":\n",
    "        os.system(\"wget -O qna_1.bolt 'https://www.dropbox.com/scl/fi/8i3qd9edhrm6zjviq7vvy/qna_1_0.7.7_frozen.bolt?dl=0&rlkey=raonu7dh3cy6mooucjrns49vf' \")\n",
    "    elif checkpoint==\"qna_2.bolt\":\n",
    "        os.system(\"wget -O qna_2.bolt 'https://www.dropbox.com/scl/fi/27psws3dcujgbma5xwsh1/qna_2_0.7.7_frozen.bolt?dl=0&rlkey=z1ivtoquspqole3i6mdmgwb9v' \")\n",
    "    elif checkpoint==\"contracts.bolt\":\n",
    "        os.system(\"wget -O contracts.bolt 'https://www.dropbox.com/scl/fi/dk9bw59bix245d9x49nhy/contracts_0.7.7_frozen.bolt?dl=0&rlkey=xs9uzyv65sug30oi201sy7u6v' \")\n",
    "    else:\n",
    "        print(\"please choose the checkpoint from the aforementioned list in the comment only\")\n",
    "\n",
    "db.from_udt(\n",
    "    udt=bolt.UniversalDeepTransformer.load(checkpoint),\n",
    "    id_col=\"DOC_ID\", id_delimiter=\":\", query_col=\"QUERY\", \n",
    "    input_dim=50_000, hidden_dim=2048, extreme_output_dim=50_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep CSV data\n",
    "\n",
    "Let's insert things into it!\n",
    "\n",
    "Currently, we support adding as many CSV files as you wish. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document. \n",
    "\n",
    "The file is required to have a column named \"DOC_ID\" with rows numbered from 0 to n_rows-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from document_impls import CSV\n",
    "\n",
    "csv_files = ['sample_nda.csv']\n",
    "csv_docs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = CSV(\n",
    "        path=\"sample_nda.csv\",\n",
    "        id_column=\"DOC_ID\",\n",
    "        strong_columns=[\"passage\"],\n",
    "        weak_columns=[\"para\"],  \n",
    "        reference_columns=[\"passage\"])\n",
    "\n",
    "    csv_docs.append(csv_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert CSV files into NeuralDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(csv_docs, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(csv_docs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just train on the docs\n",
    "\n",
    "Do not worry abt files being inserted multiple times, the DB takes care of de-duplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = db.insert(csv_docs, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "\n",
    "Now let's start searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"what is the termination period\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text())\n",
    "    # print(result.context(radius=3))\n",
    "    # print(result.source())\n",
    "    # print(result.metadata())\n",
    "    # result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"made by and between\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text())\n",
    "    # print(result.context(radius=3))\n",
    "    # print(result.source())\n",
    "    # print(result.metadata())\n",
    "    # result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
    "\n",
    "Now let's ask a tricky question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"who are the parties involved?\",\n",
    "    top_k=2,\n",
    "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text())\n",
    "    # print(result.context(radius=3))\n",
    "    # print(result.source())\n",
    "    # print(result.metadata())\n",
    "    # result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! looks like when we search for \"parties involved\", we do not get the correct paragraph in the 1st position. \n",
    "\n",
    "No worries, we'll show shot to teach the model to correct it's retrieval.\n",
    "\n",
    "### RLHF\n",
    "\n",
    "Let's go over some of NeuralDB's advanced features. The first one is text-to-text association. This allows you to teach the model that two keywords, phrases, or concepts are related.\n",
    "\n",
    "Based on the above example, let's teach the model that \"parties involved\" and the phrase \"made by between\" are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.associate(source=\"parties involved\", target=\"made by and between\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search again with the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = db.search(\n",
    "    query=\"who are the parties involved?\",\n",
    "    top_k=2,\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(result.text())\n",
    "    # print(result.source())\n",
    "    # print(result.metadata())\n",
    "    # result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go! In just a line, you taught the model to correct itself and retrieve the correct result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training (Optional)\n",
    "\n",
    "If you have supervised data for a specific CSV file in your list, you can simply train the DB on that file by specifying a source_id = source_ids[*file_number_in_your_list*].\n",
    "\n",
    "Note: The supervised file should have the query_column and id_column that you specify in the following call.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_files = ['sample_nda_sup.csv']\n",
    "\n",
    "db.supervised_train([ndb.Sup(path, query_column=\"QUERY\", id_column=\"DOC_ID\", source_id=source_ids[0]) for path in sup_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Save\n",
    "As usual, saving and loading are one-liners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your db\n",
    "db.save(\"temp.db\")\n",
    "\n",
    "# Loading is just like we showed above, with an optional progress handler\n",
    "db.from_checkpoint(\"temp.db\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import bolt_and_csv_to_checkpoint\n",
    "\n",
    "bolt_and_csv_to_checkpoint(db._savable_state.model.get_model(), csv_files[0], './playground_checkpoint/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
