{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ThirdAI's NeuralDB\n",
    "\n",
    "In this notebook, we will show \n",
    "\n",
    "1. How to use ThirdAI's Neural Database for building grounded specialized AI-Agents with ChatGPT (Retreival Augmented Generation) all on any CPU.\n",
    "\n",
    "2. (Optional) How to use your OpenAI key to get retrieval augmented answers from OpenAI.\n",
    "\n",
    "3. How to teach your Neural DB with real-time RLHF (Reinforcement Learning with Human Feedback) to correct any retrieval failures.\n",
    "\n",
    "To unlock additional features exporting the DB to ThirdAI's Playground for interactive QnA and teaching, please reach out to contact@thirdai.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thirdai's license activation\n",
    "\n",
    "import thirdai\n",
    "try:\n",
    "    thirdai.licensing.activate(\"D0F869-B61466-6A28F0-14B8C6-0AC6C6-V3\")\n",
    "except:\n",
    "    print(\"You need a license key to use ThirdAI's library. Please request a trial license at https://www.thirdai.com/try-bolt/\")\n",
    "\n",
    "thirdai.set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "import nltk\n",
    "nltk.data.path.append(\"./data/\")\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from doc_utils import documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display your CSV contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"sample_nda.csv\"\n",
    "query_column_name = \"QUERY\"\n",
    "target_column_name = \"DOC_ID\"\n",
    "\n",
    "# Visualize the dataframe and get the column names in the csv_file.\n",
    "# Your target column (id_col) name has to match the target column in the model defined above (we are using target_column_name across the notebook)\n",
    "# You will have to pick your choice of strong_columns and weak_columns for the insert and train step shown next.\n",
    "# Strong columns are usually the most important ones like titles of documents, keywords, categories etc\n",
    "# Weak columns are usually the long descriptions\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep and schema\n",
    "\n",
    "Based on the above file, this is how you can define your NeuralDB schema.\n",
    "\n",
    "Note: Your *target_column_name* has to be \"DOC_ID\" to be able to use pre-trained base models. If you're target column is something else, please rename it to \"DOC_ID\". Also, the \"DOC_ID\" should contain integers from from 0 to n_rows - 1.\n",
    "\n",
    "If you're creating a DN from scratch, this is not mandatory. \n",
    "\n",
    "We currently only support CSV files. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'search_string_name':\"QUERY\", ## keep this as it is if you want to use our base models.\n",
    "    'target_column_name':\"DOC_ID\",\n",
    "    'strong_search_columns':[\"passage\"],\n",
    "    'weak_search_columns':[\"para\"],\n",
    "    'display_columns':['passage'], # what columns to show when we return search results\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = ['sample_nda.csv']\n",
    "\n",
    "doclist = documents.DocList()\n",
    "\n",
    "doclist_ = []\n",
    "\n",
    "for file in csv_files:\n",
    "    csv_doc = documents.CSV(\n",
    "        path=file,\n",
    "        id_col=schema['target_column_name'],\n",
    "        strong_cols=schema['strong_search_columns'],\n",
    "        weak_cols=schema['weak_search_columns'],\n",
    "        display_cols=schema['display_columns'],\n",
    "    )\n",
    "    #\n",
    "    doclist.add_document(csv_doc)\n",
    "    doclist_.append(csv_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize your NeuralDB from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndb = bolt.UniversalDeepTransformer(\n",
    "    data_types = {\n",
    "        schema['search_string_name']: bolt.types.text(tokenizer=\"char-4\"),\n",
    "        schema['target_column_name']: bolt.types.categorical(delimiter=\":\"),\n",
    "    },\n",
    "    target=schema['target_column_name'],\n",
    "    n_target_classes=1000000, # this is the expected number of unique paragraphs in the DB, larger number will increase the training time\n",
    "    integer_target=True,\n",
    "    options={\"neural_db\": True}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert your CSV into the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data | source 'sample_nda.csv'\n",
      "loaded data | source 'sample_nda.csv' | vectors 18 | batches 1 | time 0s | complete\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found sparse index 55507 that exceeded dimension 50000.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m doc_config_ \u001b[39m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m'\u001b[39m:doc_config\u001b[39m.\u001b[39mintroduction_dataset,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mstrong_column_names\u001b[39m\u001b[39m'\u001b[39m:doc_config\u001b[39m.\u001b[39mstrong_cols,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_random_hashes\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m8\u001b[39m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     10\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m ndb\u001b[39m.\u001b[39;49minsert_into_neural_db_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdoc_config_)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/thirdai/bolt/udt_modifications.py:281\u001b[0m, in \u001b[0;36mmodify_mach_udt.<locals>.wrapped_introduce_documents\u001b[0;34m(self, filename, strong_column_names, weak_column_names, num_buckets_to_sample, num_random_hashes, fast_approximation)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_introduce_documents\u001b[39m(\n\u001b[1;32m    271\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    272\u001b[0m     filename: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m     fast_approximation: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    278\u001b[0m ):\n\u001b[1;32m    279\u001b[0m     data_source \u001b[39m=\u001b[39m _create_data_source(filename)\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mreturn\u001b[39;00m original_introduce_documents(\n\u001b[1;32m    282\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    283\u001b[0m         data_source,\n\u001b[1;32m    284\u001b[0m         strong_column_names,\n\u001b[1;32m    285\u001b[0m         weak_column_names,\n\u001b[1;32m    286\u001b[0m         num_buckets_to_sample,\n\u001b[1;32m    287\u001b[0m         num_random_hashes,\n\u001b[1;32m    288\u001b[0m         fast_approximation,\n\u001b[1;32m    289\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found sparse index 55507 that exceeded dimension 50000."
     ]
    }
   ],
   "source": [
    "for doc in doclist_:\n",
    "    doc_config = doc.get_config()\n",
    "    doc_config_ = {\n",
    "        'filename':doc_config.introduction_dataset,\n",
    "        'strong_column_names':doc_config.strong_cols,\n",
    "        'weak_column_names':[],\n",
    "        'num_buckets_to_sample':16,\n",
    "        'num_random_hashes':8,\n",
    "    }\n",
    "    #\n",
    "    ndb.insert_into_neural_db_batch(**doc_config_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndb.pretrain_neural_db('sample_nda.csv', schema['strong_search_columns'], schema['weak_search_columns'])\n",
    "\n",
    "## TODO: This has to be argument less. Merge this with the above call."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndb.train_neural_db('sample_nda_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many search results do you want to retrieve from your files for every query\n",
    "N_REFERENCES = 2\n",
    "\n",
    "ndb.set_decode_params(N_REFERENCES, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the NeuralDB you just built. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any open-source model of your choice (like MPT or Dolly) for answer generation with the same prompt that you use with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.qaprompts import qa_prompt, make_chain\n",
    "\n",
    "your_openai_key = \"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1, \n",
    "    openai_api_key=your_openai_key,\n",
    ")\n",
    "\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query):\n",
    "    reference_ids = ndb.query({\"QUERY\":query})\n",
    "    reference_ids = [itm[0] for itm in reference_ids]\n",
    "    references = [doclist.get_new_display_items().iloc[p] for p in reference_ids]\n",
    "    return references\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return qa_chain.run(question=query, context_str='\\n\\n'.join(references[:3]), length=\"abt 50 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CONFIDENTIALITY AGREEMENT This Confidentiality Agreement (the “Agreement”) is made by and between ACME. dba ToTheMoon Inc. with offices at 2025 Guadalupe St. Suite 260 Austin TX 78705 and StarWars dba ToTheMars with offices at the forest moon of Endor and entered as of May 3 2023 (“Effective Date”).', 'In consideration of the business discussions disclosure of Confidential Information and any future business relationship between the parties it is hereby agreed as follows: 1. CONFIDENTIAL INFORMATION. For purposes of this Agreement the term “Confidential Information” shall mean any information business plan concept idea know-how process technique program design formula algorithm or work-in-process Request for Proposal (RFP) or Request for Information (RFI) and any responses thereto engineering manufacturing marketing technical financial data or sales information or information regarding suppliers customers employees investors or business operations and other information or materials whether disclosed in written graphic oral or electronic form whether tangible or intangible and in whatever form or medium provided or which is learned or disclosed in the course of discussions studies or other work undertaken between the parties prior to or after the Effective Date.', '12. ENTIRE AGREEMENT. This Agreement constitutes the entire agreement with respect to the subject matter hereof and supersedes all prior agreements and understandings between the parties (whether written or oral) relating to the subject matter and may not be amended or modified except in a writing signed by an authorized representative of both parties. The terms of this Agreement relating to the confidentiality and non-use of Confidential Information shall continue after the termination of this Agreement for a period of the longer of (i) five (5) years or (ii) when the Confidential Information no longer qualifies as a trade secret under applicable law.', '2. NEED TO KNOW. The receiving party shall limit its disclosure of the other party’s Confidential Information to those of its officers and employees and subcontractors (i) to which such disclosure is necessary for purposes of the discussions contemplated by this Agreement and (ii) who have agreed in writing to be bound by provisions no less restrictive than those set forth in this Agreement.', 'IN WITNESS WHEREOF this Agreement has been duly executed by the parties hereto as of the latest date set forth below: Acme Inc. StarWars Inc. By: By: Name: Bugs Bunny Name: Luke Skywalker Title: CEO Title: CEO Date: May 5 2023 Date: May 7 2023']\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the effective date of this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The effective date of this Confidentiality Agreement is May 3, 2023 (ACME dba ToTheMoon Inc. and StarWars dba ToTheMars, 2023).\n"
     ]
    }
   ],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ask a query that the model gets it wrong. Subsequently, let's teach the model to correct itself using our RLHF methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provides insufficient information to determine the parties involved in this agreement.\n"
     ]
    }
   ],
   "source": [
    "query = \"who are the parties involved in this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "answer = get_answer(query, references)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to teach your model (RLHF)\n",
    "\n",
    "This is one of the marquee features that we provide. Thanks to our efficient training capabilties, we can offer you to teach the DB to correct itself in the event of it not being able to get the relevant paragraphs from the database. \n",
    "\n",
    "Also, the RLHF teachings done a NeuralDB will generalize beyond the current documents if we run *ndb.clear_index()* and insert new documents.\n",
    "\n",
    "To do RLHF, we provide two functions:\n",
    "\n",
    "1. Associate: Using this function, you can associate two phrases to give similar results. For examples, assume you're in the contract review domain. And you're interested in asking a question like \"who are the parties involved in this contract?\". However, most contracts have the phrase \"made by and between\" to suggest the parties involved in the contracts (like \"this agreement is made by and between company A and company B\"). In this scenario, you can simply call *ndb.teach_concept_association([\"parties involved\",\"made by and between\"])* and the model would learn the relation. In the subsequent documents, you're more likely to retrieve the passage containing the correct information.\n",
    "\n",
    "2. Upvote: Let's say you searched for a query \"is there a limited liability clause?\" and you got 5 search results (along with their passage IDs). If you know that the correct result is actually the 2nd one instead of the first one. Then you can simply call *ndb.upvote(\"is there a limited liability clause\",passage_id_of_the_best_search_result)*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLHF using function calls \n",
    "\n",
    "In the above example, the DB could not understand that the phrase \"date of signing\". But if you are an expert in contracts, you know that \"date of signing\" usually goes with phrases like \"duly executed\" (for example, \"this Agreement has been duly executed by the parties hereto as of the latest date set forth below ...\"). So, let's teach the DB that these two phrases should retrieve similar passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_samples = [({\"QUERY\":\"parties involved\"},{\"QUERY\":\"made by and between\"})]\n",
    "\n",
    "ndb.associate(rlhf_samples, 7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's query the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parties involved in this agreement are ACME, dba ToTheMoon Inc. with offices at 2025 Guadalupe St. Suite 260 Austin TX 78705 and StarWars dba ToTheMars with offices at the forest moon of Endor (Confidentiality Agreement).\n"
     ]
    }
   ],
   "source": [
    "query = \"who are the parties involved in this agreement?\"\n",
    "\n",
    "references = get_references(query)\n",
    "answer = get_answer(query, references)\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
