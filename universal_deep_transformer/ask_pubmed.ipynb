{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QnA with Pubmed using ThirdAI's Playground\n",
    "\n",
    "In this notebook, you will be able to\n",
    "\n",
    "1. Download ThirdAI's BOLT LLM trained on Pubmed-800K and the processed data.\n",
    "\n",
    "2. Ask any question and get relevant references from Pubmed.\n",
    "\n",
    "3. (Optional) How to use your OpenAI key to generate grounded answers without hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai==0.7.6\n",
    "!pip3 install openai\n",
    "!pip3 install paper-qa\n",
    "!pip3 install langchain\n",
    "!pip3 install json\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt,licensing\n",
    "\n",
    "import os\n",
    "if \"THIRDAI_KEY\" in os.environ:\n",
    "    licensing.activate(os.environ[\"THIRDAI_KEY\"])\n",
    "else:\n",
    "    ## Please request for a trial license @ https://www.thirdai.com/try-bolt/\n",
    "    licensing.activate(\"\")  # Enter your ThirdAI key here\n",
    "    \n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model Checkpoint\n",
    "checkpoint = \"pubmed_800k.bolt\"\n",
    "if not os.path.exists(checkpoint):\n",
    "    os.system(\"wget -O pubmed_800k.bolt https://www.dropbox.com/s/kwoqt5c7bqbisbl/pubmed_800k.bolt?dl=0\")\n",
    "\n",
    "model = bolt.UniversalDeepTransformer.load(checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset to display references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Processed Data to show references\n",
    "display_data = 'pubmed_800k_combined.json'\n",
    "if not os.path.exists(display_data):\n",
    "    os.system(\"wget -O pubmed_800k_combined.json https://www.dropbox.com/s/8phkx4fht9j2npy/pubmed_800k_combined.json?dl=0\")\n",
    "\n",
    "data_store = {}\n",
    "with open(display_data, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for json_data in data:\n",
    "    data_store[json_data[\"label\"]] = json_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Answers from OpenAI using Langchain\n",
    "\n",
    "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "  os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.prompts import qa_prompt\n",
    "from paperqa.chains import make_chain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query):\n",
    "    search_results = db.search(query,top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        references.append(result.text)\n",
    "    return references\n",
    "\n",
    "def get_answer(query, references):\n",
    "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:3]), answer_length=\"abt 50 words\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what percentage of cancer patients have depression?\"\n",
    "\n",
    "references = get_references(query)\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to detect depression in geriatric cancer patients ?\"\n",
    "\n",
    "references = get_references(query)\n",
    "answer = get_answer(query, references)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
