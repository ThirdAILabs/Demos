{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Ticket Search**\n",
    "A support ticket search system takes in a customer support request, such as:\n",
    "\n",
    "*\"My internet connection has been slow for the past few days after traveling overseas\"*\n",
    "\n",
    "and searches for similar past tickets. This is useful for both internal use as well as to help customers troubleshoot using previous customer support outcomes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Get your support ticket history**\n",
    "A CSV file with columns for ticket IDs and titles is sufficient, but our system can utilize any additional information that you give it. In this notebook, we use a dataset containing Stack Overflow questions and their respective answers as our \"support ticket history\" as these question-answer pairs emulate the problem-solution nature of support tickets. It has the following fields:\n",
    "- `\"id\"`: The ticket ID.\n",
    "- `\"title\"`: The title of the Stack Overflow post or question.\n",
    "- `\"question_body\"`: The part that explains the question and explains details regarding the context of the problem.\n",
    "- `\"answer_body\"`: The answer to the question.\n",
    "- `\"tags\"`: Categories for the question, such as \"git version\" or \"html button\".\n",
    "\n",
    "Note that the IDs must be integers between 0 and `N_TICKETS - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# This dataset is originally from https://www.kaggle.com/datasets/stackoverflow/stackoverflow?select=posts_questions\n",
    "# Preprocessed as follows:\n",
    "# - Keep 100,000 most popular resolved questions\n",
    "# - Extract relevant columns\n",
    "# - Add ticket IDs\n",
    "# We will download two files that contain the same samples but preprocessed slightly differently.\n",
    "\n",
    "# stackoverflow-display.csv retains original HTML formatting\n",
    "os.system(\"curl -L https://www.dropbox.com/s/fby91zecup9fq9r/stackoverflow-display.csv?dl=0 --output stackoverflow-display.csv\")\n",
    "\n",
    "# stackoverflow-train.csv replaces newlines in question_body and answer_body with whitespaces\n",
    "os.system(\"curl -L https://www.dropbox.com/s/amjs8d5xisiinv5/stackoverflow-train.csv?dl=0 --output stackoverflow-train.csv\")\n",
    "\n",
    "!pip3 install pandas\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_colwidth = 150\n",
    "df = pd.read_csv(\"stackoverflow-display.csv\")\n",
    "\n",
    "df.head(30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Import and Configure UDT**\n",
    "First, install the ThirdAI package and activate the license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade\n",
    "\n",
    "import thirdai\n",
    "thirdai.licensing.activate(\"AHUU-TM9U-EH4M-JWFW-VE7F-3VRH-7TMV-CHPX\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is where we define our model. Let's walk through each parameter:\n",
    "- `data_types`: this is where we define the data fields and types that our model is concerned about. We want a model that accepts a text query, which we arbitrarily named `\"query\"`, and returns a ticket ID, which is named `\"id\"` in accordance with our dataset.\n",
    "- `target`: identify the data field that we will predict. In this case, it's `\"id\"`.\n",
    "- `n_target_classes`: the number of unique ticket IDs.\n",
    "- `integer_target`: this is True since our ticket IDs are integers between 0 and `n_target_classes - 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import bolt\n",
    "\n",
    "model = bolt.UniversalDeepTransformer(\n",
    "    data_types={\n",
    "        \"query\": bolt.types.text(),\n",
    "        \"id\": bolt.types.categorical(n_classes=100_000, type=\"int\"),\n",
    "    },\n",
    "    target=\"id\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Cold-start the model**\n",
    "It can be difficult to collect a supervised dataset that maps customer queries to relevant past ticket IDs (it would look something like the following)\n",
    "```\n",
    "query,id\n",
    "npm install stuck on ubuntu machine,32887\n",
    "...\n",
    "```\n",
    "So we can instead cold-start the model on our existing support ticket history. To do so, we need to give the model the path to the dataset as well as how to use the different columns of the dataset. \n",
    "\n",
    "Each support ticket has a title, a question body, an answer body, and tags. Since the title usually summarizes the problem well, we’ll call that a **strong column**. But the question body and the answer have all the details, so we’ll add them as **weak columns**, along with tags.\n",
    "\n",
    "You should feel free to play around and see what configuration gives you the best results for your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cold_start(\n",
    "    filename=\"stackoverflow-train.csv\",\n",
    "    strong_column_names=[\"title\"],\n",
    "    weak_column_names=[\"question_body\", \"answer_body\", \"tags\"],\n",
    "    learning_rate=0.001,\n",
    "    epochs=5,\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "# Save the model for later use.\n",
    "model.save(\"ticketsearch.bolt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Display the results**\n",
    "When we call model.predict() with our query, we get scores for each ticket ID. Let’s write a helper function to display this information to customers in a nice way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stackoverflow-display.csv\")\n",
    "\n",
    "def search(query):\n",
    "    print(\"Question:\", query)\n",
    "    print(\"Results:\\n\")\n",
    "\n",
    "    scores = model.predict({\"query\": query})\n",
    "    k = 3\n",
    "    sorted_ticket_ids = scores.argsort()[-k:][::-1]\n",
    "\n",
    "    for t_id in sorted_ticket_ids:\n",
    "        result = df.iloc[t_id][\"title\"]\n",
    "        print(\"\\t> \" + result)\n",
    "\n",
    "search(\"c++ move semantics\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major advantage of this model is that it does very well with long queries with many details such as the machine type, specific error messages, or descriptive full sentences. Try these ones!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"How do I copy a vector in rust without getting borrow as mutable error\")\n",
    "search(\"I ran pip install Jupiter but terminal cannot find Jupiter and ipython command is not found\")\n",
    "search(\"Postgres db 9.1 running on AWS EC2 with ubuntu 12.04 running clusters stuck\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also wrap this in a UI, for example using `gradio`, to turn this into an app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade pydantic\n",
    "!pip3 install gradio\n",
    "import gradio as gr\n",
    "\n",
    "max_posts = 10\n",
    "\n",
    "# If you want to move this app script to a separate file, uncomment the \n",
    "# following lines to load the saved model and to load the dataset for display.\n",
    "# model = bolt.UniversalDeepTransformer.load(\"ticketsearch.bolt\")\n",
    "# df = pd.read_csv(\"stackoverflow-display.csv\")\n",
    "\n",
    "def search(query):\n",
    "    scores = model.predict({\"query\": query})\n",
    "    sorted_post_ids = scores.argsort()[-max_posts:][::-1]\n",
    "    relevant_posts = [\n",
    "        df.iloc[pid] for pid in sorted_post_ids\n",
    "    ]\n",
    "\n",
    "    title = [gr.Markdown.update(visible=True)]\n",
    "    boxes = [\n",
    "        gr.Accordion.update(visible=True, open=False, label=post['title']) \n",
    "        for post in relevant_posts\n",
    "    ]\n",
    "    bodies = [\n",
    "        gr.HTML.update(\n",
    "            visible=True,\n",
    "            value=f\"<h1>{post['title']}</h1>\\n\\n\"\n",
    "            f\"<h2>Question:</h2>\\n{post['question_body']}\\n\\n\"\n",
    "            f\"<h2>Answer:</h2>\\n{post['answer_body']}\\n\\n\"\n",
    "            f\"<h2>Tags:</h2>\\n{post['tags']}\\n\\n\")\n",
    "        for post in relevant_posts\n",
    "    ]\n",
    "\n",
    "    return title + boxes + bodies\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    query = gr.Textbox(label=\"Question\")\n",
    "    title = [gr.Markdown(\"# Relevant Posts\", visible=False)]\n",
    "    post_boxes = []\n",
    "    post_bodies = []\n",
    "    for i in range(max_posts):\n",
    "        with gr.Accordion(\"\", visible=False) as box:\n",
    "            post_boxes.append(box)\n",
    "            body = gr.HTML(\"\", visible=False)\n",
    "            post_bodies.append(body)\n",
    "    allblocks = title + post_boxes + post_bodies\n",
    "\n",
    "    # Replace `change()` with `submit()` to make the results update only after \n",
    "    # pressing 'enter' instead of at every keystroke.\n",
    "    query.change(search, query, allblocks)\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
